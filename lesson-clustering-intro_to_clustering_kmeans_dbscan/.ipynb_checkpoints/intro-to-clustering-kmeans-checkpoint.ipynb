{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Introduction to Clustering and K-Means\n",
    "\n",
    "_Authors: Dave Yerrington (SF), Joseph Nelson (DC)_\n",
    "\n",
    "_Modified for DSI-EAST-1 by Justin Pounders (ATL)_\n",
    "\n",
    "---\n",
    "\n",
    "![](https://snag.gy/kYWumd.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning Objectives\n",
    "\n",
    "*After this lesson, you will be able to:*\n",
    "\n",
    "- Describe basic unsupervised clustering problems\n",
    "- Format and preprocess data for clustering\n",
    "- Perform a K-Means Clustering Analysis\n",
    "- Evaluate clusters for fit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lesson Guide\n",
    "- [Unsupervised learning](#unsupervised)\n",
    "- [Introduction to clustering](#intro)\n",
    "- [What is clustering?](#what)\n",
    "- [KNN review](#knn)\n",
    "- [Clustering algorithms](#algos)\n",
    "- [K-means clustering](#k-means)\n",
    "- [Refresher: Euclidean distance](#euclidean)\n",
    "- [K-Means step-by-step](#km-steps)\n",
    "- [K-Means: a visual example](#vis)\n",
    "- [K-Means caveats and pitfalls](#caveats)\n",
    "    - [Sensitive to outliers](#sensitive)\n",
    "    - [Sensitive to centroid initialization](#centroid-init)\n",
    "    - [How many K?](#how-many-k)\n",
    "- [Choosing K](#choose-k)\n",
    "- [A note on K-Means convergence](#converge)\n",
    "- [K-Means in sklearn](#kmeans-skl)\n",
    "    - [Visually verifying cluster labels](#verify)\n",
    "- [Metrics: inertia and the silhouette coefficient](#sil)\n",
    "- [Practice: use K-Means on the \"Isotopic Composition Plutonium Batches\" data](#pluto)\n",
    "    - [How does scaling affect fit?](#scaling)\n",
    "- [Conclusion: K-Means tradeoffs](#conclusion)\n",
    "- [Additional resources](#resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='unsupervised'></a>\n",
    "\n",
    "## Unsupervised learning\n",
    "\n",
    "---\n",
    "\n",
    "> **Supervised --> Classification** - create a model to predict which group a point belongs to.\n",
    "\n",
    "> **Unsupervised --> Clustering** - find groups that exist in the data already.\n",
    "\n",
    "**We use unsupervised methods when we don't have labeled  data.**\n",
    "\n",
    "There are no true targets to predict, we derive the likely categories from the structure in our data.\n",
    "\n",
    "| Pros | Cons |\n",
    "|---|---|\n",
    "| No labels | Difficult to evaluate correctness without subject matter expertise |\n",
    "| Few or no assumptions about data | Scaling / normalization often required |\n",
    "| Useful for subset / segmentation discovery | Can be difficult to visualize |\n",
    "| Great for broad insights | Extremely difficult to tune |\n",
    "| Many models available | No obvious choice in many cases |\n",
    "| Black magic | Considered \"unconventional\" and unreliable |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"intro\"></a>\n",
    "## Introduction to clustering\n",
    "\n",
    "---\n",
    "\n",
    "### Uses for clustering: \n",
    "   - Find items with similar behavior (users, products, voters, etc)\n",
    "   - Market segmentation\n",
    "   - Understand complex systems\n",
    "   - Discover meaningful categories for your data\n",
    "   - Reduce the number of classes by grouping (e.g. bourbons, scotches -> whiskeys)\n",
    "   - Reduce the dimensions of your problem\n",
    "   - Pre-processing! Create labels for supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Genetics\n",
    "![](https://snag.gy/TP2RA4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Consumer Internet\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"https://snag.gy/EbLeqd.jpg\"></td>\n",
    "        <td><img src=\"https://snag.gy/xsNvK8.jpg\"></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Business\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "    <td><img src=\"https://snag.gy/pDueQ2.jpg\" width=\"500\"></td>\n",
    "    <td>\n",
    "        <li>Identifying Demographics</li>\n",
    "        <li>Spending Patterns</li>\n",
    "        <li>Consumer Trends</li>\n",
    "        <li>Customer Characteristics</li>\n",
    "        <li>Recommender Systems</li>\n",
    "        <li>Taxonomy / Categorization</li>\n",
    "    </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://snag.gy/YUt5RO.jpg\" style=\"float: left; margin-right: 25px; width: 250px\">\n",
    "\n",
    "## What problems do you think arise during \"clustering\"?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='what'></a>\n",
    "## What is Clustering? \n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://snag.gy/BdfATE.jpg\" style=\"width: 500px\">\n",
    "\n",
    "**Clustering is one of the most ubiquitous and widespread processes for assigning discrete structure to data.** \n",
    "\n",
    "In clustering, we group observations in a dataset together such that the members of a group are more similar to each other than they are to members of other groups. \n",
    "\n",
    "There are a wide variety of methods and criteria to perform this task.\n",
    "\n",
    "**Properties of clustering procedures:**\n",
    "- No \"true\" target / response to compare\n",
    "- We apply structure to data quantitatively based on specific criteria\n",
    "- Predictions of label are based on the structure of the data\n",
    "\n",
    "**For example:** your employer gives you a dataset of voter preferences from a local poll. They want you to figure out just exactly how these voters are grouping based on their preferences. The answer: clustering!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='knn'></a>\n",
    "## KNN review\n",
    "\n",
    "---\n",
    "\n",
    "KNN is a *supervised* classification method.\n",
    "\n",
    "![](https://snag.gy/WPF4ZS.jpg)\n",
    "\n",
    "\n",
    "**Check:** Why is KNN a classification method? What makes KNN supervised as opposed to unsupervised?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Any ideas of how the KNN algorithm could be modified to be *unsupervised?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"algos\"></a>\n",
    "## Clustering algorithms \n",
    "\n",
    "---\n",
    "\n",
    "The are many different algorithms that can perform clustering given a dataset:\n",
    "\n",
    "- **K-Means** (mean centroids)\n",
    "- **Heirarchical** (nested clusters by merging or splitting successively)\n",
    "- **DBSCAN** (density based)\n",
    "- **Affinity Propagation** (graph based approach to let points 'vote' on their preferred 'exemplar')\n",
    "- **Mean Shift** (can find number of clusters)\n",
    "- **Spectral Clustering**\n",
    "- **Agglomerative Clustering** (suite of algorithms all based on applying the same criteria/characteristics of one cluster to others)\n",
    "\n",
    "\n",
    "\n",
    "Today we're going to look at two of these algorithms: **k-means** and **DBSCAN**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='k-means'></a>\n",
    "## K-Means clustering\n",
    "\n",
    "---\n",
    "\n",
    "#### K-Means is the most popular clustering algorithm\n",
    "\n",
    "K-means is one of the easier methods to understand and other clustering techniques use some of the same assumptions that K-Means relies on.\n",
    "\n",
    "- **K** is the number of clusters.\n",
    "- **Means** refers to the mean points of the K clusters.\n",
    "\n",
    "*The **number** of clusters $k$ is chosen in advance.* \n",
    "\n",
    "The **goal** is to split the data into groups (or clusters) such that the total sum of squared distances from each point to the mean point of the cluster is minimized.\n",
    "\n",
    "The algorithm takes your entire dataset and iterates over its observations to determine clusters based around center points. These center points are known as **centroids**. \n",
    "\n",
    "**K-means iterative fitting:**\n",
    "1. Pick a value for k (the number of clusters to create)\n",
    "2. Initialize k 'centroids' (starting points) in your data\n",
    "3. Create your clusters. Assign each point to the nearest centroid. \n",
    "4. Make your clusters better. Move each centroid to the center of its cluster. \n",
    "5. Repeat steps 3-4 until your centroids converge. \n",
    "\n",
    "> **Note:** Unfortunately there is no formula to determine the absolute best number of $k$ clusters. Unsupervised learning is inherently subjective! We can, however, choose the \"best\" $k$ based on predetermined criteria. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='euclidean'></a>\n",
    "## Refresher: Euclidean distance\n",
    "\n",
    "---\n",
    "\n",
    "### $$ d(x_1, x_2) = \\sqrt{\\sum_{i=1}^N (x_{1i} - x_{2i})^2} $$\n",
    "\n",
    "**For example, take two points:**\n",
    "\n",
    "- $x1 = (2, -1)$\n",
    "- $x2 = (-2, 2)$\n",
    "\n",
    "**The Euclidean distance between these two points is:**\n",
    "\n",
    "### $$\\begin{aligned}\n",
    "d(x1, x2) &= \\sqrt{ (2 - (-2))^2 + ((-1) - 2)^2 } \\\\\n",
    "d(x1, x2) &= \\sqrt{25} \\\\\n",
    "d(x1, x2) &= 5 \n",
    "\\end{aligned}$$\n",
    "\n",
    "**Using sklearn:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import euclidean_distances\n",
    "\n",
    "X = np.array([[2, -1], [-2, 2]])\n",
    "\n",
    "print(euclidean_distances(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='km-steps'></a>\n",
    "## K-Means step-by-step\n",
    "\n",
    "---\n",
    "\n",
    "<table width=\"500\" cellpadding=\"50\"> \n",
    "<tr>\n",
    "   <td><img src=\"https://snag.gy/7haoS3.jpg\" style=\"width: 150px\"></td>\n",
    "   <td style=\"vertical-align: top; width: 400px;\"><br><b>Step 1.</b><br>We have data in a N-Dimensional feature space (2D for example).</td>\n",
    "</tr>\n",
    "<tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table width=500 cellpadding=\"50\"> \n",
    "<tr>\n",
    "   <td><img src=\"https://snag.gy/DaIVgk.jpg\" style=\"width: 150px !important;\"></td>\n",
    "   <td style=\"align: top; width: 400px; vertical-align: top;\"><br><b>Step 2.</b><br>Intialize K centroid (2 here).</td>\n",
    "</tr>\n",
    "<tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table width=500 cellpadding=\"50\"> \n",
    "<tr>\n",
    "   <td><img src=\"https://snag.gy/DaIVgk.jpg\" style=\"width: 150px !important;\"></td>\n",
    "   <td style=\"align: top; width: 400px; vertical-align: top;\"><b>Step 3.</b><br>Assign points to *closest* cluster based on _euclidean distance_.<br><br>$\\sqrt{(x_1-x_2)^2 + (y_1-y_2)^2}$\n",
    "\n",
    "   </td>\n",
    "</tr>\n",
    "<tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table width=500 cellpadding=\"50\"> \n",
    "<tr>\n",
    "   <td><img src=\"https://snag.gy/NY1EeT.jpg\" style=\"width: 150px !important;\"></td>\n",
    "   <td style=\"align: top; width: 400px; vertical-align: top;\"><b>Step 4.</b><br>Calculate mean of points assigned to centroid (2 here).  Update new centroid positions to mean (ie: geometric center).<br><br>$new\\ centroid\\ position= \\bar{x}, \\bar{y}$\n",
    "   </td>\n",
    "</tr>\n",
    "<tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table width=500 cellpadding=\"50\"> \n",
    "<tr>\n",
    "   <td><img src=\"https://snag.gy/tSfDZs.jpg\" style=\"width: 150px !important;\"></td>\n",
    "   <td style=\"align: top; width: 400px; vertical-align: top;\"><b>Step 5.</b><br>Repeat step 3-4, updating class membership based on centroid distance.\n",
    "   </td>\n",
    "</tr>\n",
    "<tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table width=500 cellpadding=\"50\"> \n",
    "<tr>\n",
    "   <td><img src=\"https://snag.gy/BbIicn.jpg\" style=\"width: 150px !important;\"></td>\n",
    "   <td style=\"align: top; width: 400px; vertical-align: top;\"><b>Fin.</b><br>Convergence is met once all points no longer change to a new class (defined by closest centroid distance).\n",
    "   </td>\n",
    "</tr>\n",
    "<tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='caveats'></a>\n",
    "## A few K-Means caveats...\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id='sensitive'></a>\n",
    "### K-Means is sensitive to outliers\n",
    "\n",
    "![](https://snag.gy/WFNMQY.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id='centroid-init'></a>\n",
    "### K-Means is sensitive to centroid initialization\n",
    "\n",
    "![](https://snag.gy/5sigCD.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='how-many-k'></a>\n",
    "### How many K?\n",
    "\n",
    "Sometimes it's obvious, sometimes it's not!  What do you think?\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td valign=\"bottom\" style=\"vertical-align: bottom; text-align: center;\"><img src=\"http://i.stack.imgur.com/4rU39.png\"><br>1</td>\n",
    "        <td valign=\"bottom\" style=\"vertical-align: bottom; text-align: center;\"><img src=\"http://i.stack.imgur.com/gq28F.png\"><br>2</td>\n",
    "        <td valign=\"bottom\" style=\"vertical-align: bottom; text-align: center;\"><img src=\"https://snag.gy/cWPgno.jpg\"><br>3</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='choose-k'></a>\n",
    "## Choosing K\n",
    "\n",
    "---\n",
    "\n",
    "There are different methods of initializing centroids. For instance:\n",
    "\n",
    "- Randomly\n",
    "- Manually\n",
    "- Special KMeans++ method in Sklearn (_This initializes the centroids to be generally distant from each other_)\n",
    "\n",
    "**Depending on your problem, you may find some of these are better than others.**\n",
    "\n",
    "> **Note:** Manual is recommended if you know your data well enough to see the clusters without much help, but rarely used in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='converge'></a>\n",
    "## A note on K-Means convergence\n",
    "\n",
    "---\n",
    "\n",
    "In general, k-means will converge to a solution and return a partition of k clusters, even if no natural clusters exist in the data.  It's entirely possible – in fact, *common* – that the clusters do not mean anything at all. \n",
    "\n",
    "**Knowing your domain and dataset is essential. Evaluating the clusters visually is a must (if possible).**\n",
    "\n",
    "> _\"Given enough time, K-means will always converge, however this may be to a local minimum. This is highly dependent on the initialization of the centroids. As a result, the computation is often done several times, with different initializations of the centroids. One method to help address this issue is the k-means++ initialization scheme, which has been implemented in scikit-learn (use the init='kmeans++' parameter). This initializes the centroids to be (generally) distant from each other, leading to provably better results than random initialization, as shown in the reference.\"_ [sklearn Clustering Guide](http://scikit-learn.org/stable/modules/clustering.html#k-means)\n",
    "\n",
    "![](http://www.datamilk.com/kmeans_animation.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='kmeans-skl'></a>\n",
    "## K-Means in sklearn\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from sklearn.cluster import KMeans, k_means\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "X, y = make_blobs(\n",
    "    n_samples    =  200, \n",
    "    centers      =  3, \n",
    "    n_features   =  2,\n",
    "    random_state =  0\n",
    ")\n",
    "\n",
    "\n",
    "df = pd.DataFrame(X, columns=['x', 'y'])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df.plot(kind=\"scatter\", x=\"x\", y=\"y\", figsize=(6, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=3, random_state=0).fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "After we fit our data, we can get our predicted labels from `model.labels_` and the center points`model.cluster_centers_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "predicted = model.labels_\n",
    "centroids = model.cluster_centers_\n",
    "\n",
    "print(\"Predicted clusters to points: \", predicted)\n",
    "print(\"Location of centroids: \")\n",
    "print(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df['predicted'] = predicted\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='verify'></a>\n",
    "### Visually verifying cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "df.plot(x=\"x\", y=\"y\", kind=\"scatter\", c=df['predicted'], cmap='gist_rainbow', alpha=.7)\n",
    "plt.scatter(centroids[:,:1], centroids[:,1:], marker='X', s=150, alpha=.7, c=range(0,3), cmap='gist_rainbow')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='sil'></a>\n",
    "\n",
    "## Metrics: inertia and the silhouette coefficient\n",
    "\n",
    "---\n",
    "\n",
    "**Inertia** -- sum of squared errors for each cluster\n",
    "- low inertia = dense cluster\n",
    "- ranges from 0 to very high values\n",
    "\n",
    "$$\\sum_{j=0}^{n} (x_j - \\mu_i)^2$$\n",
    "where $\\mu_i$ is a cluster centroid. (K-means explicitly tries to minimize this.)\n",
    "\n",
    "`.inertia_` is an attribute of sklearn's kmeans models.\n",
    "\n",
    "**Silhouette Coefficient** -- measure of how far apart clusters are\n",
    "- high Silhouette Score = clusters are well separated\n",
    "- ranges from -1 to 1\n",
    "\n",
    "The definition is a little involved, but intuitively the score is based on how much closer data points are to their own clusters than to the nearest neighbor cluster.\n",
    "\n",
    "We can calculate it in sklearn with `metrics.silhouette_score(X_scaled, labels, metric='euclidean')`.\n",
    "- [https://en.wikipedia.org/wiki/Silhouette_(clustering)](https://en.wikipedia.org/wiki/Silhouette_(clustering))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "inertia = model.inertia_\n",
    "sil_score = silhouette_score(df, predicted, metric='euclidean')\n",
    "\n",
    "print('Inertia          = ', inertia)\n",
    "print('Silhouette score = ', sil_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You will revisit these concepts in more depth on Thursday!  Today we will stick with the silhouette score.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='pluto'></a>\n",
    "## Practice: sklearn + K-Means with \"Isotopic Composition Plutonium Batches\"\n",
    "\n",
    "---\n",
    "\n",
    "We have a nice [data dictionary](https://vincentarelbundock.github.io/Rdatasets/doc/cluster/pluton.html)\n",
    "\n",
    "    Pu238 : the percentages of (238)Pu, always less than 2 percent.\n",
    "\n",
    "    Pu239 : the percentages of (239)Pu, typically between 60 and 80 percent (from neutron capture of Uranium, (238)U).\n",
    "\n",
    "    Pu240 : percentage of the plutonium 240 isotope.\n",
    "\n",
    "    Pu241 : percentage of the plutonium 241 isotope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "csv_file = \"https://vincentarelbundock.github.io/Rdatasets/csv/cluster/pluton.csv\"\n",
    "# there is also a copy of the csv in the assets/datasets file\n",
    "\n",
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Use Pu239 and Pu240 as our features.\n",
    "1. Select only columns \"Pu239\" and \"Pu240\" to use for our example.\n",
    "1. Plot it to see how it looks.\n",
    "1. Initialize an instance of `KMeans` from `sklearn`.\n",
    "1. Fit our sliced dataframe with the `fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Run K-Means against our 2 features with 3 clusters\n",
    "\n",
    "# 2. Assign clusters back to our dataframe\n",
    "\n",
    "# 3. Get our centroids\n",
    "\n",
    "# 4. Plot the scatter of our points with calculated centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='scaling'></a>\n",
    "### Try standardizing the data first and see how it affects the cluster assignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Repeat the same as above on scaled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='conclusion'></a>\n",
    "## Conclusion: K-Means tradeoffs\n",
    "\n",
    "---\n",
    "\n",
    "**K-Means:**\n",
    "- Unsupervised clustering model\n",
    "- Similar to KNN (but for “clustering”)\n",
    "- Iteratively finds labels given K\n",
    "- Easy to implement in sklearn\n",
    "- Sensitive to shape, scale of data\n",
    "- Optimal K hard to evaluate\n",
    "\n",
    "---\n",
    "\n",
    "| Strengths | Weaknesses |\n",
    "| -- | -- |\n",
    "| K-Means is popular because it's simple and computationally efficient. | However, K-Means is highly scale dependent and isn't suitable for data of varying shapes and densities. |\n",
    "| Easy to see results / intuitive. | Evaluating results is more subjective, requiring much more human evaluation than trusted metrics. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='resources'></a>\n",
    "\n",
    "## Additional resources\n",
    "\n",
    "---\n",
    "\n",
    "- Andrew Moore's CS class at Carnegie Mellon contains good static visualization, step-by-step. His slide deck is online here: http://www.cs.cmu.edu/~cga/ai-course/kmeans.pdf. He also links to more of his tutorials on the first page. \n",
    "- [Sci-Kit Learn Clustering Overview](http://scikit-learn.org/stable/modules/clustering.html#k-means)\n",
    "- [SKLearn K-Means Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)\n",
    "- [SKLearn Clustering Code - See _k_init__ for explanation of k++ centroid selection](https://github.com/scikit-learn/scikit-learn/blob/51a765a/sklearn/cluster/k_means_.py#L769)\n",
    "- [Clustering Tutorial](http://home.deib.polimi.it/matteucc/Clustering/tutorial_html/)\n",
    "- [Wikipedia's Deep Dive on Clustering](https://en.wikipedia.org/wiki/K-means_clustering)\n",
    "\n",
    "\n",
    "**Some helpful stackexchange questions:**\n",
    "- http://stats.stackexchange.com/questions/40613/why-dont-dummy-variables-have-the-continuous-adjacent-category-problem-in-clust\n",
    "- http://stats.stackexchange.com/questions/174556/k-means-clustering-with-dummy-variables\n",
    "- http://datascience.stackexchange.com/questions/22/k-means-clustering-for-mixed-numeric-and-categorical-data\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
