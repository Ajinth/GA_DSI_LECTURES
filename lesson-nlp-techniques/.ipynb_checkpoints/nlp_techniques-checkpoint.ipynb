{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import wikipedia\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Review Lab (50 Minutes)\n",
    "\n",
    "There are a lot of moving pieces in NLP and it is worthwhile to keep practicing the techniques we started to acquire yesterday. \n",
    "\n",
    "The first section of our lesson today will be a chance to review those topics and to practice discussing NLP and machine learning together. \n",
    "\n",
    "We'll be using a truncated version of the [Amazon Fine Food Review](https://www.kaggle.com/snap/amazon-fine-food-reviews/data) dataset. For a larger project, we would make use of the full set of data. However, in the interest of processing time, we'll use a randomly sampled set of 10,000 reviews for our training set and an additional 2,000 reviews for our test set.\n",
    "\n",
    "Your goal will be to create a predictive model that classifies a review into a high scoring review (5 stars) or not a high scoring review (1-4 stars). This value is already present in the data under the name `high_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>high_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>177748</td>\n",
       "      <td>O. Brown \"Ms. O. Khannah-Brown\"</td>\n",
       "      <td>5</td>\n",
       "      <td>1190160000</td>\n",
       "      <td>Organic, Kosher, Tasty Assortment of Premium T...</td>\n",
       "      <td>*****&lt;br /&gt;Numi's Collection Assortment Melang...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>155411</td>\n",
       "      <td>Evon Schones \"ACMEFit\"</td>\n",
       "      <td>5</td>\n",
       "      <td>1252454400</td>\n",
       "      <td>YUM!!</td>\n",
       "      <td>I love this product!  We have replaced all oth...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>232693</td>\n",
       "      <td>Kerry Hart</td>\n",
       "      <td>4</td>\n",
       "      <td>1261699200</td>\n",
       "      <td>Good item for when I ate it....</td>\n",
       "      <td>I no longer eat kashi,,,too much sugar. But wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>364352</td>\n",
       "      <td>Leonardo Rafael Camargo \"colombiaaaa\"</td>\n",
       "      <td>3</td>\n",
       "      <td>1214006400</td>\n",
       "      <td>an acquired taste, then it's good</td>\n",
       "      <td>it's not your typical pretzel. I don't eat muc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>491273</td>\n",
       "      <td>D. B. James</td>\n",
       "      <td>1</td>\n",
       "      <td>1348790400</td>\n",
       "      <td>WARNING:  High Fructose Corn Syrup</td>\n",
       "      <td>WARNING:  HIGH FRUCTOSE CORN SYRUP in this pro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id                            ProfileName  Score        Time  \\\n",
       "0  177748        O. Brown \"Ms. O. Khannah-Brown\"      5  1190160000   \n",
       "1  155411                 Evon Schones \"ACMEFit\"      5  1252454400   \n",
       "2  232693                             Kerry Hart      4  1261699200   \n",
       "3  364352  Leonardo Rafael Camargo \"colombiaaaa\"      3  1214006400   \n",
       "4  491273                            D. B. James      1  1348790400   \n",
       "\n",
       "                                             Summary  \\\n",
       "0  Organic, Kosher, Tasty Assortment of Premium T...   \n",
       "1                                              YUM!!   \n",
       "2                    Good item for when I ate it....   \n",
       "3                  an acquired taste, then it's good   \n",
       "4                 WARNING:  High Fructose Corn Syrup   \n",
       "\n",
       "                                                Text  high_score  \n",
       "0  *****<br />Numi's Collection Assortment Melang...           1  \n",
       "1  I love this product!  We have replaced all oth...           1  \n",
       "2  I no longer eat kashi,,,too much sugar. But wh...           0  \n",
       "3  it's not your typical pretzel. I don't eat muc...           0  \n",
       "4  WARNING:  HIGH FRUCTOSE CORN SYRUP in this pro...           0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('./datasets/amazon_train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>high_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>202095</td>\n",
       "      <td>heather fox</td>\n",
       "      <td>5</td>\n",
       "      <td>1247097600</td>\n",
       "      <td>annies bunnies</td>\n",
       "      <td>great deal, these are so yummy , we love them ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>114978</td>\n",
       "      <td>kar</td>\n",
       "      <td>5</td>\n",
       "      <td>1328054400</td>\n",
       "      <td>Love it!!</td>\n",
       "      <td>I enjoy this snack and my son loves it too! It...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>287940</td>\n",
       "      <td>cherina hirsh</td>\n",
       "      <td>5</td>\n",
       "      <td>1281312000</td>\n",
       "      <td>Garlic always</td>\n",
       "      <td>This product is just fantastic for anyone who ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67955</td>\n",
       "      <td>Lucy Dashwood \"Lucy Dashwood\"</td>\n",
       "      <td>5</td>\n",
       "      <td>1321574400</td>\n",
       "      <td>Delicious</td>\n",
       "      <td>The holidays wouldn't be festive without chest...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>454664</td>\n",
       "      <td>brenda peppers</td>\n",
       "      <td>5</td>\n",
       "      <td>1343088000</td>\n",
       "      <td>Toy Poodles Best Meal!</td>\n",
       "      <td>My little poddles love this brand and it's nic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id                    ProfileName  Score        Time  \\\n",
       "0  202095                    heather fox      5  1247097600   \n",
       "1  114978                            kar      5  1328054400   \n",
       "2  287940                  cherina hirsh      5  1281312000   \n",
       "3   67955  Lucy Dashwood \"Lucy Dashwood\"      5  1321574400   \n",
       "4  454664                 brenda peppers      5  1343088000   \n",
       "\n",
       "                  Summary                                               Text  \\\n",
       "0          annies bunnies  great deal, these are so yummy , we love them ...   \n",
       "1               Love it!!  I enjoy this snack and my son loves it too! It...   \n",
       "2           Garlic always  This product is just fantastic for anyone who ...   \n",
       "3               Delicious  The holidays wouldn't be festive without chest...   \n",
       "4  Toy Poodles Best Meal!  My little poddles love this brand and it's nic...   \n",
       "\n",
       "   high_score  \n",
       "0           1  \n",
       "1           1  \n",
       "2           1  \n",
       "3           1  \n",
       "4           1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('./datasets/amazon_test.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1272\n",
       "0     728\n",
       "Name: high_score, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.high_score.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into pairs and work together to do the following.\n",
    "\n",
    "#### Model Generation (30 Minutes)\n",
    "\n",
    "1. Try and create a predictive model that identifies whether a review will be a high-scoring review or not (`high_score` feature in the data). While you can use any of the NLP techniques we discussed yesterday, here are some areas to focus on:\n",
    "\n",
    "1. Should you use `CountVectorizer` or `TfidfVectorizer` to transform your DataFrame?\n",
    "    - Keep stop words or drop them?\n",
    "    - Limit the words going in using `max_df` or `min_df`?\n",
    "2. Apply dimensionality reduction using `TruncatedSVD` or not?\n",
    "    - If you do, how many components should you keep?\n",
    "3. What modeling technique should you use? (`LogisticRegression`, `RandomForestClassifier`, etc.?) How will you change the hyperparameters.\n",
    "\n",
    "Make sure that you are checking your model's performance against the test set.\n",
    "\n",
    "#### Discussion (10 Minutes)\n",
    "\n",
    "A pair from each market will come on mic and discuss how they've chosen to transform their data. Additionally, we'll compare the **mean accuracy** for each market to see who has (at this point) made the most predictive model.\n",
    "\n",
    "#### Model Refinement (10 Minutes)\n",
    "\n",
    "Continue to refine your model or include some choices made by other markets. At the end of these 10 minutes, we'll report each market's best finding (and final model) by mic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = train['Text']\n",
    "y = train['high_score']\n",
    "\n",
    "Xn = test['Text']\n",
    "yn =test['high_score']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "[CV] logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l1 .........\n",
      "[CV] logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l1 .........\n",
      "[CV] logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l1 .........\n",
      "[CV] logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l1, total=   0.9s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l1, total=   0.9s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l1, total=   0.9s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l2, total=   0.9s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l2, total=   0.9s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l1, total=   0.9s\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l2, total=   0.9s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l2 .........\n",
      "[CV] logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l1, total=   0.8s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l1, total=   0.9s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l2, total=   0.9s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l2, total=   1.0s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l2, total=   1.3s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l1, total=   1.1s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l1, total=   1.0s\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l1, total=   1.0s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l2 .........\n",
      "[CV] logreg__C=10, logreg__max_iter=100, logreg__penalty=l1 ..........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l2, total=   0.9s\n",
      "[CV] logreg__C=10, logreg__max_iter=100, logreg__penalty=l1 ..........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l2, total=   0.9s\n",
      "[CV] logreg__C=10, logreg__max_iter=100, logreg__penalty=l1 ..........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l2, total=   0.9s\n",
      "[CV] logreg__C=10, logreg__max_iter=100, logreg__penalty=l2 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=100, logreg__penalty=l1, total=   0.9s\n",
      "[CV] logreg__C=10, logreg__max_iter=100, logreg__penalty=l2 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=100, logreg__penalty=l1, total=   0.9s\n",
      "[CV] logreg__C=10, logreg__max_iter=100, logreg__penalty=l2 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=100, logreg__penalty=l1, total=   1.0s\n",
      "[CV] logreg__C=10, logreg__max_iter=150, logreg__penalty=l1 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=100, logreg__penalty=l2, total=   1.0s\n",
      "[CV] logreg__C=10, logreg__max_iter=150, logreg__penalty=l1 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=100, logreg__penalty=l2, total=   0.9s\n",
      "[CV] logreg__C=10, logreg__max_iter=150, logreg__penalty=l1 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=100, logreg__penalty=l2, total=   0.9s\n",
      "[CV] logreg__C=10, logreg__max_iter=150, logreg__penalty=l2 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=150, logreg__penalty=l1, total=   1.2s\n",
      "[CV] logreg__C=10, logreg__max_iter=150, logreg__penalty=l2 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=150, logreg__penalty=l1, total=   1.1s\n",
      "[CV] logreg__C=10, logreg__max_iter=150, logreg__penalty=l2 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=150, logreg__penalty=l1, total=   1.0s\n",
      "[CV] logreg__C=10, logreg__max_iter=200, logreg__penalty=l1 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=150, logreg__penalty=l2, total=   0.9s\n",
      "[CV] logreg__C=10, logreg__max_iter=200, logreg__penalty=l1 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=150, logreg__penalty=l2, total=   1.1s\n",
      "[CV] logreg__C=10, logreg__max_iter=200, logreg__penalty=l1 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=150, logreg__penalty=l2, total=   1.1s\n",
      "[CV] logreg__C=10, logreg__max_iter=200, logreg__penalty=l2 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=200, logreg__penalty=l1, total=   1.0s\n",
      "[CV] logreg__C=10, logreg__max_iter=200, logreg__penalty=l2 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=200, logreg__penalty=l1, total=   0.9s\n",
      "[CV] logreg__C=10, logreg__max_iter=200, logreg__penalty=l2 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=200, logreg__penalty=l1, total=   1.1s\n",
      "[CV] logreg__C=100, logreg__max_iter=100, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=10, logreg__max_iter=200, logreg__penalty=l2, total=   1.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   14.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] logreg__C=100, logreg__max_iter=100, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=10, logreg__max_iter=200, logreg__penalty=l2, total=   1.2s\n",
      "[CV] logreg__C=100, logreg__max_iter=100, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=10, logreg__max_iter=200, logreg__penalty=l2, total=   1.5s\n",
      "[CV] logreg__C=100, logreg__max_iter=100, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=100, logreg__penalty=l1, total=   1.3s\n",
      "[CV] logreg__C=100, logreg__max_iter=100, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=100, logreg__penalty=l1, total=   1.3s\n",
      "[CV] logreg__C=100, logreg__max_iter=100, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=100, logreg__penalty=l1, total=   1.3s\n",
      "[CV] logreg__C=100, logreg__max_iter=150, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=100, logreg__penalty=l2, total=   2.1s\n",
      "[CV] logreg__C=100, logreg__max_iter=150, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=100, logreg__penalty=l2, total=   1.2s\n",
      "[CV] logreg__C=100, logreg__max_iter=150, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=100, logreg__penalty=l2, total=   1.3s\n",
      "[CV] logreg__C=100, logreg__max_iter=150, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=150, logreg__penalty=l1, total=   1.3s\n",
      "[CV] logreg__C=100, logreg__max_iter=150, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=150, logreg__penalty=l1, total=   1.2s\n",
      "[CV] logreg__C=100, logreg__max_iter=150, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=150, logreg__penalty=l1, total=   1.1s\n",
      "[CV] logreg__C=100, logreg__max_iter=200, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=150, logreg__penalty=l2, total=   1.0s\n",
      "[CV] logreg__C=100, logreg__max_iter=200, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=150, logreg__penalty=l2, total=   1.0s\n",
      "[CV] logreg__C=100, logreg__max_iter=200, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=150, logreg__penalty=l2, total=   1.1s\n",
      "[CV] logreg__C=100, logreg__max_iter=200, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=200, logreg__penalty=l1, total=   1.1s\n",
      "[CV] logreg__C=100, logreg__max_iter=200, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=200, logreg__penalty=l1, total=   1.1s\n",
      "[CV] logreg__C=100, logreg__max_iter=200, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=200, logreg__penalty=l1, total=   0.9s\n",
      "[CV]  logreg__C=100, logreg__max_iter=200, logreg__penalty=l2, total=   1.0s\n",
      "[CV]  logreg__C=100, logreg__max_iter=200, logreg__penalty=l2, total=   0.9s\n",
      "[CV]  logreg__C=100, logreg__max_iter=200, logreg__penalty=l2, total=   0.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:   23.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params:  {'logreg__C': 1.0, 'logreg__max_iter': 100, 'logreg__penalty': 'l2'}\n",
      "Best Score:  0.76975\n",
      "Train-Train Score:  0.861625\n",
      "Train-Test Score:  0.783\n",
      "Real Test Score:  0.776\n",
      "Confusion Matrix \n",
      "\n",
      "[[ 394  334]\n",
      " [ 114 1158]]\n",
      "Classification Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.54      0.64       728\n",
      "          1       0.78      0.91      0.84      1272\n",
      "\n",
      "avg / total       0.78      0.78      0.76      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''Setting up the Targets and Features '''\n",
    "X = train['Text']\n",
    "y = train['high_score']\n",
    "\n",
    "Xn = test['Text']\n",
    "yn =test['high_score']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2)\n",
    "\n",
    "\n",
    "'''Setting up the TFIDVectorizer'''\n",
    "tf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "'''Intantiating Logreg'''\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "\n",
    "'''Setting up the Parameters for GridSearch'''\n",
    "params = {\n",
    "\n",
    "    'logreg__penalty': ['l1', 'l2'], \n",
    "    'logreg__C': [1.0,10,100], \n",
    "    'logreg__max_iter': [100,150,200]   \n",
    "}\n",
    "\n",
    "'''Setting the Pipeline'''\n",
    "logreg_tk_pipe = Pipeline([('vect', tf), \n",
    "                     ('logreg', logreg)])\n",
    "\n",
    "'''Fitting the Model on Training'''\n",
    "gs_logreg = GridSearchCV(logreg_tk_pipe, param_grid=params,n_jobs = -1, verbose=2, scoring='accuracy')\n",
    "gs_logreg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print ('Best Params: ' , gs_logreg.best_params_) \n",
    "print ('Best Score: ', gs_logreg.best_score_)\n",
    "print ('Train-Train Score: ',gs_logreg.score(X_train, y_train))\n",
    "print ('Train-Test Score: ', gs_logreg.score(X_test, y_test))\n",
    "print ('Real Test Score: ', gs_logreg.score(Xn, yn))\n",
    "print ('Confusion Matrix \\n')\n",
    "print (confusion_matrix(yn, gs_logreg.predict(Xn)))\n",
    "print ('Classification Report\\n')\n",
    "print (classification_report(yn, gs_logreg.predict(Xn)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params:  {'logreg__C': 100, 'logreg__max_iter': 100, 'logreg__penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "'''Best Parameters'''\n",
    "print ('Best Params: ' , gs_logreg.best_params_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:  0.657\n"
     ]
    }
   ],
   "source": [
    "'''Best Score'''\n",
    "print ('Best Score: ', gs_logreg.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Train Score:  0.66425\n"
     ]
    }
   ],
   "source": [
    "'''Training Score'''\n",
    "print ('Train-Train Score: ',gs_logreg.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Test Score:  0.656\n"
     ]
    }
   ],
   "source": [
    "'''Training Test Score'''\n",
    "print ('Train-Test Score: ', gs_logreg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.658\n"
     ]
    }
   ],
   "source": [
    "'''Real Test Score'''\n",
    "print (gs_logreg.score(Xn, yn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 218  510]\n",
      " [ 174 1098]]\n"
     ]
    }
   ],
   "source": [
    "'''Confusion Matrix'''\n",
    "print (confusion_matrix(yn, gs_logreg.predict(Xn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.30      0.39       728\n",
      "          1       0.68      0.86      0.76      1272\n",
      "\n",
      "avg / total       0.64      0.66      0.63      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''Classification Report'''\n",
    "print (classification_report(yn, gs_logreg.predict(Xn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying the CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "[CV] logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l1 .........\n",
      "[CV] logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l1 .........\n",
      "[CV] logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l1 .........\n",
      "[CV] logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l1, total=   1.0s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l1, total=   1.1s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l1, total=   1.0s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l2, total=   2.1s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l1, total=   0.8s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l2, total=   1.2s\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=100, logreg__penalty=l2, total=   1.1s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l2 .........\n",
      "[CV] logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l1, total=   1.2s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l1, total=   1.0s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l2, total=   1.4s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l2, total=   1.4s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=150, logreg__penalty=l2, total=   1.4s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l1, total=   1.1s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l1, total=   1.0s\n",
      "[CV] logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l1, total=   1.0s\n",
      "[CV] logreg__C=10, logreg__max_iter=100, logreg__penalty=l1 ..........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l2, total=   1.3s\n",
      "[CV] logreg__C=10, logreg__max_iter=100, logreg__penalty=l1 ..........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l2, total=   1.2s\n",
      "[CV] logreg__C=10, logreg__max_iter=100, logreg__penalty=l1 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=100, logreg__penalty=l1, total=   0.9s\n",
      "[CV] logreg__C=10, logreg__max_iter=100, logreg__penalty=l2 ..........\n",
      "[CV]  logreg__C=1.0, logreg__max_iter=200, logreg__penalty=l2, total=   1.0s\n",
      "[CV] logreg__C=10, logreg__max_iter=100, logreg__penalty=l2 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=100, logreg__penalty=l1, total=   1.4s\n",
      "[CV] logreg__C=10, logreg__max_iter=100, logreg__penalty=l2 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=100, logreg__penalty=l1, total=   1.2s\n",
      "[CV] logreg__C=10, logreg__max_iter=150, logreg__penalty=l1 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=100, logreg__penalty=l2, total=   1.6s\n",
      "[CV] logreg__C=10, logreg__max_iter=150, logreg__penalty=l1 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=100, logreg__penalty=l2, total=   1.7s\n",
      "[CV] logreg__C=10, logreg__max_iter=150, logreg__penalty=l1 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=150, logreg__penalty=l1, total=   1.0s\n",
      "[CV] logreg__C=10, logreg__max_iter=150, logreg__penalty=l2 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=100, logreg__penalty=l2, total=   1.6s\n",
      "[CV] logreg__C=10, logreg__max_iter=150, logreg__penalty=l2 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=150, logreg__penalty=l1, total=   1.1s\n",
      "[CV] logreg__C=10, logreg__max_iter=150, logreg__penalty=l2 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=150, logreg__penalty=l1, total=   1.1s\n",
      "[CV] logreg__C=10, logreg__max_iter=200, logreg__penalty=l1 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=150, logreg__penalty=l2, total=   1.8s\n",
      "[CV] logreg__C=10, logreg__max_iter=200, logreg__penalty=l1 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=150, logreg__penalty=l2, total=   1.6s\n",
      "[CV] logreg__C=10, logreg__max_iter=200, logreg__penalty=l1 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=200, logreg__penalty=l1, total=   1.0s\n",
      "[CV] logreg__C=10, logreg__max_iter=200, logreg__penalty=l2 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=150, logreg__penalty=l2, total=   1.4s\n",
      "[CV] logreg__C=10, logreg__max_iter=200, logreg__penalty=l2 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=200, logreg__penalty=l1, total=   1.2s\n",
      "[CV] logreg__C=10, logreg__max_iter=200, logreg__penalty=l2 ..........\n",
      "[CV]  logreg__C=10, logreg__max_iter=200, logreg__penalty=l1, total=   1.1s\n",
      "[CV] logreg__C=100, logreg__max_iter=100, logreg__penalty=l1 .........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   16.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  logreg__C=10, logreg__max_iter=200, logreg__penalty=l2, total=   1.5s\n",
      "[CV] logreg__C=100, logreg__max_iter=100, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=10, logreg__max_iter=200, logreg__penalty=l2, total=   1.7s\n",
      "[CV] logreg__C=100, logreg__max_iter=100, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=100, logreg__penalty=l1, total=   1.1s\n",
      "[CV] logreg__C=100, logreg__max_iter=100, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=10, logreg__max_iter=200, logreg__penalty=l2, total=   1.6s\n",
      "[CV] logreg__C=100, logreg__max_iter=100, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=100, logreg__penalty=l1, total=   1.0s\n",
      "[CV] logreg__C=100, logreg__max_iter=100, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=100, logreg__penalty=l1, total=   1.2s\n",
      "[CV] logreg__C=100, logreg__max_iter=150, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=100, logreg__penalty=l2, total=   1.6s\n",
      "[CV] logreg__C=100, logreg__max_iter=150, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=100, logreg__penalty=l2, total=   1.5s\n",
      "[CV] logreg__C=100, logreg__max_iter=150, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=150, logreg__penalty=l1, total=   0.9s\n",
      "[CV] logreg__C=100, logreg__max_iter=150, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=100, logreg__penalty=l2, total=   1.5s\n",
      "[CV] logreg__C=100, logreg__max_iter=150, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=150, logreg__penalty=l1, total=   1.2s\n",
      "[CV] logreg__C=100, logreg__max_iter=150, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=150, logreg__penalty=l1, total=   1.1s\n",
      "[CV] logreg__C=100, logreg__max_iter=200, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=150, logreg__penalty=l2, total=   1.5s\n",
      "[CV] logreg__C=100, logreg__max_iter=200, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=150, logreg__penalty=l2, total=   1.7s\n",
      "[CV] logreg__C=100, logreg__max_iter=200, logreg__penalty=l1 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=200, logreg__penalty=l1, total=   1.1s\n",
      "[CV] logreg__C=100, logreg__max_iter=200, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=200, logreg__penalty=l1, total=   1.1s\n",
      "[CV] logreg__C=100, logreg__max_iter=200, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=150, logreg__penalty=l2, total=   2.2s\n",
      "[CV] logreg__C=100, logreg__max_iter=200, logreg__penalty=l2 .........\n",
      "[CV]  logreg__C=100, logreg__max_iter=200, logreg__penalty=l1, total=   1.1s\n",
      "[CV]  logreg__C=100, logreg__max_iter=200, logreg__penalty=l2, total=   1.5s\n",
      "[CV]  logreg__C=100, logreg__max_iter=200, logreg__penalty=l2, total=   1.1s\n",
      "[CV]  logreg__C=100, logreg__max_iter=200, logreg__penalty=l2, total=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:   25.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'logreg__penalty': ['l1', 'l2'], 'logreg__C': [1.0, 10, 100], 'logreg__max_iter': [100, 150, 200]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Setting up the TFIDVectorizer'''\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "\n",
    "'''Intantiating Logreg'''\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "\n",
    "'''Setting up the Parameters for GridSearch'''\n",
    "params = {\n",
    "\n",
    "    'logreg__penalty': ['l1', 'l2'], \n",
    "    'logreg__C': [1.0,10,100], \n",
    "    'logreg__max_iter': [100,150,200]   \n",
    "}\n",
    "\n",
    "'''Setting the Pipeline'''\n",
    "logreg_tk_pipe = Pipeline([('vect', cv), \n",
    "                     ('logreg', logreg)])\n",
    "\n",
    "'''Fitting the Model on Training'''\n",
    "gs_logreg = GridSearchCV(logreg_tk_pipe, param_grid=params,n_jobs = -1, verbose=2, scoring='accuracy')\n",
    "gs_logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logreg__C': 1.0, 'logreg__max_iter': 100, 'logreg__penalty': 'l2'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Best Params'''\n",
    "gs_logreg.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75675000000000003"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Best Score'''\n",
    "gs_logreg.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96299999999999997"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Training Score'''\n",
    "gs_logreg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.746"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Test Score'''\n",
    "gs_logreg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76700000000000002"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Real Test Score'''\n",
    "gs_logreg.score(Xn, yn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 463,  265],\n",
       "       [ 201, 1071]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Confusion Matrix'''\n",
    "confusion_matrix(yn, gs_logreg.predict(Xn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[CV] rfc__max_features=20, rfc__n_estimators=10 ......................\n",
      "[CV] rfc__max_features=20, rfc__n_estimators=10 ......................\n",
      "[CV] rfc__max_features=20, rfc__n_estimators=10 ......................\n",
      "[CV] rfc__max_features=20, rfc__n_estimators=25 ......................\n",
      "[CV] ....... rfc__max_features=20, rfc__n_estimators=10, total=   1.4s\n",
      "[CV] rfc__max_features=20, rfc__n_estimators=25 ......................\n",
      "[CV] ....... rfc__max_features=20, rfc__n_estimators=10, total=   1.4s\n",
      "[CV] rfc__max_features=20, rfc__n_estimators=25 ......................\n",
      "[CV] ....... rfc__max_features=20, rfc__n_estimators=10, total=   1.4s\n",
      "[CV] rfc__max_features=20, rfc__n_estimators=50 ......................\n",
      "[CV] ....... rfc__max_features=20, rfc__n_estimators=25, total=   2.1s\n",
      "[CV] rfc__max_features=20, rfc__n_estimators=50 ......................\n",
      "[CV] ....... rfc__max_features=20, rfc__n_estimators=25, total=   2.0s\n",
      "[CV] ....... rfc__max_features=20, rfc__n_estimators=25, total=   1.9s\n",
      "[CV] rfc__max_features=20, rfc__n_estimators=50 ......................\n",
      "[CV] rfc__max_features=20, rfc__n_estimators=75 ......................\n",
      "[CV] ....... rfc__max_features=20, rfc__n_estimators=50, total=   2.9s\n",
      "[CV] rfc__max_features=20, rfc__n_estimators=75 ......................\n",
      "[CV] ....... rfc__max_features=20, rfc__n_estimators=50, total=   2.9s\n",
      "[CV] rfc__max_features=20, rfc__n_estimators=75 ......................\n",
      "[CV] ....... rfc__max_features=20, rfc__n_estimators=50, total=   2.9s\n",
      "[CV] rfc__max_features=20, rfc__n_estimators=100 .....................\n",
      "[CV] ....... rfc__max_features=20, rfc__n_estimators=75, total=   4.0s\n",
      "[CV] rfc__max_features=20, rfc__n_estimators=100 .....................\n",
      "[CV] ....... rfc__max_features=20, rfc__n_estimators=75, total=   4.0s\n",
      "[CV] rfc__max_features=20, rfc__n_estimators=100 .....................\n",
      "[CV] ....... rfc__max_features=20, rfc__n_estimators=75, total=   4.0s\n",
      "[CV] rfc__max_features=30, rfc__n_estimators=10 ......................\n",
      "[CV] ....... rfc__max_features=30, rfc__n_estimators=10, total=   1.2s\n",
      "[CV] rfc__max_features=30, rfc__n_estimators=10 ......................\n",
      "[CV] ...... rfc__max_features=20, rfc__n_estimators=100, total=   5.0s\n",
      "[CV] rfc__max_features=30, rfc__n_estimators=10 ......................\n",
      "[CV] ....... rfc__max_features=30, rfc__n_estimators=10, total=   1.3s\n",
      "[CV] rfc__max_features=30, rfc__n_estimators=25 ......................\n",
      "[CV] ...... rfc__max_features=20, rfc__n_estimators=100, total=   5.1s\n",
      "[CV] rfc__max_features=30, rfc__n_estimators=25 ......................\n",
      "[CV] ....... rfc__max_features=30, rfc__n_estimators=10, total=   1.2s\n",
      "[CV] rfc__max_features=30, rfc__n_estimators=25 ......................\n",
      "[CV] ...... rfc__max_features=20, rfc__n_estimators=100, total=   5.3s\n",
      "[CV] rfc__max_features=30, rfc__n_estimators=50 ......................\n",
      "[CV] ....... rfc__max_features=30, rfc__n_estimators=25, total=   2.2s\n",
      "[CV] rfc__max_features=30, rfc__n_estimators=50 ......................\n",
      "[CV] ....... rfc__max_features=30, rfc__n_estimators=25, total=   2.2s\n",
      "[CV] rfc__max_features=30, rfc__n_estimators=50 ......................\n",
      "[CV] ....... rfc__max_features=30, rfc__n_estimators=25, total=   2.0s\n",
      "[CV] rfc__max_features=30, rfc__n_estimators=75 ......................\n",
      "[CV] ....... rfc__max_features=30, rfc__n_estimators=50, total=   3.4s\n",
      "[CV] rfc__max_features=30, rfc__n_estimators=75 ......................\n",
      "[CV] ....... rfc__max_features=30, rfc__n_estimators=50, total=   3.5s\n",
      "[CV] rfc__max_features=30, rfc__n_estimators=75 ......................\n",
      "[CV] ....... rfc__max_features=30, rfc__n_estimators=50, total=   3.5s\n",
      "[CV] rfc__max_features=30, rfc__n_estimators=100 .....................\n",
      "[CV] ....... rfc__max_features=30, rfc__n_estimators=75, total=   4.5s\n",
      "[CV] rfc__max_features=30, rfc__n_estimators=100 .....................\n",
      "[CV] ....... rfc__max_features=30, rfc__n_estimators=75, total=   4.2s\n",
      "[CV] rfc__max_features=30, rfc__n_estimators=100 .....................\n",
      "[CV] ....... rfc__max_features=30, rfc__n_estimators=75, total=   4.1s\n",
      "[CV] rfc__max_features=40, rfc__n_estimators=10 ......................\n",
      "[CV] ...... rfc__max_features=30, rfc__n_estimators=100, total=   5.2s\n",
      "[CV] ....... rfc__max_features=40, rfc__n_estimators=10, total=   1.2s\n",
      "[CV] rfc__max_features=40, rfc__n_estimators=10 ......................\n",
      "[CV] rfc__max_features=40, rfc__n_estimators=10 ......................\n",
      "[CV] ...... rfc__max_features=30, rfc__n_estimators=100, total=   5.3s\n",
      "[CV] rfc__max_features=40, rfc__n_estimators=25 ......................\n",
      "[CV] ....... rfc__max_features=40, rfc__n_estimators=10, total=   1.3s\n",
      "[CV] rfc__max_features=40, rfc__n_estimators=25 ......................\n",
      "[CV] ....... rfc__max_features=40, rfc__n_estimators=10, total=   1.2s\n",
      "[CV] rfc__max_features=40, rfc__n_estimators=25 ......................\n",
      "[CV] ...... rfc__max_features=30, rfc__n_estimators=100, total=   5.4s\n",
      "[CV] rfc__max_features=40, rfc__n_estimators=50 ......................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   32.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....... rfc__max_features=40, rfc__n_estimators=25, total=   1.9s\n",
      "[CV] rfc__max_features=40, rfc__n_estimators=50 ......................\n",
      "[CV] ....... rfc__max_features=40, rfc__n_estimators=25, total=   2.0s\n",
      "[CV] rfc__max_features=40, rfc__n_estimators=50 ......................\n",
      "[CV] ....... rfc__max_features=40, rfc__n_estimators=25, total=   2.0s\n",
      "[CV] rfc__max_features=40, rfc__n_estimators=75 ......................\n",
      "[CV] ....... rfc__max_features=40, rfc__n_estimators=50, total=   3.1s\n",
      "[CV] rfc__max_features=40, rfc__n_estimators=75 ......................\n",
      "[CV] ....... rfc__max_features=40, rfc__n_estimators=50, total=   3.1s\n",
      "[CV] rfc__max_features=40, rfc__n_estimators=75 ......................\n",
      "[CV] ....... rfc__max_features=40, rfc__n_estimators=50, total=   3.0s\n",
      "[CV] rfc__max_features=40, rfc__n_estimators=100 .....................\n",
      "[CV] ....... rfc__max_features=40, rfc__n_estimators=75, total=   4.1s\n",
      "[CV] rfc__max_features=40, rfc__n_estimators=100 .....................\n",
      "[CV] ....... rfc__max_features=40, rfc__n_estimators=75, total=   4.3s\n",
      "[CV] rfc__max_features=40, rfc__n_estimators=100 .....................\n",
      "[CV] ....... rfc__max_features=40, rfc__n_estimators=75, total=   4.2s\n",
      "[CV] rfc__max_features=50, rfc__n_estimators=10 ......................\n",
      "[CV] ...... rfc__max_features=40, rfc__n_estimators=100, total=   5.2s\n",
      "[CV] rfc__max_features=50, rfc__n_estimators=10 ......................\n",
      "[CV] ....... rfc__max_features=50, rfc__n_estimators=10, total=   1.2s\n",
      "[CV] rfc__max_features=50, rfc__n_estimators=10 ......................\n",
      "[CV] ...... rfc__max_features=40, rfc__n_estimators=100, total=   5.3s\n",
      "[CV] rfc__max_features=50, rfc__n_estimators=25 ......................\n",
      "[CV] ....... rfc__max_features=50, rfc__n_estimators=10, total=   1.3s\n",
      "[CV] rfc__max_features=50, rfc__n_estimators=25 ......................\n",
      "[CV] ....... rfc__max_features=50, rfc__n_estimators=10, total=   1.5s\n",
      "[CV] rfc__max_features=50, rfc__n_estimators=25 ......................\n",
      "[CV] ....... rfc__max_features=50, rfc__n_estimators=25, total=   2.7s\n",
      "[CV] rfc__max_features=50, rfc__n_estimators=50 ......................\n",
      "[CV] ....... rfc__max_features=50, rfc__n_estimators=25, total=   2.8s\n",
      "[CV] rfc__max_features=50, rfc__n_estimators=50 ......................\n",
      "[CV] ...... rfc__max_features=40, rfc__n_estimators=100, total=   6.4s\n",
      "[CV] rfc__max_features=50, rfc__n_estimators=50 ......................\n",
      "[CV] ....... rfc__max_features=50, rfc__n_estimators=25, total=   2.7s\n",
      "[CV] rfc__max_features=50, rfc__n_estimators=75 ......................\n",
      "[CV] ....... rfc__max_features=50, rfc__n_estimators=50, total=   3.9s\n",
      "[CV] rfc__max_features=50, rfc__n_estimators=75 ......................\n",
      "[CV] ....... rfc__max_features=50, rfc__n_estimators=50, total=   4.0s\n",
      "[CV] rfc__max_features=50, rfc__n_estimators=75 ......................\n",
      "[CV] ....... rfc__max_features=50, rfc__n_estimators=50, total=   3.9s\n",
      "[CV] rfc__max_features=50, rfc__n_estimators=100 .....................\n",
      "[CV] ....... rfc__max_features=50, rfc__n_estimators=75, total=   5.5s\n",
      "[CV] rfc__max_features=50, rfc__n_estimators=100 .....................\n",
      "[CV] ....... rfc__max_features=50, rfc__n_estimators=75, total=   5.9s\n",
      "[CV] rfc__max_features=50, rfc__n_estimators=100 .....................\n",
      "[CV] ....... rfc__max_features=50, rfc__n_estimators=75, total=   5.8s\n",
      "[CV] ...... rfc__max_features=50, rfc__n_estimators=100, total=   7.1s\n",
      "[CV] ...... rfc__max_features=50, rfc__n_estimators=100, total=   6.4s\n",
      "[CV] ...... rfc__max_features=50, rfc__n_estimators=100, total=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "  ...imators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'rfc__n_estimators': [10, 25, 50, 75, 100], 'rfc__max_features': [20, 30, 40, 50]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest Classifier \n",
    "\n",
    "'''Setting up the TFIDVectorizer'''\n",
    "\n",
    "tf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "'''Intantiating Logreg'''\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "\n",
    "'''Setting up the Parameters for GridSearch'''\n",
    "params = {\n",
    "\n",
    "    'rfc__n_estimators': [10,25,50,75,100], \n",
    "    'rfc__max_features': [20,30,40,50]   \n",
    "}\n",
    "\n",
    "'''Setting the Pipeline'''\n",
    "logreg_tk_pipe = Pipeline([('vect', tf), \n",
    "                     ('rfc', rfc)])\n",
    "\n",
    "'''Fitting the Model on Training'''\n",
    "gs_logreg = GridSearchCV(logreg_tk_pipe, param_grid=params,n_jobs = -1, verbose=2, scoring='accuracy')\n",
    "gs_logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rfc__max_features': 50, 'rfc__n_estimators': 100}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Best Params'''\n",
    "gs_logreg.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Training Score'''\n",
    "gs_logreg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Techniques\n",
    "\n",
    "Today's lesson is designed as an introduction to more advanced libraries or techniques in the realm of Natural Language Processing. These techniques can help you gain even greater accuracy in your modeling, but require more in-depth knowledge of new libraries, new techniques, etc. \n",
    "\n",
    "While we'll be introducing a lot of new material today, we'll be doing our best to limit the discussion to what is most immediately helpful. Each of these libraries and techniques has much more going on than we have time to discuss this week and we encourage you to spend time investigating and understanding these libraries. However, **mastery of these libraries, techniques, and materials introduced today is not required nor expected.**\n",
    "\n",
    "For Project 4 and your Capstone Project, if you are pursuing an NLP approach, these libraries may be very helpful. However, you can get a lot of mileage out of refining and using the sklearn libraries that we discussed yesterday. A good workflow is to try simple answers first and move into more advanced techniques as your use-case requires -- your goals as modelers should be to make best choice that you can, contingent on time and use-case. Having something work, but not be 100% correct is better than having something 100% correct that doesn't work yet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `spacy` to extract parts of speech and named entities\n",
    "\n",
    "[`spaCy`](https://spacy.io/) is a large-scale NLP and text processing library designed to help you extract useful information from text in a speedy and accurate manner. You can imagine it like `CountVectorizer()` turned up to 11. It has underpinnings to C to increase speed and a focus on usability.\n",
    "\n",
    "`spaCy` does *so* much more than we are able to discuss at this point. It is quickly becoming the go-to library for text processing and feature extraction for text. Today, we'll use it to extract parts of speech and named entities.\n",
    "\n",
    "### Parts of Speech\n",
    "\n",
    "We may want to use some derived statistics about parts of speech in our work as Data Scientists, either as the inputs to a model (document _x_ is _y_% verbs) or to help us modify the inputs to a model (we may want to treat `book` the verb differently than `book` the noun). While many different libraries can do parts of speech (`textblob`, which we'll introduce shortly, can do that as well), we'll introduce this using `spaCy`.\n",
    "\n",
    "First, we set up some text from Wikipedia to parse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chicago ( ( listen) or ), officially the City of Chicago, is the third-most populous city in the United States. With over 2.7 million residents, it is also the most populous city in both the state of Illinois and the Midwestern United States. It is the county seat of Cook County. The Chicago metropolitan area, often referred to as Chicagoland, has nearly 10 million people and is the third-largest in the U.S. Chicago has often been called a global architecture capital. Chicago is considered one o\n"
     ]
    }
   ],
   "source": [
    "chicago = wikipedia.page('chicago')\n",
    "\n",
    "print(chicago.content[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create sentences by splitting on `.`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chicago ( ( listen) or ), officially the City of Chicago, is the third-most populous city in the United States',\n",
       " 'With over 2.7 million residents, it is also the most populous city in both the state of Illinois and the Midwestern United States',\n",
       " 'It is the county seat of Cook County',\n",
       " 'The Chicago metropolitan area, often referred to as Chicagoland, has nearly 10 million people and is the third-largest in the U.S',\n",
       " 'Chicago has often been called a global architecture capital']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chicago_sents = chicago.content.split('. ')\n",
    "chicago_sents[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll set up a model in `spaCy`. This lets `spaCy` know what to use as its internal corpus. We name this model `nlp` by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll feed a sentence into `nlp`. This will automatically split the text into a generator of tokens (one token to each word). These tokens will have the part of speech already tagged in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With over 2.7 million residents, it is also the most populous city in both the state of Illinois and the Midwestern United States\n",
      "With ADP\n",
      "over ADP\n",
      "2.7 NUM\n",
      "million NUM\n",
      "residents NOUN\n",
      ", PUNCT\n",
      "it PRON\n",
      "is VERB\n",
      "also ADV\n",
      "the DET\n",
      "most ADV\n",
      "populous ADJ\n",
      "city NOUN\n",
      "in ADP\n",
      "both CCONJ\n",
      "the DET\n",
      "state NOUN\n",
      "of ADP\n",
      "Illinois PROPN\n",
      "and CCONJ\n",
      "the DET\n",
      "Midwestern PROPN\n",
      "United PROPN\n",
      "States PROPN\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(chicago_sents[1])\n",
    "print(doc)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to convert this into a set of part of speech tags, we could add in a little extra Python to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ADP': 4, 'NUM': 2, 'NOUN': 3, 'PUNCT': 1, 'PRON': 1, 'VERB': 1, 'ADV': 2, 'DET': 3, 'ADJ': 1, 'CCONJ': 2, 'PROPN': 4}\n"
     ]
    }
   ],
   "source": [
    "tags = {}\n",
    "for token in doc:\n",
    "    if token.pos_ not in tags.keys():\n",
    "        tags[token.pos_] = 1\n",
    "    else:\n",
    "        tags[token.pos_] += 1\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many more tags to that `spaCy` can provide for us:\n",
    "\n",
    "- Text: The original word text.\n",
    "- Lemma: The base form of the word.\n",
    "- POS: The simple part-of-speech tag.\n",
    "- Tag: The detailed part-of-speech tag.\n",
    "- Dep: Syntactic dependency, i.e. the relation between tokens.\n",
    "- Shape: The word shape  capitalisation, punctuation, digits.\n",
    "- is alpha: Is the token an alpha character?\n",
    "- is stop: Is the token part of a stop list, i.e. the most common words of the language?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text\tLemma\tPOS\tDetailed POS\tDependency\tShape\tIs alphabetic?\tIs stopword?\n",
      "With\twith\tADP\tIN\tprep\tXxxx\tTrue\tFalse\n",
      "over\tover\tADP\tIN\tquantmod\txxxx\tTrue\tTrue\n",
      "2.7\t2.7\tNUM\tCD\tcompound\td.d\tFalse\tFalse\n",
      "million\tmillion\tNUM\tCD\tnummod\txxxx\tTrue\tFalse\n",
      "residents\tresident\tNOUN\tNNS\tpobj\txxxx\tTrue\tFalse\n",
      ",\t,\tPUNCT\t,\tpunct\t,\tFalse\tFalse\n",
      "it\t-PRON-\tPRON\tPRP\tnsubj\txx\tTrue\tTrue\n",
      "is\tbe\tVERB\tVBZ\tROOT\txx\tTrue\tTrue\n",
      "also\tconjurer\tADV\tRB\tadvmod\txxxx\tTrue\tTrue\n",
      "the\tthe\tDET\tDT\tdet\txxx\tTrue\tTrue\n",
      "most\tmuch\tADV\tRBS\tadvmod\txxxx\tTrue\tTrue\n",
      "populous\tpopulous\tADJ\tJJ\tamod\txxxx\tTrue\tFalse\n",
      "city\tcity\tNOUN\tNN\tattr\txxxx\tTrue\tFalse\n",
      "in\tin\tADP\tIN\tprep\txx\tTrue\tTrue\n",
      "both\tboth\tCCONJ\tCC\tpredet\txxxx\tTrue\tTrue\n",
      "the\tthe\tDET\tDT\tdet\txxx\tTrue\tTrue\n",
      "state\tstate\tNOUN\tNN\tpobj\txxxx\tTrue\tFalse\n",
      "of\tof\tADP\tIN\tprep\txx\tTrue\tTrue\n",
      "Illinois\tillinois\tPROPN\tNNP\tpobj\tXxxxx\tTrue\tFalse\n",
      "and\tand\tCCONJ\tCC\tcc\txxx\tTrue\tTrue\n",
      "the\tthe\tDET\tDT\tdet\txxx\tTrue\tTrue\n",
      "Midwestern\tmidwestern\tPROPN\tNNP\tcompound\tXxxxx\tTrue\tFalse\n",
      "United\tunited\tPROPN\tNNP\tcompound\tXxxxx\tTrue\tFalse\n",
      "States\tstates\tPROPN\tNNP\tconj\tXxxxx\tTrue\tFalse\n"
     ]
    }
   ],
   "source": [
    "print('\\t'.join(['Text', 'Lemma', 'POS', 'Detailed POS', 'Dependency',\n",
    "                'Shape', 'Is alphabetic?', 'Is stopword?']))\n",
    "for token in doc:\n",
    "    print('\\t'.join([token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "          token.shape_, str(token.is_alpha), str(token.is_stop)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check For Understanding 1 (10 minutes)\n",
    "\n",
    "With a partner, do the following:\n",
    "\n",
    "1. Pick two different wikipedia articles\n",
    "2. Get the content using the `wikipedia` library\n",
    "3. Using `spacy`, derive the following:\n",
    "    1. How many tokens are in your article?\n",
    "    2. How many parts of speech are in each article? How often do they occur?\n",
    "    3. As a percentage of the total number of tokens, how often does each part of speech occur?\n",
    "4. Does it look like there's a difference across your documents? What other types of documents would have different distributions of parts of speech?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bb = wikipedia.page('Bill_Belichick')\n",
    "bb_model = nlp(bb.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PROPN': 1170, 'NOUN': 933, 'VERB': 603, 'NUM': 295, 'PUNCT': 831, 'DET': 583, 'ADJ': 384, 'ADP': 693, 'PART': 106, 'ADV': 155, 'PRON': 99, 'SPACE': 127, 'CCONJ': 130, 'SYM': 70, 'X': 15}\n"
     ]
    }
   ],
   "source": [
    "tags = {}\n",
    "for token in bb_model:\n",
    "    if token.pos_ not in tags.keys():\n",
    "        tags[token.pos_] = 1\n",
    "    else:\n",
    "        tags[token.pos_] += 1\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entities\n",
    "\n",
    "Named entities are business, people, countries, or other things that refer to a specific person, place, or thing (think `Apple`, computer manufacturer versus `apple`, delicious crunchy fruit). `spaCy` can identify named entities for us, which we can either highlight or drop from our analyses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've parsed a string of text using `spaCy`, we can call out the named entities using the `.ents` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With over 2.7 million residents, it is also the most populous city in both the state of Illinois and the Midwestern United States <class 'spacy.tokens.doc.Doc'>\n"
     ]
    }
   ],
   "source": [
    "print(doc, type(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over 2.7 million CARDINAL\n",
      "Illinois GPE\n",
      "the Midwestern United States GPE\n"
     ]
    }
   ],
   "source": [
    "for named_entity in doc.ents:\n",
    "    print(named_entity.text, named_entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`spaCy` provides a set of labels for each type of named entity:\n",
    "\n",
    "|Label|Description|\n",
    "|:-- | :-- |\n",
    "|PERSON |\tPeople, including fictional. |\n",
    "|NORP |\tNationalities or religious or political groups. |\n",
    "|FACILITY |\tBuildings, airports, highways, bridges, etc. |\n",
    "|ORG |\tCompanies, agencies, institutions, etc. |\n",
    "|GPE |\tCountries, cities, states. |\n",
    "|LOC |\tNon-GPE locations, mountain ranges, bodies of water. |\n",
    "|PRODUCT |\tObjects, vehicles, foods, etc. (Not services.) |\n",
    "|EVENT |\tNamed hurricanes, battles, wars, sports events, etc. |\n",
    "|WORK_OF_ART |\tTitles of books, songs, etc. |\n",
    "|LAW |\tNamed documents made into laws. |\n",
    "|LANGUAGE |\tAny named language.|\n",
    "|DATE |\tAbsolute or relative dates or periods. |\n",
    "|TIME |\tTimes smaller than a day. |\n",
    "|PERCENT |\tPercentage, including \"%\".\n",
    "|MONEY |\tMonetary values, including unit. |\n",
    "|QUANTITY |\tMeasurements, as of weight or distance. |\n",
    "|ORDINAL |\t\"first\", \"second\", etc. |\n",
    "|CARDINAL |\tNumerals that do not fall under another type. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to see all the unique named entities in the Chicago page, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chicago ( ( listen) or ), officially the City of Chicago, is the third-most populous city in the United States. With over 2.7 million residents, it is also the most populous city in both the state of Illinois and the Midwestern United States. It is the county seat of Cook County. The Chicago metropolitan area, often referred to as Chicagoland, has nearly 10 million people and is the third-largest in the U.S. Chicago has often been called a global architecture capital. Chicago is considered one o'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chicago.content[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Northwestern Memorial Hospital', 'December 2, 1942', 'the Ida Crown Jewish Academy', '315,000 square feet', 'HALS', 'Fox', '42', 'Vietnamese', 'Truman College', 'Prison Break', 'six', 'between 2010 and 2040', '\\n\\n\\n', 'annual', 'Lake', 'the Institute of Gerontology', 'Landmarks', 'Democrat', '1900 to 1939', 'Wilbur Wright College', 'Exelon', 'Mies van der', '1837', '3 million', 'Chicago Hope', 'about 3,500', '10 feet', 'African-Americans', \"Fairbanks's\", '2012', 'Metrovi', 'the National Basketball Association', 'between 1920 and 1930', 'West Town', 'Wood Street', 'Magdalena Abakanowicz', '452', 'first eight seasons', '0.1%', 'South Side Chicago', 'Greek', 'ImprovOlympic', '1 million', 'Pilsen', 'New Orleans', 'Founder of Chicago', 'The CME Group', 'the end of the 19th century', 'Chicago Dance Crash', 'the University of Chicago Crime Lab', 'Rockford', '416', '32', 'the John Marshall Law School', 'Anish Kapoor', '92', 'first', 'Mother Teresa', '1930', 'Michigan Avenue', 'year 2014', 'the year 2016', '1970', 'the Harris Theater', 'Jean Dubuffet', '1906', 'ThyssenKrupp North America', \"O'Hare Airports\", '943', '458', \"the U.S. Census Bureau's\", 'Pontiac', '2,600', '61.7', '1876', 'John Paul II', 'four consecutive years', 'American Community Survey', 'Oldenburg', 'four', 'The Merchandise Mart', 'the Western Hemisphere', 'the Garfield Park Conservatory', '1989', '0.6%', 'William Carlos Williams', 'Ace Hardware', 'The Chicago Police Department', 'the Arizona Cardinals', 'more than 77%', \"Don't Tell Me\", 'over 200', 'Ford Motor Company', 'Albert Raby', 'about 300,000', 'U of C', 'one quarter', '1933', 'Richard Teller Crane', 'Kendall', 'every year', 'early 1962', 'the 1880s and 1890s', '32.9%', '1924', \"O'Hare International Airport\", '75.8', '1860s', 'Broadway', 'the Near South Side', 'WBBM', 'South Bend', 'Fortune Global 500 companies and 17', 'UIC', 'During World War', 'twenty-six miles', 'The Museum Campus', 'Iowa', 'the Evangelical Covenant Church', '750', 'Republican National Convention', 'the American Hospital Association and Blue Cross', 'United States', 'McKenna', 'FIFA World Cup', 'Amtrak', 'Michigan Canal', 'the Chicago Portage', 'Roosevelt University', '52', \"Ferris Bueller's\", 'the ESPN Radio-owned', 'Honduran', 'Alexander Calder', 'United Continental Holdings', 'Indians', 'Chicago Public Radio', 'the Maxwell Street', 'the Lyric Opera', 'recent years  the Bears (', 'Cleveland', '5b', 'Southern', 'thousands', 'Ellen Gates Starr', 'the Harris Theater for Music and Dance', 'La Villita', '1897', 'Six', 'the Museum Campus', 'Sunday', '390', 'District of the Federal Reserve', 'the summer', 'The American Medical Association, Accreditation Council', 'the Chicago City Council', 'under 2.7 million', 'the University of Chicago Medical Center', 'Jaume Plensa', '860-880 Lake Shore Drive Apartments', 'University of Illinois at Chicago', 'the Port District', '26th Street', 'The South Side', 'Four Seasons', 'Calumet', 'the Chicago Metropolitan Area', 'Chicago High School for the Arts', 'Western Avenue', 'the New Negro Movement', 'MLS', 'Chi-Town', 'Sixteen Candles', 'Republicans', '506', 'Irish', 'Chicago State University', 'Sox', 'Christopher Columbus', '1883', 'the City Beautiful Movement', 'over 3.6 million', '1.308 million', 'Lake Michigan', 'Water Tower Place', 'Chicago Rockford International Airport', '250 million US gallons', '2002', 'Puerto Rican', 'Jean Baptiste Point du Sable', 'the Chicago Mercantile Exchange', '1893', 'America', 'the Chicago Sanitary and Ship Canal', 'Harry Caray', 'Standing Beast', 'Illinois International Port', 'six Stanley Cups', 'the Chicago State Cougars (', '1919', \"the United States'\", 'Northwest Indiana', 'Watch Dogs', 'nineteen', 'Programs', 'The Chicago Lincoln', '910 m', 'State Street', 'the Chicago Climate Exchange', 'the Chicago Sun-Times', 'August 12, 1833', 'the Gangster Era', 'Jones College Prep', 'Picnic', 'the Port of Chicago', 'Megabus', '\\nList of fiction', 'Marshall Field', 'the Chicago Opera Theater', 'the University of Illinois Medical Center at Chicago', 'Cadillac Palace Theatre', 'Harold Washington', 'the Fine Arts Building', 'Ed Paschke', \"Anish Kapoor's\", 'National Blue Ribbon School', 'Daley Plaza', 'Fort Dearborn', 'West Side', 'This American Life', 'The Regional Transportation Authority', 'the National Weather Service', '2014', 'the Chicago Tribune', 'the Chicago Shakespeare Theater', '21.4%', '54,188', 'Chicago Academy for the Arts', 'Horto', 'I.O.', 'the Federal ATF', 'Inland Northern American', 'the Near North Side', 'the University of Chicago Cultural Policy Center', '30,000', 'Jefferson Park', 'Carbondale', 'Chicago Fire', '8,390,000 square feet', 'Buckingham', '510', 'Financial Times', 'The City of Chicago', '1968', 'Common Council', 'Checagou', 'Chopin Park', 'Lithuanians', 'Illinoisan', '1975', 'First', 'WGN America', 'Rush University Medical Center', 'Still Standing, The League', 'The Good Wife', '2016', 'December 2016', '7%', 'the 1920s', 'Chicagoland', 'P.D.', 'Calumet Harbor', 'The Chicago Board of Trade', 'John H. Stroger', 'Gary/Chicago International Airport', 'Mount Carmel High School', '91', 'Jesuit', 'the Tribune Broadcasting-owned', \"Northern Indiana Commuter Transportation District's\", 'Obama', 'about one', 'Brewster', 'Democratic', 'the Great Northern Migration (Saar', 'the 1850s and', 'Lawrence Avenue', 'Division', '2020', 'Downers Grove', 'Mexican', 'Horizon League', 'Gwendolyn Brooks', 'Chicago Festival Ballet', 'Checker', 'Ten years later', 'daily', 'the Federal Reserve Bank of Chicago', 'Adlai Stevenson', '109', 'Alinea', 'New York', 'GE Healthcare', 'Solidarity Promenade', \"Frank Gehry's\", 'the Willis Tower', 'May 4, 1886', 'PRI', '1955', 'the Ford Center for the Performing Arts Oriental Theatre, Bank of America Theatre', '\\nNational Register of Historic Places', '1912', '2006', 'Illinois State', 'The Fourth Presbyterian Church', 'Hull House', 'Navy Pier', '29%', 'South', '2004', 'Montgomery Ward', 'Korean', 'the University of Illinois at Chicago', 'September 9, 2013', '2,900 shootings13%', 'Lollapalooza', 'the Green Bay Packers', 'Cook', 'Chicago Board of Health', '2.5', 'recent years', 'Race Riot', 'Two years later', 'Jews', 'U.S.', 'Democratic Party', '448', '910', 'Seattle', '5%', 'Labor', '355', 'Loyola University Chicago', 'Ravinia Festival', '201415', 'Polasek', 'U.S. News & World Report', '1910', \"Oprah Winfrey's\", 'Little Italy', 'Cloud Gate', '2.7 million', 'Sears) Tower', 'Miami', 'the summer of 2016', 'Early Edition', 'roughly 4%', 'De La Salle Institute', 'Graduate Medical Education', 'the Black Belt', 'the University of Chicago Divinity School', 'the Chicago Freedom Movement', 'June 15, 1835', 'The University of Chicago Oriental Institute', 'Franklin D. Roosevelt', 'the Eastern United States', 'Burnham Park', 'Late in the 19th century', 'the Windy City Times', 'Sister', 'WGN', 'the 19th century', 'Kenwood', '1977', 'the Century of Progress International Exposition Worlds Fair', 'the DuSable Museum of African American History', 'the Southern United States', 'Major League Baseball', '200', 'the Rehabilitation Institute of Chicago', 'Central and Eastern Europe', 'Democrats', 'Las Vegas', 'the \"Top Ten Cities', '1869', 'the Soviet Union', 'Ottawa', '98.1%', 'Grant Park', '400,545', 'Ogden', '100,000', 'This Great Migration', 'Paseo Boricua', '1929', 'The Chicago Tribune', 'the Hubbard Street Dance Chicago', 'Sinister 2 and Suicide Squad', \"North America's\", '31.7%', 'the Chicago Fire Department', 'The Man (a.k.a', 'over 780,000 square meters', 'Los Angeles', 'The Chicago Marathon', 'Reagan', 'Boston', 'Exposition', '176.5 m', 'five', \"Benjamin Ferguson's\", 'Du Sable', 'the Great Lakes', 'South Side', 'Walk Score', 'Maywood', 'John Farwell', 'American', '200th', 'the UIC Flames', '0.40 km2', 'the Lyce Franais de Chicago', 'Poetry.', 'CPS', 'Blackhawks', 'the Great Lakes Megalopolis', 'F', 'U.S. House', 'African American', \"O'Hare Airport\", 'the Schwinn Bicycle Company', '2005', 'KennedyKing College', 'Edmund Dick Taylor', 'The City Beautiful', 'DuPage', '0.5%', 'Milwaukee', 'only 15.65', 'William Rainey Harper', 'Pizzeria Uno', 'Hospital of Chicago', 'the Feinberg School of Medicine', 'the Chicago Building', 'Salvadoran', '38.9', 'Douglas', 'Amrany', 'White (', 'The Dark Knight', 'Chicago', '2007', '1873', '12', '1.1%', 'Michael Jordan', '1998', 'East Wacker', 'American Association of Nurse Anesthetists', 'Grand Calumet River', 'the Midwestern United States', 'the Treaty of Chicago', 'D.C.', 'Northerly Island', 'over 1,000,000', 'the Driehaus Museum', 'MLB', 'Historic Places', 'Glencoe', '2010, 2013, 2015', 'Stritch School of Medicine', 'Central Park', 'Burnham', '6 miles', 'Josephinum Academy', 'the Globalization and World Cities Research Network', 'more than 6,000', '11', 'Electronic Dance Music', 'fourth', 'Midwest', 'the spring', 'the Town of Chicago', '24.3', 'March 1937', 'Greektown', \"Loyola University Chicago's\", 'Yellow', 'July 2016', '1.72 million', '90%', \"Frdric Chopin's\", '94', 'Elston', 'U.S. Presidents (Eisenhower', 'the National Museum of Mexican Art', 'Uptown', 'Rick Tramonto', 'the middle of the night', 'Pakistani', \"the 'L'\", 'two', 'twenty-four', 'February 1856', 'the Chicago Bears', 'ComEd', 'U.S. Department of Transportation', 'approximately 153,000', 'Anton Cermak', 'Millions', 'National Register of Historic Places', 'the MasterCard Worldwide Centers of Commerce', 'September 10, 2012', '13.4%', 'Midway Airport', 'night', 'Sneak Previews', 'Wrigley Field', '1892', 'Buddhists', '47,074', 'the Plan of Chicago', '1874', '29.3%', '578', 'the University of Chicago', \"the Women's National Basketball Association\", 'second', 'Hyde Park', '490', 'Boystown', '1940 to 1979', 'Toyota Park', 'the year', 'Hong Kong', 'Sufjan Stevens', 'WGN-TV', 'Tribune', 'Italians', 'NPR', 'World War II', '672', 'The Frugal Gourmet', 'Allium', 'Pier', 'the Democratic Party', 'StreetWise', '62.8%', 'the Art Institute of Chicago', 'the National Hockey League', 'Blue Shield Association', 'Thorvaldsen', 'Serbs', 'Lincoln Park', 'the McCormick Place Convention Center', 'Grundy', 'one day', 'annually', 'five 50,000 watt', 'every day of the year', 'Trump International Hotel', '16th', 'fifth', 'the end of 2018', 'about 70', 'Lakeview', 'Kppen', '20', 'Major League Soccer', 'Combo', '55 percent', 'Malcolm X College', 'Little Seoul', 'Richard J. Daley College', 'sixteen', 'General Electric', 'the Chicago Teachers Union', 'Stephen Douglas', 'Filipino', \"St. Adalbert's Church\", 'Colombian', 'about $640 billion', '14 million bushels', 'Montenegrins', '19', 'CTA', 'bin', 'the Adler Planetarium & Astronomy Museum', 'Great Lakes', 'the Great Chicago Fire', '4,331', 'US', 'Bobby Hull', 'Vesuvio', 'Crusader', 'Metra', 'fewer than 200', 'The McLaughlin Group', 'Thanksgiving', 'Family Matters', 'the North Side', 'Europe', 'HQ', 'CBOE', '1889', \"Richard J. Daley's\", '16.14', 'Illinois Congressman', 'January 20, 1985', 'about $658.6 billion', 'Taylor Street', 'NBC', 'Armour and Company', 'Orange, Brown, Purple, Pink', 'nearly 150 percent', '24hour', '600', 'June 2017', 'Ferris', 'Chagall', 'Chodzinski', 'Warsaw', 'Department of Transportation', 'Humboldt Park', 'the Moody Bible Institute', 'American College of Surgeons', 'Claire', 'Sears Tower', 'the Shedd Aquarium', 'the early 20th century', 'Plensa', '3.8%', 'Saint Xavier University', 'American College of Healthcare Executives', '1,045,560', '45', 'Abbott Laboratories', \"Dion O'Banion\", '2.7%', '$1.95 billion', '10,000', 'Merc', 'the Chicago Stock Exchange', 'Prohibition', 'The White Sox', 'the 20082012 American Community Survey', 'From 1995 to 2008', '1983', 'Lithuanian', 'The Oprah Winfrey Show', 'the White Sox', '33', 'Hispanic', 'Claes Oldenburg', 'DW60', 'Jens Ludwig', \"Moore's\", 'Constitution', 'PBS', 'Iroquois County', '579', '\\n\\nSporting News', 'the United States Army', \"Marshall Field's\", 'Cook County Forest', 'Mike', '800,000 barrels', '1907', 'U.S. Census', 'the Erikson Institute', 'Lake Calumet Harbor', 'the years', 'Little Vietnam', 'Auditorium Building', 'Baxter International', '397', 'Art Nouveau', 'Two', 'Czechs', '1688', '10%', '3,200', '96', 'St. Louis', 'Art Institute of Chicago', 'the Illinois River', 'Jackson Park', '$2.5 billion', 'Miegs Field', 'the early 1960s', '18th', 'Bruce DuMont', 'Lake Calumet', 'the Chicago Literary Renaissance', 'Roger Brown', 'Blue Island', 'Richard J. Daley', 'Peace High School', 'Missouri Valley Conference', 'Loop', 'Ivan Albright', 'Jane Byrne', 'English', 'The Chicago Loop', 'the American League', 'Kane', 'Thai', 'DuSable Park', 'the Flag of Chicago', 'Richard M. Daley', 'Molly', '5,800', '1872', 'San Antonio', 'Rush University', '1997', 'the Chicago Regional Port District', 'weeks', 'the Midway Plaisance', 'the Brookfield Zoo', 'Lincoln Park Zoo', 'AL', 'SouthtownStar', '449', \"Boyle's The Alarm\", 'more than 14%', '60 miles', 'Parks', 'The Lithuanian Opera Company of Chicago', '40 km', 'the Adler School of Professional Psychology', 'Robert Morris University Illinois', 'Miro', 'AP', 'Healthcare', 'Clark Street', 'the National League', 'Maps of Chicago', 'DePaul University', 'the University of Chicago Laboratory Schools', 'Helmut Jahn', 'Art Institute of Chicago, Museum Campus', 'Albany Park', 'Robb Report', 'Peruvian', 'North Side', 'Native American', 'more than one', 'Bill Savage', '294', 'Chicagoua', '1953-54', 'the 1840s', 'WNBA', 'Enrico Fermi', 'United Airlines', 'Crunelle', 'the winter season', '18.5', 'the 1780s', 'Belmont Avenue', '98', 'the 1920s and 1930s', 'CareerBuilder', 'the Battle of Fort Dearborn', 'the Chicago Loop', 'Cumulus Media-owned', 'the Society for Human Rights', 'the Chicago Board Options Exchange', 'the Mississippi River', '1993, 2006', 'CW', 'Republican', 'the Native American', '105', 'July 24, 1934', '468', '970', 'June 2016', '44th', 'the Pitchfork Music Festival', 'Flamingo', 'summer months', 'IL', '1908', 'Saint-Gaudens', 'Indian', 'The A.V. Club', 'Civic', 'July', 'Tower', 'Caribbean', 'the American', 'Batcolumn', 'about 2 days', 'Muslims', 'the mid-nineteenth century', '1974', 'Heald Square Monument', 'North Chicago', 'the Chicago Medical School', 'Charlie Trotter', 'the Near West Side', 'West Sides', 'about 4 miles', 'the Chicago Black Renaissance', 'seven', 'eight', '1985', '410', 'Parliament', '\\n\\nRenowned Chicago', 'one', 'the Chicago Board of Trade', '8', 'Chase Tower', 'NFL', '205 m', 'eleven', 'Bosnians', 'William Butler Yeats', 'John Ashbery', 'January', 'the mid-18th century', 'Rice High School', 'the Goodman Theatre', 'The Chicago Transit Authority', 'Jack Brickhouse', 'Agora', \"Ann & Robert H. Lurie Children's\", 'Divvy', 'the last half of the 19th century', 'GRAB', '93', 'the U.S. Army Corps of Engineers', 'the National Register of Historic Places', 'Whitney M. Young', 'WLS', '1850 to 1890', 'Masaryk', 'about 1900', '28.9%', 'the Calumet River', 'John Whitfield Bunn', 'Cond Nast Traveler', 'Lake Shore Drive', 'Cook County', 'Rohe', '1901', 'EastWest University', '44,103', '3.6 million', 'the Chicago White Sox', 'South Halsted Street', 'Lincoln', 'Seventh', '79', 'Wiki', 'St. Ignatius College Preparatory School', '41', 'the Allstate Arena', '1.6%', 'the Southwest Side', '55th', '9.7 km', 'Midwestern', 'the Lutheran School of Theology at Chicago', 'Streeterville', 'Midway International Airport', 'the Steppenwolf Theatre Company and Victory Gardens Theater', '6th', '2,695,598', 'Dallin', 'Asian', 'CBS', 'Jesse Brown VA Hospital', '201011', 'Abraham Lincoln', 'the Tribune Media', 'Presbyterian', 'Spearman', '43', \"the Illinois State's\", 'Armour Square', '27.5 million', '\\n\\nChicago Wilderness\\nList', '19th century', 'the City of Chicago', 'Superfans', 'Latin', 'Detroit', '580', 'Irv Kupcinet', 'early 1920s', 'Poles', 'the United Center', 'Washington', 'The Second City', 'Picasso', 'the Buehler Center', 'McDonald', 'the School of the Art Institute of Chicago', '4', 'Dubuffet', 'Democratic National Convention', 'Dan Ryan', 'the Great Lakes region', '24 hours', 'Florida', '125', 'One', \"Lamb Chop's\", 'two miles', 'the forty years', 'ABC', 'sixty years', 'the Great Depression', 'Discovery', '9th', 'Newark', 'the Chicago Imagists', '196 ft', 'Peoria', 'UBS', '26%', 'the Museum of Broadcast Communications', 'Marc Chagall', 'New York City', 'NFL Championships', 'about 130', 'the Chicago Cubs', 'Latino', 'the Sinaloa Cartel', '88', 'the Chicago Police Department', 'Defender', 'December 20, 2014', 'USDA', 'the Obama Foundation', 'a World Series', 'Newcity', 'Harold Washington College', 'Standing Lincoln', 'Lorado Taft', 'Super Bowl', 'RTA', 'San Francisco', 'Bowman', '762', 'Charles B. Atwood', 'Gateway Theatre', '1885', '13.2', 'more than 100,000', 'West Loop', 'Logan', 'only one', 'the Museum of Contemporary Art', 'OliveHarvey College', 'Each year', '1679', \"T. S. Eliot's\", 'Justice', 'Jr.', '12 m', '32.6%', '80', 'Greyhound Lines', '53', 'Law', 'The River North Gallery District', '190', 'the Kansas City Southern Railway', 'Opera House', 'LaPorte', 'Langston Hughes', 'the latter half of the 20th century', 'Black Belt', 'nearly 400 acres', 'Chicagou', '2019', 'about 4.48 million', 'Sears', 'McCormick Place', '2011', 'Potawatomi', 'Annual', '16 percent', 'Walgreens', 'The Love Song of J. Alfred Prufrock', 'The Robie House', 'Best Sports City', 'Beltway', 'National Louis University', 'More than half', 'Julius Rosenwald', 'Batman Begins', '176.2 m', 'Yellow Cab Companies', 'between 1958', 'several square miles', 'the 1990s', 'Preston Bradley Hall', '77', 'the year before', 'St. Patrick High School and Resurrection High School', 'French', 'sixth', 'Japanese', 'NHL', '97 km', 'the Barack Obama Presidential Center', 'Lane Technical College Prep High School', 'Illinois', 'Brass Era', '1994', 'the Hyde Park Township', 'three', 'Cabrini', 'the Jefferson Township', 'Rochester', 'WMVP', 'Martin Luther King', 'North America', 'the Feltre School in River North', 'McHenry', 'Five', 'Receiver of Public Monies', '415', 'Park District', 'several decades', 'WMAQ 5', '15.94', 'nearly two-thirds', 'Crown Fountain', 'eight seasons', 'Moose', 'the 2006 WNBA season', '1860', 'Desi', 'the Northwest Indian War', 'LGBT', '1917', '\\n\\nChicago', '1867', 'roughly 60%', 'World Marathon Majors', 'Northeastern Illinois University', 'Haymarket', 'Louis Sullivan', 'the Chicago Region Environmental and Transport Efficiency', 'the John Hancock Center', 'Commonwealth Edison', 'Bugs Moran', 'the Feinberg School of Medicine of Northwestern University', 'the first half of 2013', 'Northwestern University', 'Soldier Field', '1965', 'close to five years', '31', 'the U.S. Futures Exchange', 'the Windy City', 'Metaxa', 'summer', 'nearly 25,000', 'Statue of the Republic', 'Bridgeview', 'Little Calumet River', 'the Chicago Botanic Garden', 'Seven', 'third', 'Day Off', 'February 23, 2011', 'the 1980s and 90s', '35', '3 km', 'Civil War', 'the Kansas', 'the Treaty of Greenville', 'Kinzie Street Bridge', 'over 4,000', 'Jim Nutt', 'Grant Park Music Festival', 'U.S. Representatives', '1833', 'Chicago Union Railroad', '1,000,000', 'the 1850s', '3,000', 'the 1870s and 1880s', 'the \"Original Six\"', '75', 'Joseph Jefferson Awards', 'Caterpillar Inc.', 'May 16, 2011', 'Columbia College Chicago', '55', 'Lithuanian Chicagoans', 'North', 'the U.S. Chicago', 'Abakanowicz', '0.4%', \"Humboldt Park's Institute of Puerto Rican Arts\", 'the 1910s', 'over 8,000 acres', 'Polish', 'early summer', '1942', 'Indianapolis', 'Port Huron', 'The Illinois Medical District', 'Albanians', 'the Crown Fountain', 'the century', 'Navy', 'The Chicago Bulls of', 'The Chicago Symphony Orchestra', 'CBS Radio', '1848', 'Grand Rapids', '0.3%', 'Willis', 'Orlando', '100 acres', 'the last two decades of the 19th century', 'Cityscape', 'the Northern District', '2009', 'the West Coast', 'the Illinois Institute of Art  Chicago', 'Chase Bank', 'Buckingham Fountain', 'Accreditation Council for Continuing Medical Education', 'Barbara Rossi', 'the British School of Chicago', 'the 1940s', 'the Catholic Theological Union', '22nd Street', 'NYMEX', 'Portland', 'Germans', 'the Chicago Cardinals', 'hundreds', 'Home Alone', '20082012', 'Columbian Exposition', 'Rick Bayless', 'North Avenue', 'Robert Lostutter', 'the Chicago Architecture Foundation', 'about 75%', 'Brioschi', 'W-02-03', '500', 'Museum of Science and Industry', 'the Chicago Public Library', 'the AT&T Plaza', '1795', '431', 'Wrigley', 'Ireland', 'as many as 21 days', 'Derrick Rose', 'Beauty and Crate & Barrel', 'Grant Achatz', 'Dziennik Zwizkowy', 'Batman', 'ten', 'Iroquois', 'Orbitz', 'NowSecure', 'fifty', 'Visitor Information Center', 'North American', 'Chicago Innerview', 'MacCormac College', '1980', '1871', 'Integrys Energy Group', 'Illinois State Board of Health', '54 million', 'Northern American', 'Chicago Med', 'the Evangelical Lutheran Church in America', '2010', 'since 1945', 'The Joffrey Ballet', 'Imagist', 'Boeing', '29,000 square meters', 'Rosemont', 'Strachovsk', 'D1', '1987', 'the dawn of the century', '47,408', 'African Americans', '', 'CSO', 'John Root', 'Barack Obama', 'Aging, Health & Society', 'Cook County Jail', '55.7', 'Andersonville', '160', '46', 'DMOZ\\n', 'Lady Michelle Obama', '1969', 'The University of Chicago', 'Oak Park', 'the Sears Tower', 'World Series', 'Near Eastern', 'American Osteopathic Association', 'the South Side of Chicago', 'the Peggy Notebaert Nature Museum', 'more than US$13.7 billion', '34', 'the New York Mercantile Exchange', 'Ethnically', 'The Willis Tower', 'Ecuadorian', 'South Shore Line', 'The Illinois Department of Tourism', 'Kearney', 'The Roman Catholic Archdiocese', 'its first hundred years', 'Draugas', 'the American Geographical Society Library\\nHistoric American Landscapes Survey', '9', '20052009', 'The Chicago School of Professional Psychology', 'Chinese', 'Bennett', 'Divergent', 'seven years later', 'Kennedy', 'COMEX', 'the Logan Square Boulevards Historic District', '1992', 'Polish Patches', 'Punky Brewster', \"Bill Swerski's\", 'American League', '21st', '1993', '42,063', 'between 1955 and 1971', 'the 1980s', 'Bronzeville', 'the Cook County Circuit Court', 'Between 1910 and 1930', 'Rogers Park', 'The Loop', 'Cubs', '66', 'Major League', 'the Saturday Night Live', 'the Standard Oil Building', 'Jane Addams', 'American Dental Association', 'White House', '11.09 million', 'John H. Rauch', 'Frdric Chopin', 'the late 1920s', 'about 200', 'Ojibwe', \"Edward Kemys's\", 'South Shore', '1,200 acres', '4.0', 'the Morgan Park Academy', '1945', 'Northside College Preparatory High School', 'Henri Joutel', 'the Illinois International Port District', 'World', 'Groupon, Feedburner', 'the Chicago Pride Parade', 'the Chicago School', '0.2%', 'Devon Avenue', 'WBBM 2', 'Catholic', 'Havliek', 'Copernicus', '29.7%', 'the Western Wheel Company', 'Marist High School', '2001', '57', '1927', 'more than 4,000', 'between 1910 and 1920', '3', 'Symphony Center', 'Swedes', 'CME Group', 'the Red, Blue', 'The Blues Brothers', 'the United States', 'the Home Insurance Building', \"New York's\", 'Children, Kenan & Kel', 'the Chicago Reader', 'Al Capone', 'GE Transportation', 'The Bob Newhart Show', 'Ezra Pound', 'Wisconsin', 'the U.S. Department of Education', '1995', '60 m', 'the 201011 season', 'the Wabash Avenue Bridge', 'The Midwestern University Chicago College of Osteopathic Medicine', 'the Chicago Board of Trade Building', 'Brookfield', 'the Chicago History Museum', 'Singapore', 'NBA', '16', 'the Aon Center', \"Prentice Women's Hospital\", '1934', 'more than 570', '\\n', 'Croatians', 'November', 'Grace', 'the National Football League', 'Northwest Side', 'Onion', 'African-American', 'Poetry', 'Archer Daniels Midland', \"People's Parade\", 'The Port', 'Taste of Chicago', 'three days', 'Memorial Parks', '13', '20th', '58', 'Alderman Eugene Sawyer', 'the Museum of Science and Industry', 'the South Side', 'the Illinois and Michigan Canal', 'seventh', 'Jr. Hospital of Cook County', 'Galena', 'about 300', '20132014 20th Day', 'Shimer College', '1984', 'the Francis W. Parker School', 'St. Rita of Cascia High School', 'Today', 'year', 'Deerfield', '59%', 'Robert de LaSalle', 'State', '1950', 'Indiana', 'Advanced Placement', '233,903', 'Showtime', 'Harlem', 'the mile', '2003', 'Rate Field', 'the Daily Herald', 'Metra Electric Line', 'Harriet Monroe', 'The Near West Side', '71%', 'the Commodities Exchange Inc.', 'several consecutive days', 'Leon Golub', 'Rahm Emanuel', 'today', '1:1scale', '50.17 million', 'the twentieth century', 'Michelin Guide', 'nearly 10 million', 'Bulls', '10.4', '290', 'IL-10', 'Frank Lloyd Wright', 'Central Chicago\\n', 'Midway', 'Society for Clinical Pathology', '12.5%', 'Peoples Gas', 'Chicagoans', 'over 20 million', '45.0%', 'The Chicagoland Chamber of Commerce', 'John Crerar', 'Notes', '1 mile', '1966', 'Kraft Heinz', 'the Pritzker Military Library', 'Royal Baths', 'June', 'the Polish Museum of America', '97', 'James Merrill', 'Illinois Institute of Technology', 'About 18.3%', 'Daniel Burnham', '2015', 'the Latin School of Chicago', 'Signal of Peace', 'Academy of Nutrition and Dietetics', \"Lions, Saint-Gaudens's Abraham Lincoln\", 'North Park University', 'the Field Museum of Natural History', '50', 'The 2015 year-end', 'winter', 'the Public Land Survey System', 'DePaul College Prep', 'The North Side', 'Cuban', '42 km', '5.5%', '0.7%', 'the 20th century', 'Magnificent Mile', 'the Red and Blue', 'Gary', 'Stan Mikita', 'the Chicago River', 'half', 'Kankakee', 'African', 'The Chicago Blackhawks', 'Walter Payton', 'Millennium Park', 'Dutch Wheels', '1926', '1979', 'Quincy', 'Lake, McHenry, DuPage', 'William Thompson', '25 miles', 'Chicago Cultural Center', 'Guatemalan', 'The City Council', '65', '1803', 'July daily', 'Washington Park', 'the Jay Pritzker Pavilion', \"the World's Religions\", 'The University of Illinois College of Medicine', '1.2%', 'Willis Tower', 'Manhattan Project', 'Tony Accardo', 'Objectivist', '3,000 linear feet', 'June 4, 1998', 'Amrany and Rotblatt-Amrany', 'up to eight', 'Jackson Parks', 'Saturday, March 4, 1837', 'each year', 'ULTA', 'Urbana', 'Union Station', 'Hindus', '20142016', 'West Ridge', 'The Institute for Clinical Social Work', 'Christians', 'since 1992', 'recent 2015', 'Ukraine', '22.1%', '1812', 'Time Out Chicago', 'the Ping Tom Memorial Park', 'seven years', 'Site Selection', 'Joliet Junior College', '30', '1956', 'British', 'Harpo Studios', '22', 'Fortune 1000', '2008', '22%', \"Wacaw Szymanowski's\", \"St. Valentine's Day Massacre\", '26 miles', '1900', 'American Literature', 'Global Cities Index', '1905', '201 meters', 'North Side Chicago\\n', 'Robot, Mean Girls, Wanted', '25%', 'early 2018', 'nine', 'Cristo Rey Jesuit High School', 'the Chicago Public Schools', 'Carl Sandburg', 'Lutheran', '28', '2013', 'the following decades', 'Ogden Avenue', 'M.D', 'the Great Lakes south', 'Kociuszko', 'the National Mall', '1866', 'Lincoln Park Conservatory', 'Topography', 'Pace', 'Academy of General Dentistry', 'World Trade Center', 'between 1851 and 1920', 'the Prairie School', 'Music of the Baroque', 'Landing Lakefront Terminal', 'four years', 'two-thirds', '90', 'Egyptian', 'late September 1687:', 'Italian', 'about 29'} 1545\n"
     ]
    }
   ],
   "source": [
    "chicago_model = nlp(chicago.content)\n",
    "named_entities = []\n",
    "for entity in chicago_model.ents:\n",
    "    named_entities.append(entity.text)\n",
    "print(set(named_entities), len(set(named_entities)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Understanding 2 (10 Minutes)\n",
    "\n",
    "As a market (or as groups in your market), please discuss the following:\n",
    "\n",
    "1. As modelers, we will frequently have to make decisions about how to transform data. If you were using NLP to predict things, would it make sense to keep named entities? Would it make sense to drop them? If it would depend on the circumstances, under what circumstances would it make sense to keep or drop named entities?\n",
    "\n",
    "We'll have a couple of markets come on mic to discuss cases they identified where keeping named entities might make sense and cases where it would not make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `textblob` to do sentiment analysis\n",
    "\n",
    "We can also use a library known as [`textblob`](https://textblob.readthedocs.io/en/dev/) to do a **lot** of text transformation and extraction on our behalf. For our purposes, we are going to use it to analyze text and derive the overall sentiment of the text.\n",
    "\n",
    "Sentiment can be split into two related scales:\n",
    "\n",
    "- subjectivity (0 to 1): scores closer to 0 are more objective in tone, scores closer to 1 are more subjective in tone\n",
    "- polarity (-1 to 1): scores closer to -1 are more negative in tone, closer to 0 are more neutral, and closer to 1 are more positive in tone.\n",
    "\n",
    "Using `textblob` is user-friendly -- pass a string into a `Textblob()` class and then call the `.sentiment.polarity` or `sentiment.subjectivity` attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "really_good_review = '''\n",
    "Goodness me, what a fantastic movie. \n",
    "Caught the world premiere at the Toronto International Film Festival and \n",
    "the entire theater laughed until they cried. \n",
    "Amazingly directed, HILARIOUSLY funny, it blends a 1930s gangster \n",
    "stylishness into a Hong Kong kung fu movie to astonishing results. \n",
    "Who would've thought you could top Shaolin Soccer? \n",
    "Not me, until I saw this movie. Stephen Chow pulled it off. \n",
    "Chow's comedic timing gets better and better with every movie \n",
    "he makes, and while his films are depending more and more on \n",
    "CGI these days, and makes this movie much more a fantasy kung \n",
    "fu film than a traditional one, it hardly detracts from the \n",
    "enjoyable experience. Make it your mission to see this film - \n",
    "it will be one of the most entertaining you ever see. \n",
    "I can't remember the last film I enjoyed myself in more. \n",
    "My eyes still hurt from wiping away tears of laughter. Seriously.  \n",
    "'''\n",
    "\n",
    "really_bad_review = '''\n",
    "Thank you for coming into your performance review Mr. Smith.\n",
    "The company is concerned about your performance. Lately your work has \n",
    "been subpar and at times counter to this company's stated goals.\n",
    "Your demeanor has been aggresive and at times hostile to your \n",
    "fellow coworkers.\n",
    "We have no choice but to terminate your employment, effective \n",
    "immediately. Thank you.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5749999999999998 0.33295454545454545\n",
      "0.7 0.15\n"
     ]
    }
   ],
   "source": [
    "good_review = TextBlob(really_good_review)\n",
    "print(good_review.sentiment.subjectivity, good_review.sentiment.polarity)\n",
    "\n",
    "bad_review = TextBlob(really_bad_review)\n",
    "print(bad_review.sentiment.subjectivity, bad_review.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Understanding 3 (5 Minutes)\n",
    "\n",
    "Individually, please answer the following:\n",
    "\n",
    "1. What type of subjectivity and polarity scores would you expect wikipedia articles to have?\n",
    "2. Confirm your hypothesis by using `textblob` on some of the wikipedia pages we have used so far. Were your thoughts confirmed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjectivity:  0.41458578770627064\n",
      "Polarity:  0.12113963145288432\n",
      "Sentiment:  Sentiment(polarity=0.12113963145288432, subjectivity=0.41458578770627064)\n"
     ]
    }
   ],
   "source": [
    "bb = wikipedia.page('Bill_Belichick')\n",
    "bb_model = nlp(bb.content)\n",
    "\n",
    "bb_review = TextBlob(str(bb_model))\n",
    "print ('Subjectivity: ', bb_review.subjectivity)\n",
    "print ('Polarity: ', bb_review.polarity)\n",
    "print ('Sentiment: ', bb_review.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjectivity:  0.358245305279938\n",
      "Polarity:  0.10134327833462034\n",
      "Sentiment:  Sentiment(polarity=0.10134327833462034, subjectivity=0.358245305279938)\n"
     ]
    }
   ],
   "source": [
    "bb = wikipedia.page('Sachin_Tendulkar')\n",
    "bb_model = nlp(bb.content)\n",
    "\n",
    "bb_review = TextBlob(str(bb_model))\n",
    "print ('Subjectivity: ', bb_review.subjectivity)\n",
    "print ('Polarity: ', bb_review.polarity)\n",
    "print ('Sentiment: ', bb_review.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity:  -0.08819444444444446\n"
     ]
    }
   ],
   "source": [
    "t_tweet = 'Why would Kim Jong-un insult me by calling me \"old,\"\\\n",
    "when I would NEVER call him \"short and fat?\" Oh well, I try so hard to be his friend\\\n",
    "- and maybe someday that will happen!'\n",
    "\n",
    "tt_review = TextBlob(str(t_tweet))\n",
    "print ('Polarity: ', tt_review.polarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding these features to DataFrames\n",
    "\n",
    "We may want to include these features into a DataFrame for use in a later model. The most straightforward way to do so would be to apply them using Pandas.\n",
    "\n",
    "Here, we'll make use of the same dataset on economic news that we used yesterday. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relevance</th>\n",
       "      <th>headline</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Yields on CDs Fell in the Latest Week</td>\n",
       "      <td>NEW YORK -- Yields on most certificates of dep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>The Morning Brief: White House Seeks to Limit ...</td>\n",
       "      <td>The Wall Street Journal OnlineThe Morning Brie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Banking Bill Negotiators Set Compromise --- Pl...</td>\n",
       "      <td>WASHINGTON -- In an effort to achieve banking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Manager's Journal: Sniffing Out Drug Abusers I...</td>\n",
       "      <td>The statistics on the enormous costs of employ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Currency Trading: Dollar Remains in Tight Rang...</td>\n",
       "      <td>NEW YORK -- Indecision marked the dollar's ton...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   relevance                                           headline  \\\n",
       "0          1              Yields on CDs Fell in the Latest Week   \n",
       "1          0  The Morning Brief: White House Seeks to Limit ...   \n",
       "2          0  Banking Bill Negotiators Set Compromise --- Pl...   \n",
       "3          0  Manager's Journal: Sniffing Out Drug Abusers I...   \n",
       "4          1  Currency Trading: Dollar Remains in Tight Rang...   \n",
       "\n",
       "                                                text  \n",
       "0  NEW YORK -- Yields on most certificates of dep...  \n",
       "1  The Wall Street Journal OnlineThe Morning Brie...  \n",
       "2  WASHINGTON -- In an effort to achieve banking ...  \n",
       "3  The statistics on the enormous costs of employ...  \n",
       "4  NEW YORK -- Indecision marked the dollar's ton...  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "econ = pd.read_csv('datasets/economic_news.csv',\n",
    "                  usecols=[7, 11, 14],\n",
    "                  nrows=200)\n",
    "econ['text'] = econ['text'].apply(lambda x: x.replace('</br>', ''))\n",
    "econ['relevance'] = econ['relevance'].apply(lambda x: 1 if x == 'yes' else 0)\n",
    "econ.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(econ[['text']],\n",
    "                                                   econ['relevance'],\n",
    "                                                   test_size=0.50,\n",
    "                                                   random_state=8675309)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `spaCy` to create a column for the number of monetary-based named entities, followed by using `textblob` to create a polarity score for each article.\n",
    "\n",
    "While we can try to put this into a lambda function, it will probably be easiest in this case to define four functions and apply them.\n",
    "\n",
    "However, because we're sequentially loading up each row of data and processing it, this can be a little bit of a time and memory sink. Expect processing to take some extra time for this step.*\n",
    "\n",
    "* **note**: for spacy, there are faster ways to process the data that do not involve pushing it through Pandas. Investigate the spacy `pipe` method if you're looking to do a larger amount of text transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def number_of_monetary_ents(text):\n",
    "    text = nlp(text)\n",
    "    return len([x.text for x in text.ents if x.label_ == 'MONEY'])\n",
    "\n",
    "def polarity(text):\n",
    "    text = TextBlob(text)\n",
    "    return text.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train['num_monetary'] = X_train['text'].apply(number_of_monetary_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    100.000000\n",
       "mean       1.420000\n",
       "std        2.123367\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        0.000000\n",
       "75%        2.000000\n",
       "max        8.000000\n",
       "Name: num_monetary, dtype: float64"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['num_monetary'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x12231a710>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD3CAYAAADxJYRbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x128d47d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train['num_monetary'].plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train['polarity'] = X_train['text'].apply(polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x122314390>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD3CAYAAADxJYRbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nviQVxNKXpIL8PwGc6p5TD/l3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12229cf98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train['polarity'].describe()\n",
    "X_train['polarity'].plot(kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also pass this into a predictive model to see if these features can assist predicting economic status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98\n",
      "[[78  1]\n",
      " [ 1 20]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99        79\n",
      "          1       0.95      0.95      0.95        21\n",
      "\n",
      "avg / total       0.98      0.98      0.98       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train[['num_monetary', 'polarity']], y_train)\n",
    "print(rfc.score(X_train[['num_monetary', 'polarity']], y_train))\n",
    "predictions = rfc.predict(X_train[['num_monetary', 'polarity']])\n",
    "print(confusion_matrix(y_train, predictions))\n",
    "print(classification_report(y_train, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n",
      "[[56 11]\n",
      " [29  4]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.84      0.74        67\n",
      "          1       0.27      0.12      0.17        33\n",
      "\n",
      "avg / total       0.53      0.60      0.55       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_test['num_monetary'] = X_test['text'].apply(number_of_monetary_ents)\n",
    "X_test['polarity'] = X_test['text'].apply(polarity)\n",
    "print(rfc.score(X_test[['num_monetary', 'polarity']], y_test))\n",
    "predictions = rfc.predict(X_test[['num_monetary', 'polarity']])\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: you're probably wondering _why_ we would try these things when they don't seem to immediately help. During a larger project, we will likely spend days if not weeks on feature extraction and analysis and will want to make as many useful features as possible to make as good a model as possible. Other techniques may involve more nuanced modeling, such as looking at the sequence of parts of speech, etc. Part of this lesson is designed to expose to what is out there so that when faced with a situation where those techniques may be useful, you're aware of their existence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning documents to topics using LDA\n",
    "\n",
    "LDA (Latent Dirichlet Allocation) is an unstructured machine learning technique that iteratively attempts to find clusters of words that are likely to happen together across multiple documents. We interpret the co-occurance of these words together to be analgous to different topics discussed in across a body of documents. \n",
    "\n",
    "LDA works by iteratively guessing how likely a given word is to be part of a given topic until we tell it to stop. \n",
    "\n",
    "This process of updating probabilities will make more sense after next weeks lectures on Bayes, but we'll quickly discuss here and move forward.\n",
    "\n",
    "(Explanation cribbed from [Introduction to Latent Dirichlet Allocation](http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/))\n",
    "\n",
    "We begin by picking a set of documents and a number of topics that we want to generate. One way that we do this is what's known as collapsed Gibbs saampling. We do the following:\n",
    "\n",
    "1. Randomly assign every word in every document to one of the $k$ topics:\n",
    "    - $w$: a word in a document\n",
    "    - $d$: a document\n",
    "    - $k$: a topic\n",
    "2. At this point, every word has a likelihood that they belong in a given a topic, based on the other words in documents that they exist in. \n",
    "3. Iterate through every word in every document and:\n",
    "    1. Assume that every other word has the correct likelihood that they belong to each topic (so, `apple` might have a distribution of `[0.1, 0.1, 0.2, 0.4, 0.2]` for five topics.\n",
    "    2. Look at the likelihood of seeing word $w$ in document $d$ and adjust the topic probabilities as needed\n",
    "    > for example, if there are a lot of words in topic 1 in document $d$ and word $w$ has a stronger likelihood of being in topic 2, because we're assuming that every **other** distribution is correct, we should change our understanding of where word $w$ belongs and tweak it more in favor of belonging to topic 1, not topic 2\n",
    "    \n",
    "You can kind of interpret this with an analogy:\n",
    "\n",
    "> Imagine you move to a new town and you don't know what sort of people you want to hang out with. You imagine there's five different groups of people. You start visiting different places around town (the park, the library, the mall, etc.) and noting who's there. Everytime you go to a place you start adjusting your expectation on who you'll see there (such as the goths constantly are at the mall, so we should expect less and less that they'll show up at the library). This is (very roughly) analgous to what LDA is doing.\n",
    "\n",
    "The name latent dirichlet allocation should begin to make more sense in this context:\n",
    "- latent -- because we have no explicit marker of topic and are grouping things together based on features we are inferring, not seeing\n",
    "- [dirichlet](https://en.wikipedia.org/wiki/Dirichlet_distribution) -- is a type of probability distribution for multiple vectors at once (like a bunch of words towards a bunch of topics)\n",
    "- allocation -- we are allocating different words to different topics via this iterative updating of priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both sklearn and `gensim`, a library we will discuss in the context of a technique called `word2vec`, can handle LDA. However, we'll rely on the sklearn implementation here to reduce the amount of extra work we'll need to do in picking up a new library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's reimport all of the economic news data instead of just the first 200 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEW YORK -- Yields on most certificates of dep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Wall Street Journal OnlineThe Morning Brie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WASHINGTON -- In an effort to achieve banking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The statistics on the enormous costs of employ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NEW YORK -- Indecision marked the dollar's ton...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  NEW YORK -- Yields on most certificates of dep...\n",
       "1  The Wall Street Journal OnlineThe Morning Brie...\n",
       "2  WASHINGTON -- In an effort to achieve banking ...\n",
       "3  The statistics on the enormous costs of employ...\n",
       "4  NEW YORK -- Indecision marked the dollar's ton..."
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "econ = pd.read_csv('datasets/economic_news.csv',\n",
    "                  usecols=[14])\n",
    "econ['text'] = econ['text'].apply(lambda x: x.replace('</br>', ''))\n",
    "econ.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll transform the data using `CountVectorizer` and removing stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8000x46379 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 802395 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "cv.fit(econ['text'].values)\n",
    "X = cv.transform(econ['text'].values)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll instantiate an LDA and fit it to our sparse matrix of words. We have to provide a number of topics that we are looking for (in this case, we're looking for 5 topics). We'll also store the names of the each of the words created during the `CountVectorizer` step for use with the LDA results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajinthchristudas/anaconda/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:508: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
       "             learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n",
       "             mean_change_tol=0.001, n_jobs=1, n_topics=5, perp_tol=0.1,\n",
       "             random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = cv.get_feature_names()\n",
    "lda = LatentDirichletAllocation(n_topics=5)\n",
    "\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of our work will be held in the `.components_` feature. Each row of this array is one of our topics and each column (in order) is a word created by `CountVectorizer`. The values are the relative \"likelihoods\" that the word $w$ should be in topic $t$.\n",
    "\n",
    "> From the sklearn docs, `.components_`: \"can be viewed as pseudocount that represents the number of times word j was assigned to topic i. It can also be viewed as distribution over the words for each topic after normalization\" (we could normalize by dividing row total for that topic). \n",
    "\n",
    "For our purposes, it's enough to say that bigger values means the word belongs more in that topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 46379)\n"
     ]
    }
   ],
   "source": [
    "print(lda.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(lda.components_,\n",
    "                      columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000100</th>\n",
       "      <th>000accepted</th>\n",
       "      <th>000unemployment</th>\n",
       "      <th>001</th>\n",
       "      <th>0010</th>\n",
       "      <th>0013</th>\n",
       "      <th>0015</th>\n",
       "      <th>002</th>\n",
       "      <th>...</th>\n",
       "      <th>then</th>\n",
       "      <th>to</th>\n",
       "      <th>ui</th>\n",
       "      <th>unemployment</th>\n",
       "      <th>ways</th>\n",
       "      <th>well</th>\n",
       "      <th>whether</th>\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.203854</td>\n",
       "      <td>322.206571</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200002</td>\n",
       "      <td>0.200001</td>\n",
       "      <td>0.209062</td>\n",
       "      <td>0.200001</td>\n",
       "      <td>0.200001</td>\n",
       "      <td>0.200001</td>\n",
       "      <td>0.200577</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208041</td>\n",
       "      <td>1.234803</td>\n",
       "      <td>0.200515</td>\n",
       "      <td>0.208134</td>\n",
       "      <td>1.481782</td>\n",
       "      <td>1.289065</td>\n",
       "      <td>1.271349</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200001</td>\n",
       "      <td>0.200001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58.764054</td>\n",
       "      <td>346.612096</td>\n",
       "      <td>2.175198</td>\n",
       "      <td>1.087461</td>\n",
       "      <td>0.200002</td>\n",
       "      <td>2.526059</td>\n",
       "      <td>0.202767</td>\n",
       "      <td>0.206810</td>\n",
       "      <td>0.206782</td>\n",
       "      <td>0.204406</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200001</td>\n",
       "      <td>0.201031</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.201004</td>\n",
       "      <td>0.200001</td>\n",
       "      <td>0.200001</td>\n",
       "      <td>0.201092</td>\n",
       "      <td>0.200689</td>\n",
       "      <td>0.200002</td>\n",
       "      <td>1.421174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.238539</td>\n",
       "      <td>1.702866</td>\n",
       "      <td>0.282684</td>\n",
       "      <td>0.200015</td>\n",
       "      <td>0.200009</td>\n",
       "      <td>0.207014</td>\n",
       "      <td>0.200025</td>\n",
       "      <td>1.347521</td>\n",
       "      <td>1.347592</td>\n",
       "      <td>0.200004</td>\n",
       "      <td>...</td>\n",
       "      <td>1.508752</td>\n",
       "      <td>0.200004</td>\n",
       "      <td>2.278142</td>\n",
       "      <td>0.203604</td>\n",
       "      <td>0.200005</td>\n",
       "      <td>0.203121</td>\n",
       "      <td>0.200004</td>\n",
       "      <td>2.191706</td>\n",
       "      <td>0.200006</td>\n",
       "      <td>0.200004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.380120</td>\n",
       "      <td>778.328327</td>\n",
       "      <td>0.200775</td>\n",
       "      <td>0.202302</td>\n",
       "      <td>0.963732</td>\n",
       "      <td>5.127238</td>\n",
       "      <td>0.927350</td>\n",
       "      <td>0.205122</td>\n",
       "      <td>0.205079</td>\n",
       "      <td>1.417878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.201324</td>\n",
       "      <td>0.200896</td>\n",
       "      <td>0.201229</td>\n",
       "      <td>2.727806</td>\n",
       "      <td>0.201096</td>\n",
       "      <td>0.202102</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.063419</td>\n",
       "      <td>0.202169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.205303</td>\n",
       "      <td>698.603277</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.202788</td>\n",
       "      <td>0.200001</td>\n",
       "      <td>0.201442</td>\n",
       "      <td>0.200001</td>\n",
       "      <td>0.200001</td>\n",
       "      <td>0.200001</td>\n",
       "      <td>0.213637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200008</td>\n",
       "      <td>0.203856</td>\n",
       "      <td>0.201292</td>\n",
       "      <td>0.200149</td>\n",
       "      <td>0.207512</td>\n",
       "      <td>0.204289</td>\n",
       "      <td>0.202598</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200001</td>\n",
       "      <td>0.200001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  46379 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          00         000    000100  000accepted  000unemployment       001  \\\n",
       "0   0.203854  322.206571  0.200000     0.200002         0.200001  0.209062   \n",
       "1  58.764054  346.612096  2.175198     1.087461         0.200002  2.526059   \n",
       "2   0.238539    1.702866  0.282684     0.200015         0.200009  0.207014   \n",
       "3  20.380120  778.328327  0.200775     0.202302         0.963732  5.127238   \n",
       "4   0.205303  698.603277  0.200000     0.202788         0.200001  0.201442   \n",
       "\n",
       "       0010      0013      0015       002    ...       then      to  \\\n",
       "0  0.200001  0.200001  0.200001  0.200577    ...     0.208041  1.234803   \n",
       "1  0.202767  0.206810  0.206782  0.204406    ...     0.200001  0.201031   \n",
       "2  0.200025  1.347521  1.347592  0.200004    ...     1.508752  0.200004   \n",
       "3  0.927350  0.205122  0.205079  1.417878    ...     0.201324  0.200896   \n",
       "4  0.200001  0.200001  0.200001  0.213637    ...     0.200008  0.203856   \n",
       "\n",
       "       ui  unemployment    ways    well  whether        \\\n",
       "0  0.200515        0.208134  1.481782  1.289065   1.271349  0.200000   \n",
       "1  0.200000        0.201004  0.200001  0.200001   0.201092  0.200689   \n",
       "2  2.278142        0.203604  0.200005  0.203121   0.200004  2.191706   \n",
       "3  0.201229        2.727806  0.201096  0.202102   0.200000  0.200000   \n",
       "4  0.201292        0.200149  0.207512  0.204289   0.202598  0.200000   \n",
       "\n",
       "        2        _  \n",
       "0  0.200001  0.200001  \n",
       "1  0.200002  1.421174  \n",
       "2  0.200006  0.200004  \n",
       "3  1.063419  0.202169  \n",
       "4  0.200001  0.200001  \n",
       "\n",
       "[5 rows x 46379 columns]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 46379)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "results.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what words are most likely in each topic, we could sort by the biggest values for each topic.\n",
    "\n",
    "> Every feature has a likelihood of being in a topic, just a very, very low one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "president said tax budget s house federal economic administration government  year congress state deficit bush unemployment states spending years mr new billion yesterday chairman \n",
      "\n",
      "Topic 1\n",
      "stock market stocks new million average dow york shares trading points index jones day today industrial said investors week exchange s rose nasdaq cents year \n",
      "\n",
      "Topic 2\n",
      " s   european europe t cent  canada  kong hong canadian germany south airlines ecb coal air korea we united the german \n",
      "\n",
      "Topic 3\n",
      "year said rates percent rate economy prices inflation economic fed growth market federal dollar reserve new billion month bond markets yesterday quarter months bank price \n",
      "\n",
      "Topic 4\n",
      "said new companies money year years company financial bank says fund people business time funds banks industry market 000 million home securities mr pay billion \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for topic in range(5):\n",
    "    print('Topic', topic)\n",
    "    word_list = results.T[topic].sort_values(ascending=False).index\n",
    "    print(' '.join(word_list[0:25]), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we change the number of topics, we should see the topics change slightly. Remember that because this is an unstructured technique our editorial power as the modeler is important to identify useful topics. \n",
    "\n",
    "However, this provides a powerful tool to create summaries of larger bodies of documents!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Understanding 4 (20 Minutes)\n",
    "\n",
    "In pairs, do the following:\n",
    "\n",
    "1. Rerun the LDA, choosing 10 topics instead of 5. \n",
    "> Make sure that you can explain what each line of the code does to each other. This can be as generic as \"This runs an LDA with 10 components on a matrix of words and documents\" but it's important to be able to explain what a block of code is doing. In particular, make sure that you're able to explain what has happened in this line of code above `word_list = results.T[topic].sort_values(ascending=False).index` -- if you need to, start with the very first portion (`results`) and investigate what each subsequent step does.\n",
    "2. Look at the results of your LDA. How would you summarize what each topic says?\n",
    "3. Does 10 look to be a correct number of topics? Are the same words showing up in multiple topics? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
