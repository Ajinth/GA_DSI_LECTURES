{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import wikipedia\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Review Lab (50 Minutes)\n",
    "\n",
    "There are a lot of moving pieces in NLP and it is worthwhile to keep practicing the techniques we started to acquire yesterday. \n",
    "\n",
    "The first section of our lesson today will be a chance to review those topics and to practice discussing NLP and machine learning together. \n",
    "\n",
    "We'll be using a truncated version of the [Amazon Fine Food Review](https://www.kaggle.com/snap/amazon-fine-food-reviews/data) dataset. For a larger project, we would make use of the full set of data. However, in the interest of processing time, we'll use a randomly sampled set of 10,000 reviews for our training set and an additional 2,000 reviews for our test set.\n",
    "\n",
    "Your goal will be to create a predictive model that classifies a review into a high scoring review (5 stars) or not a high scoring review (1-4 stars). This value is already present in the data under the name `high_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>high_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>177748</td>\n",
       "      <td>O. Brown \"Ms. O. Khannah-Brown\"</td>\n",
       "      <td>5</td>\n",
       "      <td>1190160000</td>\n",
       "      <td>Organic, Kosher, Tasty Assortment of Premium T...</td>\n",
       "      <td>*****&lt;br /&gt;Numi's Collection Assortment Melang...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>155411</td>\n",
       "      <td>Evon Schones \"ACMEFit\"</td>\n",
       "      <td>5</td>\n",
       "      <td>1252454400</td>\n",
       "      <td>YUM!!</td>\n",
       "      <td>I love this product!  We have replaced all oth...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>232693</td>\n",
       "      <td>Kerry Hart</td>\n",
       "      <td>4</td>\n",
       "      <td>1261699200</td>\n",
       "      <td>Good item for when I ate it....</td>\n",
       "      <td>I no longer eat kashi,,,too much sugar. But wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>364352</td>\n",
       "      <td>Leonardo Rafael Camargo \"colombiaaaa\"</td>\n",
       "      <td>3</td>\n",
       "      <td>1214006400</td>\n",
       "      <td>an acquired taste, then it's good</td>\n",
       "      <td>it's not your typical pretzel. I don't eat muc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>491273</td>\n",
       "      <td>D. B. James</td>\n",
       "      <td>1</td>\n",
       "      <td>1348790400</td>\n",
       "      <td>WARNING:  High Fructose Corn Syrup</td>\n",
       "      <td>WARNING:  HIGH FRUCTOSE CORN SYRUP in this pro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id                            ProfileName  Score        Time  \\\n",
       "0  177748        O. Brown \"Ms. O. Khannah-Brown\"      5  1190160000   \n",
       "1  155411                 Evon Schones \"ACMEFit\"      5  1252454400   \n",
       "2  232693                             Kerry Hart      4  1261699200   \n",
       "3  364352  Leonardo Rafael Camargo \"colombiaaaa\"      3  1214006400   \n",
       "4  491273                            D. B. James      1  1348790400   \n",
       "\n",
       "                                             Summary  \\\n",
       "0  Organic, Kosher, Tasty Assortment of Premium T...   \n",
       "1                                              YUM!!   \n",
       "2                    Good item for when I ate it....   \n",
       "3                  an acquired taste, then it's good   \n",
       "4                 WARNING:  High Fructose Corn Syrup   \n",
       "\n",
       "                                                Text  high_score  \n",
       "0  *****<br />Numi's Collection Assortment Melang...           1  \n",
       "1  I love this product!  We have replaced all oth...           1  \n",
       "2  I no longer eat kashi,,,too much sugar. But wh...           0  \n",
       "3  it's not your typical pretzel. I don't eat muc...           0  \n",
       "4  WARNING:  HIGH FRUCTOSE CORN SYRUP in this pro...           0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('./datasets/amazon_train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>high_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>202095</td>\n",
       "      <td>heather fox</td>\n",
       "      <td>5</td>\n",
       "      <td>1247097600</td>\n",
       "      <td>annies bunnies</td>\n",
       "      <td>great deal, these are so yummy , we love them ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>114978</td>\n",
       "      <td>kar</td>\n",
       "      <td>5</td>\n",
       "      <td>1328054400</td>\n",
       "      <td>Love it!!</td>\n",
       "      <td>I enjoy this snack and my son loves it too! It...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>287940</td>\n",
       "      <td>cherina hirsh</td>\n",
       "      <td>5</td>\n",
       "      <td>1281312000</td>\n",
       "      <td>Garlic always</td>\n",
       "      <td>This product is just fantastic for anyone who ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67955</td>\n",
       "      <td>Lucy Dashwood \"Lucy Dashwood\"</td>\n",
       "      <td>5</td>\n",
       "      <td>1321574400</td>\n",
       "      <td>Delicious</td>\n",
       "      <td>The holidays wouldn't be festive without chest...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>454664</td>\n",
       "      <td>brenda peppers</td>\n",
       "      <td>5</td>\n",
       "      <td>1343088000</td>\n",
       "      <td>Toy Poodles Best Meal!</td>\n",
       "      <td>My little poddles love this brand and it's nic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id                    ProfileName  Score        Time  \\\n",
       "0  202095                    heather fox      5  1247097600   \n",
       "1  114978                            kar      5  1328054400   \n",
       "2  287940                  cherina hirsh      5  1281312000   \n",
       "3   67955  Lucy Dashwood \"Lucy Dashwood\"      5  1321574400   \n",
       "4  454664                 brenda peppers      5  1343088000   \n",
       "\n",
       "                  Summary                                               Text  \\\n",
       "0          annies bunnies  great deal, these are so yummy , we love them ...   \n",
       "1               Love it!!  I enjoy this snack and my son loves it too! It...   \n",
       "2           Garlic always  This product is just fantastic for anyone who ...   \n",
       "3               Delicious  The holidays wouldn't be festive without chest...   \n",
       "4  Toy Poodles Best Meal!  My little poddles love this brand and it's nic...   \n",
       "\n",
       "   high_score  \n",
       "0           1  \n",
       "1           1  \n",
       "2           1  \n",
       "3           1  \n",
       "4           1  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('./datasets/amazon_test.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into pairs and work together to do the following.\n",
    "\n",
    "#### Model Generation (30 Minutes)\n",
    "\n",
    "1. Try and create a predictive model that identifies whether a review will be a high-scoring review or not (`high_score` feature in the data). While you can use any of the NLP techniques we discussed yesterday, here are some areas to focus on:\n",
    "\n",
    "1. Should you use `CountVectorizer` or `TfidfVectorizer` to transform your DataFrame?\n",
    "    - Keep stop words or drop them?\n",
    "    - Limit the words going in using `max_df` or `min_df`?\n",
    "2. Apply dimensionality reduction using `TruncatedSVD` or not?\n",
    "    - If you do, how many components should you keep?\n",
    "3. What modeling technique should you use? (`LogisticRegression`, `RandomForestClassifier`, etc.?) How will you change the hyperparameters.\n",
    "\n",
    "Make sure that you are checking your model's performance against the test set.\n",
    "\n",
    "#### Discussion (10 Minutes)\n",
    "\n",
    "A pair from each market will come on mic and discuss how they've chosen to transform their data. Additionally, we'll compare the **mean accuracy** for each market to see who has (at this point) made the most predictive model.\n",
    "\n",
    "#### Model Refinement (10 Minutes)\n",
    "\n",
    "Continue to refine your model or include some choices made by other markets. At the end of these 10 minutes, we'll report each market's best finding (and final model) by mic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "On Training Data\n",
      "0.737\n",
      "[[1087 2555]\n",
      " [  75 6283]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.30      0.45      3642\n",
      "          1       0.71      0.99      0.83      6358\n",
      "\n",
      "avg / total       0.79      0.74      0.69     10000\n",
      "\n",
      "\n",
      "On Test Data\n",
      "0.6905\n",
      "[[ 137  591]\n",
      " [  28 1244]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.19      0.31       728\n",
      "          1       0.68      0.98      0.80      1272\n",
      "\n",
      "avg / total       0.73      0.69      0.62      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample model code for this model -- not optimized!\n",
    "\n",
    "# Use tfidf to take out 1,500 of the most common features with stop words removed\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=1500)\n",
    "tfidf.fit(train['Text'])\n",
    "X = tfidf.transform(train['Text'])\n",
    "\n",
    "# 1,500 features might be quite large, so use TruncatedSVD to reduce to 150 \n",
    "tsvd = TruncatedSVD(n_components=150)\n",
    "tsvd.fit(X)\n",
    "X = tsvd.transform(X)\n",
    "\n",
    "# Use a RandomForestClassifier to create the model\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=7)\n",
    "rfc.fit(X, train['high_score'])\n",
    "\n",
    "# Results on training data\n",
    "\n",
    "print('\\nOn Training Data')\n",
    "print(rfc.score(X, train['high_score']))\n",
    "train_predictions = rfc.predict(X)\n",
    "\n",
    "print(confusion_matrix(train['high_score'], train_predictions))\n",
    "print(classification_report(train['high_score'], train_predictions))\n",
    "\n",
    "# Results on test data\n",
    "\n",
    "test_X = tfidf.transform(test['Text'])\n",
    "test_X = tsvd.transform(test_X)\n",
    "\n",
    "print('\\nOn Test Data')\n",
    "print(rfc.score(test_X, test['high_score']))\n",
    "test_predictions = rfc.predict(test_X)\n",
    "\n",
    "print(confusion_matrix(test['high_score'], test_predictions))\n",
    "print(classification_report(test['high_score'], test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Techniques\n",
    "\n",
    "Today's lesson is designed as an introduction to more advanced libraries or techniques in the realm of Natural Language Processing. These techniques can help you gain even greater accuracy in your modeling, but require more in-depth knowledge of new libraries, new techniques, etc. \n",
    "\n",
    "While we'll be introducing a lot of new material today, we'll be doing our best to limit the discussion to what is most immediately helpful. Each of these libraries and techniques has much more going on than we have time to discuss this week and we encourage you to spend time investigating and understanding these libraries. However, **mastery of these libraries, techniques, and materials introduced today is not required nor expected.**\n",
    "\n",
    "For Project 4 and your Capstone Project, if you are pursuing an NLP approach, these libraries may be very helpful. However, you can get a lot of mileage out of refining and using the sklearn libraries that we discussed yesterday. A good workflow is to try simple answers first and move into more advanced techniques as your use-case requires -- your goals as modelers should be to make best choice that you can, contingent on time and use-case. Having something work, but not be 100% correct is better than having something 100% correct that doesn't work yet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `spacy` to extract parts of speech and named entities\n",
    "\n",
    "[`spaCy`](https://spacy.io/) is a large-scale NLP and text processing library designed to help you extract useful information from text in a speedy and accurate manner. You can imagine it like `CountVectorizer()` turned up to 11. It has underpinnings to C to increase speed and a focus on usability.\n",
    "\n",
    "`spaCy` does *so* much more than we are able to discuss at this point. It is quickly becoming the go-to library for text processing and feature extraction for text. Today, we'll use it to extract parts of speech and named entities.\n",
    "\n",
    "### Parts of Speech\n",
    "\n",
    "We may want to use some derived statistics about parts of speech in our work as Data Scientists, either as the inputs to a model (document _x_ is _y_% verbs) or to help us modify the inputs to a model (we may want to treat `book` the verb differently than `book` the noun). While many different libraries can do parts of speech (`textblob`, which we'll introduce shortly, can do that as well), we'll introduce this using `spaCy`.\n",
    "\n",
    "First, we set up some text from Wikipedia to parse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chicago ( ( listen) or ), officially the City of Chicago, is the third-most populous city in the United States. With over 2.7 million residents, it is also the most populous city in both the state of Illinois and the Midwestern United States. It is the county seat of Cook County. The Chicago metropolitan area, often referred to as Chicagoland, has nearly 10 million people and is the third-largest in the U.S. Chicago has often been called a global architecture capital. Chicago is considered one o\n"
     ]
    }
   ],
   "source": [
    "chicago = wikipedia.page('chicago')\n",
    "\n",
    "print(chicago.content[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create sentences by splitting on `.`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chicago ( ( listen) or ), officially the City of Chicago, is the third-most populous city in the United States',\n",
       " 'With over 2.7 million residents, it is also the most populous city in both the state of Illinois and the Midwestern United States',\n",
       " 'It is the county seat of Cook County',\n",
       " 'The Chicago metropolitan area, often referred to as Chicagoland, has nearly 10 million people and is the third-largest in the U.S',\n",
       " 'Chicago has often been called a global architecture capital']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chicago_sents = chicago.content.split('. ')\n",
    "chicago_sents[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll set up a model in `spaCy`. This lets `spaCy` know what to use as its internal corpus. We name this model `nlp` by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll feed a sentence into `nlp`. This will automatically split the text into a generator of tokens (one token to each word). These tokens will have the part of speech already tagged in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With over 2.7 million residents, it is also the most populous city in both the state of Illinois and the Midwestern United States\n",
      "With ADP\n",
      "over ADP\n",
      "2.7 NUM\n",
      "million NUM\n",
      "residents NOUN\n",
      ", PUNCT\n",
      "it PRON\n",
      "is VERB\n",
      "also ADV\n",
      "the DET\n",
      "most ADV\n",
      "populous ADJ\n",
      "city NOUN\n",
      "in ADP\n",
      "both CCONJ\n",
      "the DET\n",
      "state NOUN\n",
      "of ADP\n",
      "Illinois PROPN\n",
      "and CCONJ\n",
      "the DET\n",
      "Midwestern PROPN\n",
      "United PROPN\n",
      "States PROPN\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(chicago_sents[1])\n",
    "print(doc)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to convert this into a set of part of speech tags, we could add in a little extra Python to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ADP': 4, 'NUM': 2, 'NOUN': 3, 'PUNCT': 1, 'PRON': 1, 'VERB': 1, 'ADV': 2, 'DET': 3, 'ADJ': 1, 'CCONJ': 2, 'PROPN': 4}\n"
     ]
    }
   ],
   "source": [
    "tags = {}\n",
    "for token in doc:\n",
    "    if token.pos_ not in tags.keys():\n",
    "        tags[token.pos_] = 1\n",
    "    else:\n",
    "        tags[token.pos_] += 1\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many more tags to that `spaCy` can provide for us:\n",
    "\n",
    "- Text: The original word text.\n",
    "- Lemma: The base form of the word.\n",
    "- POS: The simple part-of-speech tag.\n",
    "- Tag: The detailed part-of-speech tag.\n",
    "- Dep: Syntactic dependency, i.e. the relation between tokens.\n",
    "- Shape: The word shape – capitalisation, punctuation, digits.\n",
    "- is alpha: Is the token an alpha character?\n",
    "- is stop: Is the token part of a stop list, i.e. the most common words of the language?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text\tLemma\tPOS\tDetailed POS\tDependency\tShape\tIs alphabetic?\tIs stopword?\n",
      "With\twith\tADP\tIN\tprep\tXxxx\tTrue\tFalse\n",
      "over\tover\tADP\tIN\tquantmod\txxxx\tTrue\tTrue\n",
      "2.7\t2.7\tNUM\tCD\tcompound\td.d\tFalse\tFalse\n",
      "million\tmillion\tNUM\tCD\tnummod\txxxx\tTrue\tFalse\n",
      "residents\tresident\tNOUN\tNNS\tpobj\txxxx\tTrue\tFalse\n",
      ",\t,\tPUNCT\t,\tpunct\t,\tFalse\tFalse\n",
      "it\t-PRON-\tPRON\tPRP\tnsubj\txx\tTrue\tTrue\n",
      "is\tbe\tVERB\tVBZ\tROOT\txx\tTrue\tTrue\n",
      "also\tconjurer\tADV\tRB\tadvmod\txxxx\tTrue\tTrue\n",
      "the\tthe\tDET\tDT\tdet\txxx\tTrue\tTrue\n",
      "most\tmuch\tADV\tRBS\tadvmod\txxxx\tTrue\tTrue\n",
      "populous\tpopulous\tADJ\tJJ\tamod\txxxx\tTrue\tFalse\n",
      "city\tcity\tNOUN\tNN\tattr\txxxx\tTrue\tFalse\n",
      "in\tin\tADP\tIN\tprep\txx\tTrue\tTrue\n",
      "both\tboth\tCCONJ\tCC\tpredet\txxxx\tTrue\tTrue\n",
      "the\tthe\tDET\tDT\tdet\txxx\tTrue\tTrue\n",
      "state\tstate\tNOUN\tNN\tpobj\txxxx\tTrue\tFalse\n",
      "of\tof\tADP\tIN\tprep\txx\tTrue\tTrue\n",
      "Illinois\tillinois\tPROPN\tNNP\tpobj\tXxxxx\tTrue\tFalse\n",
      "and\tand\tCCONJ\tCC\tcc\txxx\tTrue\tTrue\n",
      "the\tthe\tDET\tDT\tdet\txxx\tTrue\tTrue\n",
      "Midwestern\tmidwestern\tPROPN\tNNP\tcompound\tXxxxx\tTrue\tFalse\n",
      "United\tunited\tPROPN\tNNP\tcompound\tXxxxx\tTrue\tFalse\n",
      "States\tstates\tPROPN\tNNP\tconj\tXxxxx\tTrue\tFalse\n"
     ]
    }
   ],
   "source": [
    "print('\\t'.join(['Text', 'Lemma', 'POS', 'Detailed POS', 'Dependency',\n",
    "                'Shape', 'Is alphabetic?', 'Is stopword?']))\n",
    "for token in doc:\n",
    "    print('\\t'.join([token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "          token.shape_, str(token.is_alpha), str(token.is_stop)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check For Understanding 1 (10 minutes)\n",
    "\n",
    "With a partner, do the following:\n",
    "\n",
    "1. Pick two different wikipedia articles\n",
    "2. Get the content using the `wikipedia` library\n",
    "3. Using `spacy`, derive the following:\n",
    "    1. How many tokens are in your article?\n",
    "    2. How many parts of speech are in each article? How often do they occur?\n",
    "    3. As a percentage of the total number of tokens, how often does each part of speech occur?\n",
    "4. Does it look like there's a difference across your documents? What other types of documents would have different distributions of parts of speech?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donald John Trump (born June 14, 1946) is the 45th and current President of the United States, in office since January 20, 2017. Before entering politics, he was a businessman and television personality.\n",
      "Trump was born in the New York City borough of Queens. He earned an economics degree from the Wharton School of the University of Pennsylvania. A third-generation businessman, Trump followed in the footsteps of his grandmother Elizabeth and father Fred in running the family real estate company. \n"
     ]
    }
   ],
   "source": [
    "trump = wikipedia.page('Donald Trump')\n",
    "print(trump.content[0:500])\n",
    "trump_model = nlp(trump.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barack Hussein Obama II (US:  ( listen) bə-RAHK hoo-SAYN oh-BAH-mə; born August 4, 1961) is an American politician who served as the 44th President of the United States from 2009 to 2017. The first African American to assume the presidency in American history, he previously served in the U.S. Senate representing Illinois from 2005 to 2008 and in the Illinois State Senate from 1997 to 2004.\n",
      "Obama was born in 1961 in Honolulu, Hawaii, two years after the territory was admitted to the Union as the \n"
     ]
    }
   ],
   "source": [
    "obama = wikipedia.page('Barack Obama')\n",
    "print(obama.content[0:500])\n",
    "obama_model = nlp(obama.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trump_tags = {\n",
    "    'all': 0\n",
    "}\n",
    "\n",
    "obama_tags = {\n",
    "    'all': 0\n",
    "}\n",
    "\n",
    "for token in trump_model:\n",
    "    trump_tags['all'] += 1\n",
    "    if token.pos_ not in trump_tags.keys():\n",
    "        trump_tags[token.pos_] = 1\n",
    "    else:\n",
    "        trump_tags[token.pos_] += 1\n",
    "        \n",
    "for token in obama_model:\n",
    "    obama_tags['all'] += 1\n",
    "    if token.pos_ not in obama_tags.keys():\n",
    "        obama_tags[token.pos_] = 1\n",
    "    else:\n",
    "        obama_tags[token.pos_] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tags, Trump: 16537 Obama: 15936\n"
     ]
    }
   ],
   "source": [
    "print('Total tags, Trump:', trump_tags['all'], \n",
    "     'Obama:', obama_tags['all'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump tags\n",
      "all 16537 100.00\n",
      "PROPN 2393 14.47\n",
      "PUNCT 2183 13.20\n",
      "VERB 2024 12.24\n",
      "NUM 646 3.91\n",
      "DET 1190 7.20\n",
      "ADJ 1271 7.69\n",
      "CCONJ 470 2.84\n",
      "ADP 1996 12.07\n",
      "NOUN 2706 16.36\n",
      "PRON 318 1.92\n",
      "SPACE 269 1.63\n",
      "ADV 414 2.50\n",
      "PART 312 1.89\n",
      "SYM 278 1.68\n",
      "X 65 0.39\n",
      "INTJ 2 0.01\n"
     ]
    }
   ],
   "source": [
    "print('Trump tags')\n",
    "for k, v in trump_tags.items():\n",
    "    print(k, v, '%.2f' % (v * 100 / trump_tags['all']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obama tags\n",
      "all 15936 100.00\n",
      "PROPN 2717 17.05\n",
      "PUNCT 1995 12.52\n",
      "SPACE 263 1.65\n",
      "VERB 1651 10.36\n",
      "ADJ 1131 7.10\n",
      "NOUN 2567 16.11\n",
      "NUM 778 4.88\n",
      "DET 1180 7.40\n",
      "ADP 2129 13.36\n",
      "PART 299 1.88\n",
      "PRON 229 1.44\n",
      "ADV 327 2.05\n",
      "CCONJ 437 2.74\n",
      "SYM 180 1.13\n",
      "X 51 0.32\n",
      "INTJ 2 0.01\n"
     ]
    }
   ],
   "source": [
    "print('Obama tags')\n",
    "for k, v in obama_tags.items():\n",
    "    print(k, v, '%.2f' % (v * 100 / obama_tags['all']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Looks like more Proper Nouns for Obama and more verbs for Trump. Very interesting!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entities\n",
    "\n",
    "Named entities are business, people, countries, or other things that refer to a specific person, place, or thing (think `Apple`, computer manufacturer versus `apple`, delicious crunchy fruit). `spaCy` can identify named entities for us, which we can either highlight or drop from our analyses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've parsed a string of text using `spaCy`, we can call out the named entities using the `.ents` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With over 2.7 million residents, it is also the most populous city in both the state of Illinois and the Midwestern United States <class 'spacy.tokens.doc.Doc'>\n"
     ]
    }
   ],
   "source": [
    "print(doc, type(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over 2.7 million CARDINAL\n",
      "Illinois GPE\n",
      "the Midwestern United States GPE\n"
     ]
    }
   ],
   "source": [
    "for named_entity in doc.ents:\n",
    "    print(named_entity.text, named_entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`spaCy` provides a set of labels for each type of named entity:\n",
    "\n",
    "|Label|Description|\n",
    "|:-- | :-- |\n",
    "|PERSON |\tPeople, including fictional. |\n",
    "|NORP |\tNationalities or religious or political groups. |\n",
    "|FACILITY |\tBuildings, airports, highways, bridges, etc. |\n",
    "|ORG |\tCompanies, agencies, institutions, etc. |\n",
    "|GPE |\tCountries, cities, states. |\n",
    "|LOC |\tNon-GPE locations, mountain ranges, bodies of water. |\n",
    "|PRODUCT |\tObjects, vehicles, foods, etc. (Not services.) |\n",
    "|EVENT |\tNamed hurricanes, battles, wars, sports events, etc. |\n",
    "|WORK_OF_ART |\tTitles of books, songs, etc. |\n",
    "|LAW |\tNamed documents made into laws. |\n",
    "|LANGUAGE |\tAny named language.|\n",
    "|DATE |\tAbsolute or relative dates or periods. |\n",
    "|TIME |\tTimes smaller than a day. |\n",
    "|PERCENT |\tPercentage, including \"%\".\n",
    "|MONEY |\tMonetary values, including unit. |\n",
    "|QUANTITY |\tMeasurements, as of weight or distance. |\n",
    "|ORDINAL |\t\"first\", \"second\", etc. |\n",
    "|CARDINAL |\tNumerals that do not fall under another type. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to see all the unique named entities in the Chicago page, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chicago ( ( listen) or ), officially the City of Chicago, is the third-most populous city in the United States. With over 2.7 million residents, it is also the most populous city in both the state of Illinois and the Midwestern United States. It is the county seat of Cook County. The Chicago metropolitan area, often referred to as Chicagoland, has nearly 10 million people and is the third-largest in the U.S. Chicago has often been called a global architecture capital. Chicago is considered one o'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chicago.content[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The University of Illinois College of Medicine', 'ThyssenKrupp North America', 'Downers Grove', '176.5 m', \"the 'L'\", \"Richard J. Daley's\", 'Southern', 'John Whitfield Bunn', 'the National Mall', 'Constitution', 'Founder of Chicago', 'Paseo Boricua', 'summer', 'Kościuszko', '1968', \"Anish Kapoor's\", 'approximately 153,000', '294', 'Super Bowl', 'Michael Jordan', 'the middle of the night', 'Grace', 'John Root', 'This Great Migration', 'Köppen', 'the University of Chicago Laboratory Schools', 'Langston Hughes', 'the year before', 'the University of Illinois Medical Center at Chicago', 'Caterpillar Inc.', 'La Villita', 'The Regional Transportation Authority', '6 miles', 'three days', 'the year', 'the Town of Chicago', 'Wrigley', 'Montgomery Ward', 'Archer Daniels Midland', 'the University of Chicago Divinity School', '201 meters', 'the early 20th century', 'the 20th century', '29%', 'the year 2016', 'Copernicus', 'Michelin Guide', 'Willis', 'the Port District', 'Sears) Tower', '2019', 'Ellen Gates Starr', '290', '12.5%', 'Poetry.', '1867', 'September 9, 2013', 'Robb Report', '1905', 'GRAB', 'The North Side', '2001', 'nearly 150 percent', 'The Merchandise Mart', 'the Prairie School', 'Checagou', 'February 23, 2011', 'CPS', 'the Morgan Park Academy', 'John H. Stroger', '490', 'D.C.', '42', '400,545', '8', \"Don't Tell Me\", 'Iroquois', 'Du Sable', 'Home Alone', 'Chi-Town', 'Ogden Avenue', 'the Chicago Regional Port District', 'the Jefferson Township', 'Hull House', 'recent years', 'Children, Kenan & Kel', \"Dion O'Banion\", 'the Barack Obama Presidential Center', 'Central and Eastern Europe', '3,200', 'Pizzeria Uno', 'Ukraine', 'the Great Lakes south', 'July daily', 'three', '1917', \"the U.S. Census Bureau's\", 'Roger Brown', '2012', '24.3', 'Grant Achatz', 'Little Italy', 'Logan', 'about 130', 'the Lycée Français de Chicago', 'The Institute for Clinical Social Work', 'World Series', 'Kankakee', 'Defender', '910 m', 'twenty-four', 'Rush University Medical Center', 'the University of Chicago', 'Batman Begins', 'December 2, 1942', 'Hindus', 'WMVP', 'Jane Addams', 'IL-10', '1.308 million', 'the Great Chicago Fire', 'Douglas', 'Orlando', 'Midway', 'John Ashbery', 'the Red and Blue', '28', 'Cabrini', 'National Blue Ribbon School', 'Sufjan Stevens', 'about 300', 'the City Beautiful Movement', 'the Illinois River', 'the Treaty of Greenville', 'the Pitchfork Music Festival', '800,000 barrels', 'close to five years', 'roughly 4%', 'more than 100,000', 'between 1851 and 1920', 'Anish Kapoor', 'DW60', 'UIC', 'seven years later', 'Marshall Field', 'Armour Square', 'Indians', 'the Hyde Park Township', 'Tower', 'the University of Chicago Cultural Policy Center', 'the Museum of Science and Industry', 'WLS', 'Detroit', 'Lincoln Park Zoo', 'McCormick Place', 'Bugs Moran', 'Jean Baptiste Point du Sable', '1993, 2006', 'the Chicago Portage', 'The Illinois Medical District', 'the Red, Blue', \"Lions, Saint-Gaudens's Abraham Lincoln\", 'Chicago Board of Health', 'The South Side', 'New York City', 'Civil War', 'South Side Chicago', '57', '416', 'Las Vegas', '4.0', 'Chase Tower', 'the Lyric Opera', '1893', 'Jens Ludwig', 'Dan Ryan', '415', 'seven years', '1897', \"the United States'\", 'North American', '1945', 'the Near South Side', 'the Jay Pritzker Pavilion', 'Germans', 'the early 1960s', 'Chodzinski', '26 miles', '30,000', 'the Tribune Broadcasting-owned', 'the Moody Bible Institute', '46', 'the Chicago Building', 'Mexican', 'Park District', 'P.D.', 'Walter Payton', '1837', 'the Northern District', 'The Chicago Lincoln', 'The Bob Newhart Show', 'Blackhawks', 'Polish', '75', 'CareerBuilder', '2013', 'the 1880s and 1890s', 'the Chicago City Council', 'the Chicago Architecture Foundation', 'UBS', 'Little Vietnam', 'Marist High School', 'Chicago High School for the Arts', 'about 1900', 'Roosevelt University', 'Armour and Company', 'the West Coast', 'Chicago Innerview', 'Boystown', 'San Francisco', '\\n', 'March 1937', '410', 'U of C', 'Sears Tower', 'eight', 'Washington', 'Newark', '1919', '98', 'the Midwestern United States', 'Boeing', 'Bulls', 'Law', 'The Joffrey Ballet', 'Marc Chagall', 'The Chicago Bulls of', '\\n\\n\\n', 'May 16, 2011', 'North Avenue', 'Chicago Rockford International Airport', '100 acres', 'September 10, 2012', 'Jews', '3,000', 'under 2.7 million', 'The Chicago Police Department', 'Historic Places', 'Rochester', 'DuSable Park', 'Daniel Burnham', 'Department of Transportation', 'Chicago Dance Crash', 'Native American', 'LGBT', 'Labor', 'fewer than 200', 'The Midwestern University Chicago College of Osteopathic Medicine', 'Statue of the Republic', 'American College of Surgeons', 'the Great Lakes region', 'July', 'Metra Electric Line', 'The Chicago Tribune', 'six', 'This American Life', 'Honduran', 'roughly 60%', '579', 'Havliček', 'Alderman Eugene Sawyer', 'Christians', 'Agora', 'the Feltre School in River North', 'as many as 21 days', 'East–West University', 'over 1,000,000', 'the Chicago Police Department', 'Today', 'Caribbean', 'Fox', '22', 'New York', 'Prohibition', 'Julius Rosenwald', '60 miles', 'more than 570', 'Picasso', 'The Chicagoland Chamber of Commerce', '2006', '1965', 'the Evangelical Lutheran Church in America', 'Groupon, Feedburner', 'Music of the Baroque', 'Taste of Chicago', '1876', 'William Rainey Harper', 'the end of 2018', 'Claire', 'U.S. Representatives', 'NFL', 'Saint-Gaudens', 'four consecutive years', 'South Bend', 'the Chicago Region Environmental and Transport Efficiency', '\\nList of fiction', 'Metra', 'Richard J. Daley', 'late September 1687:', 'about 300,000', 'Cook County Jail', '19', 'the Chicago State Cougars (', 'Carl Sandburg', '94', 'Charles B. Atwood', '20', 'Willis Tower', '1901', 'the ESPN Radio-owned', 'the latter half of the 20th century', 'thousands', 'The City Council', 'Democrat', 'Topography', '31', '1955', '1984', 'January 20, 1985', '1930', 'Divvy', 'the University of Illinois at Chicago', 'U.S.', 'Batman', 'the 1980s', 'about 200', 'Midwestern', '1995', \"the Illinois State's\", 'Museum of Science and Industry', 'American Association of Nurse Anesthetists', '2008–2012', 'Maps of Chicago', 'Deerfield', 'Rohe', '600', '41', 'South Side', \"O'Hare Airports\", 'Aging, Health & Society', 'Chicago Cultural Center', 'U.S. News & World Report', 'the Chicago Black Renaissance', '15.94', 'several square miles', 'the summer', '1993', 'Cook County Forest', '943', 'Best Sports City', 'White (', 'the mile', '2010', '92', 'Richard Teller Crane', 'about 2 days', 'Chinese', 'the Goodman Theatre', 'Landing Lakefront Terminal', 'Grundy', 'The University of Chicago Oriental Institute', 'the National Register of Historic Places', '1900 to 1939', 'the Home Insurance Building', '1989', '500', 'Parliament', '672', 'The Chicago Marathon', 'Sinister 2 and Suicide Squad', 'the Feinberg School of Medicine', 'The Blues Brothers', 'Chicago Festival Ballet', 'AP', 'Truman College', 'South Shore', '16', 'twenty-six miles', 'Latin', 'early 1962', 'Ottawa', '66', 'the Soviet Union', 'the Mississippi River', \"Moore's\", 'Masaryk', 'Calumet Harbor', 'Albert Raby', 'North', 'CBS Radio', 'African', '1994', '1929', 'Orbitz', '1933', \"Humboldt Park's Institute of Puerto Rican Arts\", 'Greektown', 'Milwaukee', '44th', 'the forty years', 'HALS', 'Indianapolis', 'Egyptian', 'Brioschi', 'Harry Caray', 'Civic', 'Abbott Laboratories', 'Illinois State', 'Combo', '18th', 'World Marathon Majors', 'Illinois', 'Chicago Med', '205 m', '°', \"Ann & Robert H. Lurie Children's\", 'the 1920s and 1930s', 'Blue Island', 'Iowa', 'Lakeview', 'the first half of 2013', 'MacCormac College', 'the Great Lakes', 'Northwestern University', 'Divergent', '$2.5 billion', 'the Museum of Broadcast Communications', '22.1%', \"O'Hare Airport\", 'Chicagoans', 'Singapore', 'the Chicago Literary Renaissance', 'North Side Chicago\\n', 'the Great Northern Migration (Saar', 'Peace High School', 'every day of the year', 'Superfans', 'the Battle of Fort Dearborn', '27.5 million', '52', 'Sunday', 'the Plan of Chicago', 'Northside College Preparatory High School', 'North America', \"Boyle's The Alarm\", 'Quincy', 'Miami', 'the Museum Campus', 'today', '0.5%', 'HQ', '1980', 'Pontiac', 'Columbian Exposition', '61.7', 'Michigan Avenue', 'Kinzie Street Bridge', 'WMAQ 5', 'The River North Gallery District', 'Meštrović', 'Glencoe', 'Olive–Harvey College', 'Northern American', 'Iroquois County', 'Burnham Park', '1889', 'Standing Lincoln', 'Christopher Columbus', 'December 20, 2014', '54 million', '22nd Street', 'Draugas', 'the MasterCard Worldwide Centers of Commerce', 'Rogers Park', 'Lady Michelle Obama', '448', 'the British School of Chicago', 'IL', 'French', '29.3%', '390', 'Midwest', '24 hours', 'the Chicago Sanitary and Ship Canal', 'the Chicago Sun-Times', 'U.S. Census', 'Commonwealth Edison', 'the century', 'Rosemont', 'the Standard Oil Building', 'Vietnamese', 'August 12, 1833', 'Lincoln Park Conservatory', 'Republicans', 'the Chicago Fire Department', 'Peruvian', 'Hong Kong', 'The City Beautiful', 'Gwendolyn Brooks', 'Columbia College Chicago', '970', '1956', 'Parks', '53', 'the 1850s', 'Beauty and Crate & Barrel', 'the Ford Center for the Performing Arts Oriental Theatre, Bank of America Theatre', 'Megabus', 'the National League', 'COMEX', '1873', 'American College of Healthcare Executives', 'More than half', 'every year', 'Orange, Brown, Purple, Pink', '55th', 'Democratic National Convention', 'Landmarks', 'Financial Times', 'the dawn of the century', 'The McLaughlin Group', 'U.S. Department of Transportation', 'Receiver of Public Monies', 'the North Side', 'Early Edition', 'Lincoln', 'Division', 'Maywood', 'five', \"Edward Kemys's\", 'Brookfield', 'over 20 million', 'Central Chicago\\n', '1860s', 'Streeterville', 'the American League', 'Kearney', 'Pakistani', 'a World Series', 'Lane Technical College Prep High School', 'early summer', 'Oak Park', 'the Wabash Avenue Bridge', 'the 19th century', '1998', 'Poles', 'the John Marshall Law School', 'Programs', 'about 4.48 million', '452', 'Jim Nutt', 'Italian', 'DMOZ\\n', 'Chicago Union Railroad', 'Hyde Park', 'NBA', '\\n\\nSporting News', 'Crusader', '176.2 m', '16 percent', 'Reagan', '1688', 'Stan Mikita', 'the Chicago Reader', '2,695,598', 'over 3.6 million', 'Ecuadorian', 'One', 'World War II', 'the Midway Plaisance', 'Portland', 'Midway Airport', 'Water Tower Place', 'five 50,000 watt', 'NBC', '9th', 'National Register of Historic Places', 'Academy of Nutrition and Dietetics', 'the Obama Foundation', 'the Native American', 'The Chicago Blackhawks', '1966', 'the summer of 2016', 'Leon Golub', 'the 1910s', '1 mile', 'Picnic', '5%', '2014–2016', 'the John Hancock Center', 'the Tribune Media', 'the Illinois International Port District', 'African Americans', '26th Street', 'November', 'more than one', 'Asian', 'CW', 'Greyhound Lines', 'winter', 'two-thirds', 'Irish', '2004', 'eight seasons', 'Robert Lostutter', 'The Chicago Transit Authority', 'Michigan Canal', 'May 4, 1886', 'Millions', 'NPR', 'the last two decades of the 19th century', 'the Chicago Imagists', '1983', 'the Chicago Metropolitan Area', '31.7%', 'sixty years', '32.6%', '397', '10.4', 'Carbondale', 'more than 14%', 'Boston', 'Yellow Cab Companies', '1803', 'the AT&T Plaza', 'American', 'June 4, 1998', 'Little Calumet River', 'WGN America', '762', 'six Stanley Cups', 'NFL Championships', '510', 'Daley Plaza', '160', 'Cuban', 'the Illinois Institute of Art – Chicago', '1679', \"Northern Indiana Commuter Transportation District's\", 'Ogden', 'the Chicago River', '44,103', 'over 8,000 acres', 'Port Huron', 'Harpo Studios', 'Florida', \"O'Hare International Airport\", '4,331', 'Shimer College', 'State Street', 'the 2010–11 season', 'Poetry', 'Miegs Field', 'the Schwinn Bicycle Company', 'Heald Square Monument', 'December 2016', 'Malcolm X College', 'Rahm Emanuel', 'the Eastern United States', 'Watch Dogs', 'United Airlines', 'Sneak Previews', 'U.S. House', 'British', '28.9%', 'Kenwood', '7%', 'the Logan Square Boulevards Historic District', 'the Chicago White Sox', 'June 2017', 'the National Weather Service', 'the Evangelical Covenant Church', '1907', 'first', 'Jackson Park', 'between 1910 and 1920', 'bin', '93', 'Auditorium Building', 'Between 1910 and 1930', 'Frédéric Chopin', 'Sixteen Candles', 'Family Matters', '2014', 'Site Selection', 'the Society for Human Rights', '2,900 shootings—13%', 'the mid-18th century', 'Robert Morris University Illinois', 'Harriet Monroe', 'Lake Calumet', 'the Federal Reserve Bank of Chicago', 'Mies van der', \"Bill Swerski's\", 'the White Sox', 'PRI', 'Adlai Stevenson', 'Ivan Albright', 'Lake Shore Drive', 'The CME Group', '60 m', '0.1%', 'Dubuffet', 'Al Capone', 'January', \"Loyola University Chicago's\", 'Mother Teresa', 'recent 2015', 'The Oprah Winfrey Show', 'McHenry', 'the Museum of Contemporary Art', 'Filipino', 'Indiana', 'The Good Wife', '16th', '580', 'Standing Beast', 'Jane Byrne', 'the Rehabilitation Institute of Chicago', 'Near Eastern', '1795', '1872', 'Puerto Rican', '1977', 'the 1940s', 'Dutch Wheels', 'Fortune 1000', 'Buckingham', '1871', '1924', 'Horizon League', 'the Allstate Arena', 'Louis Sullivan', 'nearly 25,000', 'the Windy City Times', 'The Man (a.k.a', 'the U.S. Chicago', 'more than 6,000', 'Inland Northern American', '2016', 'the McCormick Place Convention Center', 'the American Geographical Society Library\\nHistoric American Landscapes Survey', 'Lincoln Park', 'Metaxa', '468', 'The Roman Catholic Archdiocese', '1912', 'the Near North Side', '77', 'Jr. Hospital of Cook County', '47,408', 'the University of Chicago Medical Center', 'Chicago State University', '29.7%', 'Europe', '125', 'the 1920s', '2014–15', 'Kane', '1974', 'The A.V. Club', 'the Calumet River', '34', '1.6%', '1883', 'West Loop', 'Illinois State Board of Health', 'the Chicago History Museum', '355', 'Lake Michigan', 'Joseph Jefferson Awards', '1992', 'two miles', 'Mount Carmel High School', 'The Dark Knight', 'Rate Field', \"T. S. Eliot's\", 'GE Healthcare', 'first eight seasons', 'De La Salle Institute', 'United Continental Holdings', 'year 2014', 'fifty', 'Rick Bayless', '47,074', '2010, 2013, 2015', '3.6 million', 'the Driehaus Museum', '1997', \"New York's\", 'four years', '1906', 'Northerly Island', 'Western Avenue', 'William Carlos Williams', '3,000 linear feet', 'the Latin School of Chicago', 'Peoples Gas', 'Chicagoland', \"St. Valentine's Day Massacre\", '59%', 'the Chicago School', 'Loyola University Chicago', 'American Dental Association', 'half', '0.6%', 'Muslims', 'Newcity', '2,600', 'about 4 miles', '65', 'the Commodities Exchange Inc.', 'Chase Bank', 'Jean Dubuffet', 'Moose', 'Ravinia Festival', 'Cristo Rey Jesuit High School', 'the end of the 19th century', 'The Second City', 'Jefferson Park', 'the Chicago Mercantile Exchange', \"Ferris Bueller's\", '20th', 'Josephinum Academy', 'Polish Patches', 'William Thompson', 'Graduate Medical Education', 'more than US$13.7 billion', 'Warsaw', 'Flamingo', 'Navy Pier', '1950', '458', 'Lorado Taft', 'since 1992', 'William Butler Yeats', 'the University of Chicago Crime Lab', '35', 'Barack Obama', 'Time Out Chicago', 'more than 77%', 'Dallin', 'San Antonio', 'the U.S. Futures Exchange', 'Czechs', '5.5%', 'the winter season', 'Rush University', 'Union Station', 'the Buehler Center', 'the Chicago Teachers Union', 'Charlie Trotter', 'the Chicago Climate Exchange', 'Lake, McHenry, DuPage', 'Greek', 'Illinoisan', '55 percent', 'Amrany', '91', 'FIFA World Cup', '62.8%', 'Kendall', 'Democratic', 'English', 'Martin Luther King', 'DuPage', '449', 'Helmut Jahn', 'West Ridge', 'the Ping Tom Memorial Park', 'the Peggy Notebaert Nature Museum', 'North Chicago', 'Northwest Side', 'Henri Joutel', 'Yellow', 'the Brookfield Zoo', 'Chicago Academy for the Arts', '10%', 'the National Basketball Association', 'Midway International Airport', 'Trump International Hotel', '109', 'From 1995 to 2008', 'Clark Street', 'The Love Song of J. Alfred Prufrock', 'the UIC Flames', 'SouthtownStar', 'the Chicago Loop', '1833', 'The Frugal Gourmet', 'Miro', '431', 'the Kansas', '98.1%', 'Beltway', 'Peoria', 'Horto', 'Chagall', 'Justice', 'February 1856', 'the Feinberg School of Medicine of Northwestern University', 'Brass Era', '0.2%', '14 million bushels', 'Molly', '40 km', 'the U.S. Department of Education', 'one quarter', 'July 2016', '9', 'Lollapalooza', 'South', 'Indian', '13.2', 'Lutheran', '2008', '1866', 'the Chicago Freedom Movement', 'Chicago Fire', '$1.95 billion', 'CME Group', 'World', '1940 to 1979', 'the New York Mercantile Exchange', '1885', 'the Maxwell Street', 'the Chicago Pride Parade', '860-880 Lake Shore Drive Apartments', 'St. Patrick High School and Resurrection High School', 'M.D', 'the U.S. Army Corps of Engineers', 'The Chicago School of Professional Psychology', 'the Northwest Indian War', 'Northeastern Illinois University', 'Albany Park', 'Democratic Party', 'Cubs', 'Imagist', 'the Art Institute of Chicago', 'American Literature', 'Black Belt', 'Northwest Indiana', 'Accreditation Council for Continuing Medical Education', 'eleven', '1969', 'The Chicago Loop', 'Chicago', 'Grant Park Music Festival', 'Ace Hardware', 'Buckingham Fountain', 'Wiki', 'Seven', 'Chicagou', 'over 4,000', 'Chopin Park', 'the Aon Center', '6th', 'Bridgeview', '105', '55.7', 'Lithuanian Chicagoans', '\\n\\nRenowned Chicago', 'Race Riot', 'WBBM 2', '96', 'Tony Accardo', 'US', 'Blue Shield Association', 'Magdalena Abakanowicz', 'the Chicago Botanic Garden', '13.4%', 'American Community Survey', 'Sears', 'Abraham Lincoln', 'Visitor Information Center', 'State', 'hundreds', 'Tribune', '2002', '25 miles', 'ULTA', 'Grand Calumet River', 'the Arizona Cardinals', '1,045,560', 'Hospital of Chicago', 'the United States', 'the 1840s', \"Wacław Szymanowski's\", 'the Erikson Institute', 'Punky Brewster', 'Illinois Institute of Technology', 'the 1780s', 'Six', 'the late 1920s', 'the American', '42,063', 'the Port of Chicago', 'AL', 'the Cook County Circuit Court', 'Sister', 'ComEd', 'Manhattan Project', '2020', 'seven', 'Crunelle', 'Kraft Heinz', 'Calumet', 'between 1920 and 1930', 'the Pritzker Military Library', '315,000 square feet', '11.09 million', 'Japanese', 'St. Ignatius College Preparatory School', 'Global Cities Index', '58', '50.17 million', '1985', 'The City of Chicago', 'the South Side of Chicago', 'Exposition', '2010–11', 'Brewster', 'night', 'St. Rita of Cascia High School', 'West Sides', '71%', 'Grand Rapids', 'Jack Brickhouse', '13', 'Thorvaldsen', 'the Chicago Opera Theater', 'Korean', 'recent years – the Bears (', 'the Garfield Park Conservatory', 'the Chicago Board of Trade', 'The Museum Campus', '1.2%', 'Lithuanian', '1,200 acres', 'between 1958', 'Electronic Dance Music', 'nearly two-thirds', '32.9%', '196 ft', 'Lawrence Avenue', '1910', '1848', \"the Women's National Basketball Association\", 'Jackson Parks', 'the \"Top Ten Cities', 'During World War', '2011', 'Robot, Mean Girls, Wanted', \"St. Adalbert's Church\", 'RTA', 'over 780,000 square meters', \"Frédéric Chopin's\", '55', '5b', 'Republican National Convention', '1970', 'the Institute of Gerontology', '22%', 'CBS', 'Illinois International Port', '1892', 'Frank Lloyd Wright', 'sixth', '1908', 'North Side', 'Desi', 'The Loop', 'Ojibwe', 'only one', 'Showtime', 'Ed Paschke', 'About 18.3%', 'Vesuvio', '100,000', 'Croatians', '30', 'Alexander Calder', 'the Windy City', 'Ethnically', 'early 2018', 'Albanians', 'Chicago Hope', 'The Chicago Symphony Orchestra', 'Saturday, March 4, 1837', 'New Orleans', '200', 'WNBA', 'June', '8,390,000 square feet', '\\n\\nChicago', 'the Black Belt', '\\nNational Register of Historic Places', 'the National Hockey League', '90', 'the Chicago Cubs', 'the City of Chicago', 'Cleveland', 'Bobby Hull', 'Whitney M. Young', 'American Osteopathic Association', 'Broadway', 'the Flag of Chicago', '1900', 'John Paul II', 'I.O.', 'the Field Museum of Natural History', 'United States', 'since 1945', 'the Adler School of Professional Psychology', '1:1‑scale', 'Bruce DuMont', 'Plensa', '26%', 'the Gangster Era', 'StreetWise', 'Cumulus Media-owned', 'its first hundred years', 'Cadillac Palace Theatre', 'the United Center', 'the Harris Theater for Music and Dance', '0.3%', 'Richard M. Daley', 'the Sears Tower', 'John H. Rauch', 'District of the Federal Reserve', 'Integrys Energy Group', 'Jesse Brown VA Hospital', '10,000', 'the Chicago Bears', 'Exelon', 'Art Institute of Chicago, Museum Campus', 'WGN', '5,800', '18.5', 'Wood Street', 'the Chicago Tribune', 'seventh', 'Soldier Field', '4', 'about 29', 'the \"Original Six\"', 'Richard J. Daley College', '2013–2014 20th Day', 'the Fine Arts Building', 'the Steppenwolf Theatre Company and Victory Gardens Theater', 'Abakanowicz', 'MLB', 'Toyota Park', '75.8', \"People's Parade\", \"Oprah Winfrey's\", 'Harold Washington College', 'Lake Calumet Harbor', 'Obama', 'DePaul College Prep', 'the spring', 'Alinea', 'the Western Hemisphere', '0.4%', 'Taylor Street', 'nearly 10 million', 'East Wacker', 'Late in the 19th century', 'Galena', '2.7 million', 'the Crown Fountain', 'Condé Nast Traveler', 'U.S. Presidents (Eisenhower', 'John Farwell', 'Ford Motor Company', 'July 24, 1934', 'The 2015 year-end', 'Bowman', 'The University of Chicago', 'the Chicago Shakespeare Theater', 'the Polish Museum of America', 'W-02-03', \"Frank Gehry's\", 'John Crerar', '12 m', 'Lake', 'Presbyterian', 'The Port', '54,188', 'South Shore Line', 'Allium', 'the Treaty of Chicago', 'annual', 'the New Negro Movement', '1942', 'one day', 'the last half of the 19th century', 'Andersonville', '750', 'Academy of General Dentistry', 'up to eight', 'CTA', 'Day Off', 'the School of the Art Institute of Chicago', 'Saint Xavier University', 'the Southern United States', '−33', '50', 'African American', 'Salvadoran', 'Merc', '21st', 'NYMEX', 'Kennedy–King College', 'Guatemalan', 'the Adler Planetarium & Astronomy Museum', '88', 'Cloud Gate', 'NHL', 'The Illinois Department of Tourism', 'the 2008–2012 American Community Survey', 'Preston Bradley Hall', 'early 1920s', 'about 3,500', 'Bill Savage', 'Devon Avenue', 'four', 'nine', 'the Century of Progress International Exposition Worlds Fair', 'Signal of Peace', 'more than 4,000', 'NowSecure', 'the Willis Tower', 'Royal Baths', '80', 'The Fourth Presbyterian Church', '29,000 square meters', 'South Halsted Street', 'The White Sox', 'The American Medical Association, Accreditation Council', 'the United States Army', '19th century', 'St. Louis', 'Wisconsin', 'Sox', 'Potawatomi', 'Art Nouveau', 'Great Lakes', '\\n\\nChicago Wilderness\\nList', 'Swedes', 'Walgreens', 'Bennett', '1934', 'Humboldt Park', 'Jaume Plensa', 'the Chicago Cardinals', '21.4%', 'Lithuanians', 'Gateway Theatre', 'Barbara Rossi', 'the Democratic Party', 'the Great Lakes Megalopolis', 'the Ida Crown Jewish Academy', 'about one', 'Pace', '1.1%', \"Fairbanks's\", '1975', '32', '1,000,000', 'Spearman', '97 km', 'Claes Oldenburg', 'Burnham', 'University of Illinois at Chicago', '250 million US gallons', 'Hispanic', 'Thai', 'the Great Depression', 'Two years later', '1812', 'Ten years later', 'North Park University', 'Colombian', 'several decades', 'Discovery', '10 feet', '12', 'Thanksgiving', 'Five', 'Missouri Valley Conference', 'Fort Dearborn', 'one', 'only 15.65', 'Urbana', '3 km', 'Cook County', 'American League', 'the National Football League', 'Amtrak', 'McDonald', 'the Shedd Aquarium', 'Two', 'James Merrill', 'Cook', '3 million', \"the World's Religions\", 'daily', 'the Francis W. Parker School', 'ABC', 'between 2010 and 2040', 'the 1990s', 'the Daily Herald', \"Prentice Women's Hospital\", \"Marshall Field's\", \"Lamb Chop's\", '97', 'Dziennik Związkowy', 'the twentieth century', 'Chicagoua', 'Seattle', 'ImprovOlympic', 'the Western Wheel Company', 'Rockford', '2007', 'Ferris', '2009', 'between 1955 and 1971', 'Opera House', 'Memorial Parks', 'the Catholic Theological Union', 'the Green Bay Packers', 'Ireland', 'Democrats', 'The Lithuanian Opera Company of Chicago', 'the years', 'Prison Break', 'Jesuit', '2015', 'Loop', 'the Chicago Board of Trade Building', 'Jr.', '°F', 'Strachovský', 'Advanced Placement', 'the Chicago Medical School', 'the Illinois and Michigan Canal', '45.0%', 'Edmund Dick Taylor', 'Gary/Chicago International Airport', '1869', 'Enrico Fermi', 'The Robie House', 'Ezra Pound', 'First', '45', 'Fortune Global 500 companies and 17', 'White House', '43', 'Buddhists', '3', '9.7 km', 'World Trade Center', 'Grant Park', 'the Saturday Night Live', 'nearly 400 acres', 'the Lutheran School of Theology at Chicago', 'the Chicago Board Options Exchange', 'Franklin D. Roosevelt', 'Mike', 'Bosnians', '910', 'Each year', 'America', '0.7%', 'Batcolumn', '578', 'Little Seoul', 'the Chicago Public Library', 'the 2006 WNBA season', '1953-54', 'two', 'Solidarity Promenade', 'Gary', 'Onion', 'Robert de LaSalle', 'Navy', 'June 2016', 'third', '2003', 'MLS', '1987', 'Annual', 'Wrigley Field', 'Republican', '24‑hour', 'General Electric', 'Major League', 'Symphony Center', 'the American Hospital Association and Blue Cross', 'Common Council', 'about 75%', 'Northwestern Memorial Hospital', 'Major League Soccer', '2.5', '3.8%', 'June 15, 1835', 'Major League Baseball', 'Millennium Park', '16.14', 'Art Institute of Chicago', 'Serbs', '506', 'Pilsen', \"Benjamin Ferguson's\", 'Objectivist', 'Kennedy', 'second', 'Healthcare', 'the Globalization and World Cities Research Network', '1874', 'Amrany and Rotblatt-Amrany', 'the Federal ATF', 'ten', 'McKenna', 'Society for Clinical Pathology', 'The Chicago Board of Trade', 'Irv Kupcinet', 'Joliet Junior College', 'WBBM', 'fourth', 'Anton Cermak', '190', 'West Town', 'African-American', 'African-Americans', '200th', 'several consecutive days', 'the Public Land Survey System', 'the National Museum of Mexican Art', 'sixteen', 'DePaul University', 'Belmont Avenue', '1 million', 'Wilbur Wright College', 'Latino', 'Cityscape', 'PBS', 'Notes', 'Rice High School', 'LaPorte', '1927', 'WGN-TV', 'the Hubbard Street Dance Chicago', '90%', 'the South Side', 'Seventh', 'the Harris Theater', 'Illinois Congressman', 'the mid-nineteenth century', 'the Kansas City Southern Railway', 'Central Park', 'the DuSable Museum of African American History', 'the 1850s and', '1.72 million', 'the 1980s and 90s', 'over 200', 'Rick Tramonto', 'Uptown', 'fifth', 'about 70', '1926', 'the Chicago Public Schools', 'Derrick Rose', 'Jones College Prep', 'Los Angeles', '233,903', 'Oldenburg', '25%', 'the Southwest Side', 'The Near West Side', '2.7%', 'annually', 'Stephen Douglas', 'Polasek', 'Washington Park', 'USDA', 'Checker', 'the Near West Side', 'about $658.6 billion', 'Magnificent Mile', '1860', '0.40 km2', 'Harold Washington', 'Bronzeville', 'Harlem', '11', 'the 1870s and 1880s', '2005–2009', 'nineteen', 'each year', '79', 'West Side', 'National Louis University', 'about $640 billion', 'CBOE', 'Crown Fountain', 'Montenegrins', 'Catholic', 'Chicago Public Radio', 'Haymarket', 'year', 'Elston', '1979', 'Baxter International', 'GE Transportation', 'Still Standing, The League', 'the Chicago Stock Exchange', 'weeks', 'CSO', 'Stritch School of Medicine', 'Four Seasons', '1850 to 1890', 'Italians', 'Walk Score', '38.9', 'D1', 'Pier', 'summer months', '42 km', '2005', \"North America's\", 'The Willis Tower', 'the Sinaloa Cartel', 'the following decades'} 1545\n"
     ]
    }
   ],
   "source": [
    "chicago_model = nlp(chicago.content)\n",
    "named_entities = []\n",
    "for entity in chicago_model.ents:\n",
    "    named_entities.append(entity.text)\n",
    "print(set(named_entities), len(set(named_entities)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Understanding 2 (10 Minutes)\n",
    "\n",
    "As a market (or as groups in your market), please discuss the following:\n",
    "\n",
    "1. As modelers, we will frequently have to make decisions about how to transform data. If you were using NLP to predict things, would it make sense to keep named entities? Would it make sense to drop them? If it would depend on the circumstances, under what circumstances would it make sense to keep or drop named entities?\n",
    "\n",
    "We'll have a couple of markets come on mic to discuss cases they identified where keeping named entities might make sense and cases where it would not make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `textblob` to do sentiment analysis\n",
    "\n",
    "We can also use a library known as [`textblob`](https://textblob.readthedocs.io/en/dev/) to do a **lot** of text transformation and extraction on our behalf. For our purposes, we are going to use it to analyze text and derive the overall sentiment of the text.\n",
    "\n",
    "Sentiment can be split into two related scales:\n",
    "\n",
    "- subjectivity (0 to 1): scores closer to 0 are more objective in tone, scores closer to 1 are more subjective in tone\n",
    "- polarity (-1 to 1): scores closer to -1 are more negative in tone, closer to 0 are more neutral, and closer to 1 are more positive in tone.\n",
    "\n",
    "Using `textblob` is user-friendly -- pass a string into a `Textblob()` class and then call the `.sentiment.polarity` or `sentiment.subjectivity` attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "really_good_review = '''\n",
    "Goodness me, what a fantastic movie. \n",
    "Caught the world premiere at the Toronto International Film Festival and \n",
    "the entire theater laughed until they cried. \n",
    "Amazingly directed, HILARIOUSLY funny, it blends a 1930s gangster \n",
    "stylishness into a Hong Kong kung fu movie to astonishing results. \n",
    "Who would've thought you could top Shaolin Soccer? \n",
    "Not me, until I saw this movie. Stephen Chow pulled it off. \n",
    "Chow's comedic timing gets better and better with every movie \n",
    "he makes, and while his films are depending more and more on \n",
    "CGI these days, and makes this movie much more a fantasy kung \n",
    "fu film than a traditional one, it hardly detracts from the \n",
    "enjoyable experience. Make it your mission to see this film - \n",
    "it will be one of the most entertaining you ever see. \n",
    "I can't remember the last film I enjoyed myself in more. \n",
    "My eyes still hurt from wiping away tears of laughter. Seriously.  \n",
    "'''\n",
    "\n",
    "really_bad_review = '''\n",
    "Thank you for coming into your performance review Mr. Smith.\n",
    "The company is concerned about your performance. Lately your work has \n",
    "been subpar and at times counter to this company's stated goals.\n",
    "Your demeanor has been aggresive and at times hostile to your \n",
    "fellow coworkers.\n",
    "We have no choice but to terminate your employment, effective \n",
    "immediately. Thank you.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5749999999999998 0.33295454545454545\n",
      "0.7 0.15\n"
     ]
    }
   ],
   "source": [
    "good_review = TextBlob(really_good_review)\n",
    "print(good_review.sentiment.subjectivity, good_review.sentiment.polarity)\n",
    "\n",
    "bad_review = TextBlob(really_bad_review)\n",
    "print(bad_review.sentiment.subjectivity, bad_review.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Understanding 3 (5 Minutes)\n",
    "\n",
    "Individually, please answer the following:\n",
    "\n",
    "1. What type of subjectivity and polarity scores would you expect wikipedia articles to have?\n",
    "2. Confirm your hypothesis by using `textblob` on some of the wikipedia pages we have used so far. Were your thoughts confirmed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3412846858940613 0.10109983766233746\n"
     ]
    }
   ],
   "source": [
    "chicago_textblob = TextBlob(chicago.content)\n",
    "print(chicago_textblob.sentiment.subjectivity, chicago_textblob.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3528265439047681 0.08126754382568327\n"
     ]
    }
   ],
   "source": [
    "trump_textblob = TextBlob(trump.content)\n",
    "print(trump_textblob.sentiment.subjectivity, trump_textblob.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Instructor answer:\n",
    "\n",
    "I expected relatively neutral tones and subjectivity, and even for Trump, those expectations held out\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding these features to DataFrames\n",
    "\n",
    "We may want to include these features into a DataFrame for use in a later model. The most straightforward way to do so would be to apply them using Pandas.\n",
    "\n",
    "Here, we'll make use of the same dataset on economic news that we used yesterday. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relevance</th>\n",
       "      <th>headline</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Yields on CDs Fell in the Latest Week</td>\n",
       "      <td>NEW YORK -- Yields on most certificates of dep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>The Morning Brief: White House Seeks to Limit ...</td>\n",
       "      <td>The Wall Street Journal OnlineThe Morning Brie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Banking Bill Negotiators Set Compromise --- Pl...</td>\n",
       "      <td>WASHINGTON -- In an effort to achieve banking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Manager's Journal: Sniffing Out Drug Abusers I...</td>\n",
       "      <td>The statistics on the enormous costs of employ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Currency Trading: Dollar Remains in Tight Rang...</td>\n",
       "      <td>NEW YORK -- Indecision marked the dollar's ton...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   relevance                                           headline  \\\n",
       "0          1              Yields on CDs Fell in the Latest Week   \n",
       "1          0  The Morning Brief: White House Seeks to Limit ...   \n",
       "2          0  Banking Bill Negotiators Set Compromise --- Pl...   \n",
       "3          0  Manager's Journal: Sniffing Out Drug Abusers I...   \n",
       "4          1  Currency Trading: Dollar Remains in Tight Rang...   \n",
       "\n",
       "                                                text  \n",
       "0  NEW YORK -- Yields on most certificates of dep...  \n",
       "1  The Wall Street Journal OnlineThe Morning Brie...  \n",
       "2  WASHINGTON -- In an effort to achieve banking ...  \n",
       "3  The statistics on the enormous costs of employ...  \n",
       "4  NEW YORK -- Indecision marked the dollar's ton...  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "econ = pd.read_csv('datasets/economic_news.csv',\n",
    "                  usecols=[7, 11, 14],\n",
    "                  nrows=200)\n",
    "econ['text'] = econ['text'].apply(lambda x: x.replace('</br>', ''))\n",
    "econ['relevance'] = econ['relevance'].apply(lambda x: 1 if x == 'yes' else 0)\n",
    "econ.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(econ[['text']],\n",
    "                                                   econ['relevance'],\n",
    "                                                   test_size=0.50,\n",
    "                                                   random_state=8675309)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `spaCy` to create a column for the number of monetary-based named entities, followed by using `textblob` to create a polarity score for each article.\n",
    "\n",
    "While we can try to put this into a lambda function, it will probably be easiest in this case to define four functions and apply them.\n",
    "\n",
    "However, because we're sequentially loading up each row of data and processing it, this can be a little bit of a time and memory sink. Expect processing to take some extra time for this step.*\n",
    "\n",
    "* **note**: for spacy, there are faster ways to process the data that do not involve pushing it through Pandas. Investigate the spacy `pipe` method if you're looking to do a larger amount of text transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def number_of_monetary_ents(text):\n",
    "    text = nlp(text)\n",
    "    return len([x.text for x in text.ents if x.label_ == 'MONEY'])\n",
    "\n",
    "def polarity(text):\n",
    "    text = TextBlob(text)\n",
    "    return text.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train['num_monetary'] = X_train['text'].apply(number_of_monetary_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    100.000000\n",
       "mean       1.420000\n",
       "std        2.123367\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        0.000000\n",
       "75%        2.000000\n",
       "max        8.000000\n",
       "Name: num_monetary, dtype: float64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['num_monetary'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x133b909b0>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD3CAYAAADxJYRbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x133bd4048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train['num_monetary'].plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train['polarity'] = X_train['text'].apply(polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x133bfc470>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD3CAYAAADxJYRbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nviQVxNKXpIL8PwGc6p5TD/l3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f5f0b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train['polarity'].describe()\n",
    "X_train['polarity'].plot(kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also pass this into a predictive model to see if these features can assist predicting economic status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96\n",
      "[[79  0]\n",
      " [ 4 17]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.98        79\n",
      "          1       1.00      0.81      0.89        21\n",
      "\n",
      "avg / total       0.96      0.96      0.96       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train[['num_monetary', 'polarity']], y_train)\n",
    "print(rfc.score(X_train[['num_monetary', 'polarity']], y_train))\n",
    "predictions = rfc.predict(X_train[['num_monetary', 'polarity']])\n",
    "print(confusion_matrix(y_train, predictions))\n",
    "print(classification_report(y_train, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.62\n",
      "[[60  7]\n",
      " [31  2]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.90      0.76        67\n",
      "          1       0.22      0.06      0.10        33\n",
      "\n",
      "avg / total       0.52      0.62      0.54       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_test['num_monetary'] = X_test['text'].apply(number_of_monetary_ents)\n",
    "X_test['polarity'] = X_test['text'].apply(polarity)\n",
    "print(rfc.score(X_test[['num_monetary', 'polarity']], y_test))\n",
    "predictions = rfc.predict(X_test[['num_monetary', 'polarity']])\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: you're probably wondering _why_ we would try these things when they don't seem to immediately help. During a larger project, we will likely spend days if not weeks on feature extraction and analysis and will want to make as many useful features as possible to make as good a model as possible. Other techniques may involve more nuanced modeling, such as looking at the sequence of parts of speech, etc. Part of this lesson is designed to expose to what is out there so that when faced with a situation where those techniques may be useful, you're aware of their existence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning documents to topics using LDA\n",
    "\n",
    "LDA (Latent Dirichlet Allocation) is an unstructured machine learning technique that iteratively attempts to find clusters of words that are likely to happen together across multiple documents. We interpret the co-occurance of these words together to be analgous to different topics discussed in across a body of documents. \n",
    "\n",
    "LDA works by iteratively guessing how likely a given word is to be part of a given topic until we tell it to stop. \n",
    "\n",
    "This process of updating probabilities will make more sense after next weeks lectures on Bayes, but we'll quickly discuss here and move forward.\n",
    "\n",
    "(Explanation cribbed from [Introduction to Latent Dirichlet Allocation](http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/))\n",
    "\n",
    "We begin by picking a set of documents and a number of topics that we want to generate. One way that we do this is what's known as collapsed Gibbs saampling. We do the following:\n",
    "\n",
    "1. Randomly assign every word in every document to one of the $k$ topics:\n",
    "    - $w$: a word in a document\n",
    "    - $d$: a document\n",
    "    - $k$: a topic\n",
    "2. At this point, every word has a likelihood that they belong in a given a topic, based on the other words in documents that they exist in. \n",
    "3. Iterate through every word in every document and:\n",
    "    1. Assume that every other word has the correct likelihood that they belong to each topic (so, `apple` might have a distribution of `[0.1, 0.1, 0.2, 0.4, 0.2]` for five topics.\n",
    "    2. Look at the likelihood of seeing word $w$ in document $d$ and adjust the topic probabilities as needed\n",
    "    > for example, if there are a lot of words in topic 1 in document $d$ and word $w$ has a stronger likelihood of being in topic 2, because we're assuming that every **other** distribution is correct, we should change our understanding of where word $w$ belongs and tweak it more in favor of belonging to topic 1, not topic 2\n",
    "    \n",
    "You can kind of interpret this with an analogy:\n",
    "\n",
    "> Imagine you move to a new town and you don't know what sort of people you want to hang out with. You imagine there's five different groups of people. You start visiting different places around town (the park, the library, the mall, etc.) and noting who's there. Everytime you go to a place you start adjusting your expectation on who you'll see there (such as the goths constantly are at the mall, so we should expect less and less that they'll show up at the library). This is (very roughly) analgous to what LDA is doing.\n",
    "\n",
    "The name latent dirichlet allocation should begin to make more sense in this context:\n",
    "- latent -- because we have no explicit marker of topic and are grouping things together based on features we are inferring, not seeing\n",
    "- [dirichlet](https://en.wikipedia.org/wiki/Dirichlet_distribution) -- is a type of probability distribution for multiple vectors at once (like a bunch of words towards a bunch of topics)\n",
    "- allocation -- we are allocating different words to different topics via this iterative updating of priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both sklearn and `gensim`, a library we will discuss in the context of a technique called `word2vec`, can handle LDA. However, we'll rely on the sklearn implementation here to reduce the amount of extra work we'll need to do in picking up a new library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's reimport all of the economic news data instead of just the first 200 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEW YORK -- Yields on most certificates of dep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Wall Street Journal OnlineThe Morning Brie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WASHINGTON -- In an effort to achieve banking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The statistics on the enormous costs of employ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NEW YORK -- Indecision marked the dollar's ton...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  NEW YORK -- Yields on most certificates of dep...\n",
       "1  The Wall Street Journal OnlineThe Morning Brie...\n",
       "2  WASHINGTON -- In an effort to achieve banking ...\n",
       "3  The statistics on the enormous costs of employ...\n",
       "4  NEW YORK -- Indecision marked the dollar's ton..."
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "econ = pd.read_csv('datasets/economic_news.csv',\n",
    "                  usecols=[14])\n",
    "econ['text'] = econ['text'].apply(lambda x: x.replace('</br>', ''))\n",
    "econ.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll transform the data using `CountVectorizer` and removing stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8000x46379 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 802395 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "cv.fit(econ['text'].values)\n",
    "X = cv.transform(econ['text'].values)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll instantiate an LDA and fit it to our sparse matrix of words. We have to provide a number of topics that we are looking for (in this case, we're looking for 5 topics). We'll also store the names of the each of the words created during the `CountVectorizer` step for use with the LDA results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_topics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-139f1f4f2219>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLatentDirichletAllocation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_topics\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_topics' is not defined"
     ]
    }
   ],
   "source": [
    "feature_names = cv.get_feature_names()\n",
    "lda = LatentDirichletAllocation(n_topics=5)\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of our work will be held in the `.components_` feature. Each row of this array is one of our topics and each column (in order) is a word created by `CountVectorizer`. The values are the relative \"likelihoods\" that the word $w$ should be in topic $t$.\n",
    "\n",
    "> From the sklearn docs, `.components_`: \"can be viewed as pseudocount that represents the number of times word j was assigned to topic i. It can also be viewed as distribution over the words for each topic after normalization\" (we could normalize by dividing row total for that topic). \n",
    "\n",
    "For our purposes, it's enough to say that bigger values means the word belongs more in that topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(lda.components_,\n",
    "                      columns=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what words are most likely in each topic, we could sort by the biggest values for each topic.\n",
    "\n",
    "> Every feature has a likelihood of being in a topic, just a very, very low one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in range(5):\n",
    "    print('Topic', topic)\n",
    "    word_list = results.T[topic].sort_values(ascending=False).index\n",
    "    print(' '.join(word_list[0:25]), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we change the number of topics, we should see the topics change slightly. Remember that because this is an unstructured technique our editorial power as the modeler is important to identify useful topics. \n",
    "\n",
    "However, this provides a powerful tool to create summaries of larger bodies of documents!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Understanding 4 (20 Minutes)\n",
    "\n",
    "In pairs, do the following:\n",
    "\n",
    "1. Rerun the LDA, choosing 10 topics instead of 5. \n",
    "> Make sure that you can explain what each line of the code does to each other. This can be as generic as \"This runs an LDA with 10 components on a matrix of words and documents\" but it's important to be able to explain what a block of code is doing. In particular, make sure that you're able to explain what has happened in this line of code above `word_list = results.T[topic].sort_values(ascending=False).index` -- if you need to, start with the very first portion (`results`) and investigate what each subsequent step does.\n",
    "2. Look at the results of your LDA. How would you summarize what each topic says?\n",
    "3. Does 10 look to be a correct number of topics? Are the same words showing up in multiple topics? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CfU 1\n",
    "\n",
    "# X has not changed so we'll jump to the LDA fitting process\n",
    "# Explanation of each line in comments\n",
    "\n",
    "#instantiates an LDA with 10 components\n",
    "lda = LatentDirichletAllocation(n_components=10) \n",
    "\n",
    "# fits the LDA to our matrix of word counts\n",
    "lda.fit(X) \n",
    "\n",
    "# creates a dataframe of our results. \n",
    "# The columns are each word from CountVectorizer\n",
    "# The rows are the values for each topic\n",
    "# The values in the cells is the relative likelihood that it belongs there\n",
    "results = pd.DataFrame(lda.components_,\n",
    "                      columns=feature_names) \n",
    "\n",
    "# iterate over each topic\n",
    "for topic in range(10):\n",
    "    print('Topic', topic)\n",
    "    # transpose results (so that the topics are the columns)\n",
    "    # isolate the column for the specific topic we want\n",
    "    # sort from biggest to smallest\n",
    "    # get the indices (which are the words)\n",
    "    # the values for the cells are not particularly useful\n",
    "    word_list = results.T[topic].sort_values(ascending=False).index\n",
    "    # join the first 25 words together as they are the strongest topic words\n",
    "    print(' '.join(word_list[0:25]), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Instructor answer for 2 and 3\n",
    "\n",
    "Topic interpretations:\n",
    "\n",
    "Topic 0 -- looks like economic reports \n",
    "Topic 1 -- central bank discussions\n",
    "Topic 2 -- global trade, specifically asia and europe\n",
    "Topic 3 -- elections and political parties\n",
    "Topic 4 -- tech companies and stocks (maybe IPOs?)\n",
    "Topic 5 -- garbled words\n",
    "Topic 6 -- international news, sports, Israeal\n",
    "Topic 7 -- actual stock market movements\n",
    "Topic 8 -- a mix of automotive industry and international aid\n",
    "Topic 9 -- government budgets and deficits\n",
    "\n",
    "10 topics look pretty cohesive and easy to understand. We may want one or two more as some of them are a little mixed, but this works out decently well. We also apparently have some encoding issues that would be good to iron out.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
