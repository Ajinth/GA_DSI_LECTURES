{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "from sklearn.metrics import confusion_matrix, silhouette_score\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, cophenet, fcluster\n",
    "from scipy.spatial.distance import pdist\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is it?\n",
    "\n",
    "Hierarchical clustering, like k-means clustering, is another common form of clustering analysis. With this type of clustering - we seek to do exactly what the name suggests: build hierarchies of links that ultimately form clusters. Once these links are determined, they are displayed in what is called a dendrogram - a graph that displays all of these links in a hierarchical manner.\n",
    "\n",
    "![](./images/denex.png)\n",
    "\n",
    "To find clusters in a dendrogram, we can cut the graph at a cutoff of our choosing and use that as our mechanism identify clusters - we'll go over this later in the lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is Hierarchical Clustering Different from K-Means Clustering?\n",
    "\n",
    "Much like we learned about k-means clustering, hierarchical clustering is another method for classifying our data. If you recall, in k-means clustering, the algorithm groups data into a pre-defined set of clusters based on various attributes. However in the case of hierarchical clustering, the algorithm builds classifications trees of the data that merges groups of similar data points.\n",
    "\n",
    "With k-means, the boundaries between the various clusters are distinct and independent (see graph), whereas in hierarchical clustering, there are shared similarities between those groups that are represented by the classification tree. Going further - unlike with k-means, hierarchical clustering does not require you to define \"k\" as an input.\n",
    "\n",
    "![](./images/kmeans.png)\n",
    "\n",
    "All of these attributes can lend themselves to certain clustering situations - for instance, hierarchical clustering is more beneficial for smaller datasets - think about the complexity of a dendrogram from a 1000 point dataset! Likewise, this form of clustering works better when we have binary data or dummy variables: as k-means computes means in forming clusters, performing k-means on a dataset with a significant amount of variables would skew the resulting clusters and distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Does Hierarchical Clustering Work? - Demo\n",
    "\n",
    "In hierarchical clustering, instead of clustering in one step, the clusters are determined over a set of steps. At each step, the algorithm makes a choice to pair up two points based on the surrounding datapoints, with the ultimate goal that these best choices will lead to the best choice of clusters overall. Given the algorithm's method of calculating linkages based on immediate datapoints, it's known as a _greedy_ algorithm.\n",
    "\n",
    "There are two forms of hierarchical clustering: **agglomerative hierarchical clustering** and **divisive hierarchical clustering**, also known as bottom-up and top-down clustering.\n",
    "\n",
    "![](./images/hier.png)\n",
    "\n",
    "Today, we're going to look at one of the most fundamental methods for agglomerative hierarchical cluster, known as linkage clustering. Linkage clustering iterates through datapoints and computes the distance between groups by computing the distance between two neighboring datapoints, using the nearest neighbor technique that was also used by KNN.\n",
    "\n",
    "To think about the difference between agglomerative vs divisive, with the former we start with the leaves of the tree and build the trunk, and with the latter we start with the trunk of the tree and build the leaves. Both methods are applicable when using hierarchical clustering, it's just a matter of computational preference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering in Python\n",
    "\n",
    "Implementing hierarchical clustering in Python involves calling a function from the SciPy library:\n",
    "\n",
    "```python\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "Z = linkage(X, 'ward')\n",
    "``` \n",
    "\n",
    "Here, `X` represents the matrix of data that we are clustering, and `ward` tells our algorithm which method to use to calculate distance between our newly formed clusters - in this case Ward's Method (which seeks to minimize the variance when forming clusters). \n",
    "\n",
    "After we cluster, we can calculate the dendrogram using the `dendrogram()` function from SciPy.\n",
    "\n",
    "To check how well our algorithm has measured distance, we can use a value known as the _cophenetic correlation coefficient_. This metric, which measures the height of the dendrogram at the point where the last two branches merge, can tell us how well the dendrogram has measured the distance between data points in the original dataset (compared to their non-hierarchical distance) and is a helpful measure to see how well our clustering test has run.\n",
    "\n",
    "> In other words, is our algorithm gluing points that are far apart from each other because it has no choice left at that point or is it doing so because that would be the most appropriate pair to make?\n",
    "\n",
    "```python\n",
    "from scipy.cluster.hierarchy import cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "c, coph_dists = cophenet(Z, pdist(X))\n",
    "```\n",
    "\n",
    "Here, we call the cophenetic function using `cophenet` from SciPy and apply it to our clustered set, `Z`, and the distance of our original set, `X`.\n",
    "\n",
    "> For interpretation, higher values of the _cophenetic correlation coefficient_ are better -- it means that our algorithm has correctly put together clusters that are cohesive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Your Analysis & Handling Data in SciPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the `iris` dataset, loading it in from the `sklearn.datasets` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a couple of plots using Seaborn to give us a sense of how the clusters might naturally fall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.jointplot(x='sepal length (cm)', y='sepal width (cm)', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.jointplot(x='petal length (cm)', y='petal width (cm)', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's convert our data to a matrix to pass to the clustering algorithm - the matrix makes it easier for our algorithm to compute distance. Additionally, we'll standardize our inputs (because we're looking at distance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "ss.fit(df)\n",
    "X = ss.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll implement the actual clustering algorithm using the ward method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Z = linkage(X, 'ward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can calculate the cophenetic correlation coefficient to see how well our algorithm has measured the distances between the points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c, coph_dists = cophenet(Z, pdist(X))\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This correlation is near 1.0, which suggests that our clustering is fairly cohesive (i.e., our clusters are coherent and well-formed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot the dendrogram. To do this, we call dendrogram from scipy.cluster.hierarchy and input our links, and rotate the labels so we can view the graph in a more organized matter. We can then plot the denrogram with pyplot from matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,20))\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('Index Numbers')\n",
    "plt.ylabel('Distance')\n",
    "dendrogram(\n",
    "    Z,\n",
    "    leaf_rotation=90.,  \n",
    "    leaf_font_size=8.,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to interpret this graph:\n",
    "\n",
    "1. The values on the x-axis are the **index number** of each point.\n",
    "2. During each fitting step, the _two closest points_ are paired together.\n",
    "3. The height of the horizontal line joining two points together is the **ward** distance between those two points.\n",
    "4. Each successive step of the algorithm pairs together either:\n",
    "  - the next closest pair of points\n",
    "  - the next closest pair of clusters\n",
    "  - the next closest cluster and a point \n",
    "5. As the clusters in a given point are farther and farther from each other, the distance between them has to be bigger.\n",
    "6. If we draw a horizontal line at a given distance, we can see all the clusters that have been made below that distance. Our job as modelers is to pick an appropriate distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our purposes, a distance of 10 looks to be a place where we can make a good set of clusters, so we'll set it there and have the algorithm tell us which points fall into which clusters at that point.\n",
    "\n",
    "> We may choose where to put that split at one of two ways:\n",
    ">  - How many clusters do we need for a given purpose?\n",
    ">  - What separating boundary looks the best?\n",
    "\n",
    "`clusters` will give us, in order of data points, what cluster they have been assigned to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_dist = 10\n",
    "clusters = fcluster(Z, max_dist, criterion='distance')\n",
    "clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's plot our data and assign the class labels as the color:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1], c=clusters, cmap='copper')\n",
    "plt.xlabel('First Column')\n",
    "plt.ylabel('Second Column')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Understanding 1 (20 Minutes)\n",
    "\n",
    "For this Check for Understanding, we will be using a (very small) dataset on countries and the languages that their citizens speak. Each row in this dataset is a country and the values in each column are the percent of speakers for that language in the country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "languages = pd.read_csv('datasets/languages.csv')\n",
    "languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task for this Check for Understanding is to do the following:\n",
    "\n",
    "1. Drop `country` from this dataset as it is not informative. Standardize the remaining data.\n",
    "2. Create a correlation table and heatmap. Are there any languages that appear particularly related? \n",
    "3. Use scipy to hierarchically cluster countries together.\n",
    "4. What is the cophenetic correlation coefficient of your clusters? Does this value suggest that good clusters or bad clusters were made?\n",
    "4. Display a dendrogram of the results -- in your estimation, where is a good distance to draw the clusters at?\n",
    "5. **Bonus**: Using the clusters that you have made, use Pandas and plotting to investigate your clusters. What types of countries have been grouped together?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering in sklearn\n",
    "\n",
    "Scikit-learn also offers a hierarchical clustering algorithm known as [`AgglomerativeClustering`](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html):\n",
    "\n",
    "`AgglomerativeClustering` optionally takes a few different parameters, but most important is the `n_clusters` parameter -- once the algorithm has fit the clusters (such as with the dendrogram), it prunes back those clusters until it hits the number of clusters specified by that parameter. It defaults to using Ward linkage, but this can be changed if required.\n",
    "\n",
    "There's a little more control (but less deployability) in the scipy version (as well as the very useful dendrogram) but ultimately, the scikit-learn version may be easier to use, given how similar it is to other sklearn techniques we've covered in the past."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll show a brief example using the Iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll begin by standardizing the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "ss.fit(df)\n",
    "X = ss.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll ask `AgglomerativeClustering` to return back two clusters for Iris:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ag = AgglomerativeClustering(n_clusters=2)\n",
    "ag.fit(X)\n",
    "predicted_labels = ag.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could plot these labels as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=predicted_labels, cmap='copper')\n",
    "plt.xlabel('First column')\n",
    "plt.ylabel('Second column')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, sklearn's form of agglomerative clustering doesn't really give us much of an opportuntity to consider how good (or poorly) 2 clusters fits (versus some other number of clusters). Scipy's `dendrogram` gives us a much better visual display and cna help us make decisions that way. \n",
    "\n",
    "I'll typically use both -- scipy to help me figure out what a good amount of clusters might be (for example, if there is a very large distance separating the next set of clusters from being joined, that might be a good cutoff). I'll do the actual fitting in sklearn to take advantage of things like a standard API, `Pipeline` objects, etc. afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Understanding 2 (20 Minutes)\n",
    "\n",
    "In this Check for Understanding, we will be using a dataset from the UCI MLR on weekly prices on the [Dow Jones](https://archive.ics.uci.edu/ml/datasets/Dow+Jones+Index) -- it contains 750 weekly observations of stocks on the Dow Jones stock index. Those stocks available during the data were collected are:\n",
    "\n",
    "\t\t3M\t\t \tMMM\n",
    "\t\tAmerican Express \tAXP\n",
    "\t\tAlcoa\t\t\tAA\n",
    "\t\tAT&T \t\t\tT\n",
    "\t\tBank of America\t\tBAC\n",
    "\t\tBoeing \t\t \tBA\n",
    "\t\tCaterpillar \t \tCAT\n",
    "\t\tChevron \t \tCVX\n",
    "\t\tCisco Systems \t\tCSCO\n",
    "\t\tCoca-Cola \t \tKO\n",
    "\t\tDuPont \t\t \tDD\n",
    "\t\tExxonMobil \t \tXOM\n",
    "\t\tGeneral Electric \tGE\n",
    "\t\tHewlett-Packard\t\tHPQ\n",
    "\t\tThe Home Depot \t \tHD\n",
    "\t\tIntel \t\t \tINTC\n",
    "\t\tIBM \t\t \tIBM\n",
    "\t\tJohnson & Johnson \tJNJ\t\n",
    "\t\tJPMorgan Chase \t \tJPM\n",
    "\t\tKraft\t\t\tKRFT\n",
    "\t\tMcDonald's \t\tMCD\n",
    "\t\tMerck \t\t \tMRK\n",
    "\t\tMicrosoft \t \tMSFT\n",
    "\t\tPfizer \t\t \tPFE\n",
    "\t\tProcter & Gamble \tPG\n",
    "\t\tTravelers \t \tTRV\n",
    "\t\tUnited Technologies \tUTX\n",
    "\t\tVerizon \t \tVZ\n",
    "\t\tWal-Mart \t \tWMT\n",
    "\t\tWalt Disney \t \tDIS\n",
    "        \n",
    "And the features in the dataset are:\n",
    "\n",
    " - `quarter`:  the yearly quarter (1 = Jan-Mar; 2 = Apr=Jun).\n",
    " - `stock`: the stock symbol (see above)\n",
    " - `date`: the last business day of the work (this is typically a Friday)\n",
    " - `open`: the price of the stock at the beginning of the week\n",
    " - `high`: the highest price of the stock during the week\n",
    " - `low`: the lowest price of the stock during the week\n",
    " - `close`: the price of the stock at the end of the week\n",
    " - `volume`: the number of shares of stock that traded hands in the week\n",
    " - `percent_change_price`: the percentage change in price throughout the week\n",
    "percent_chagne_volume_over_last_wek: the percentage change in the number of shares of stock that traded  - `hands` for this week compared to the previous week\n",
    " - `previous_weeks_volume`: the number of shares of stock that traded hands in the previous week\n",
    " - `next_weeks_open`: the opening price of the stock in the following week\n",
    " - `next_weeks_close`: the closing price of the stock in the following week\n",
    "percent_change_next_weeks_price: the percentage change in price of the stock in the following week  - `days_to_next_dividend`: the number of days until the next dividend\n",
    " - `percent_return_next_dividend`: the percentage of return on the next dividend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stocks = pd.read_csv('datasets/dow_jones_index.csv')\n",
    "stocks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please attempt the following questions on your own:\n",
    "\n",
    "1. What data cleaning (missing data, identifiers, data encoded as strings, etc.) do you need to do to prepare this for clustering? Make those changes. Standardize your remaining features.\n",
    "2. Use scipy to cluster and answer the following questions:\n",
    "    1. What is the cophenetic correlation coefficient? What does it tell you about the clusters you've made?\n",
    "    2. Plot a dendrogram. What appears to be a good number of clusters? \n",
    "3. Use sklearn to fit a model with your chosen number of clusters (number of stocks in the dataset). Use Pandas and / or plotting to investigate the clusters you have made. What, if any, trends do you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Evaluations\n",
    "\n",
    "As we don't have a real version of \"the truth\" (like we do in a structured machine learning world), it's important to really dive into how we assess and discuss clusters that we have created.\n",
    "\n",
    "In this section, we're going to identify a few techniques that you can do to evaluate how well (or poorly) your clusters fit. However, because we don't know the \"true\" values for our clusters, ultimately **we** will be the ones making the determination of whether or not to keep fitting a clustering algorithm. There is no objective truth that we'll be able to use to judge our goodness of fit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visually Evaluating Clusters\n",
    "\n",
    "When evaluating clusters, the best and easiest method when the data allows is to visually examine the output of the clustering algorithm. After we run the algorithm, we can plot the clusters to see where the clusters are and if they make sense. \n",
    "\n",
    "It can be tricky to plot out data that has >2 dimensions (as sadly, we're limited to 3 dimensions for sight) -- there are some options around that (in practice, dimensionality reduction techniques such as t-SNE or PCA, or plotting various dimensions against each other) but those techniques can also be sensitive to hyperparameters or feature selection.\n",
    "\n",
    "Scatterplots and boxplots (in particular) can be very useful in these cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Clusters without a ground truth -- Silhouette Score\n",
    "\n",
    "The silhouette evaluates the validity of your model's clusters based on the ideas of cohesion and separation.\n",
    "\n",
    "#### Cohesion\n",
    "\n",
    "Cohesion measures clustering effectiveness within a cluster. It is calculated:\n",
    "\n",
    "- C: a given cluster\n",
    "- c: the centroid of that cluster\n",
    "- d: distance metric (such as euclidian)\n",
    "- x: observation\n",
    "- $\\in$: a set of / containing\n",
    "\n",
    "$$ \\hat{C}(C_i) = \\sum_{x \\in C_i} d(x, c_i)$$\n",
    "\n",
    "#### Separation\n",
    "\n",
    "Separation measures clustering effectiveness between clusters. It is calculated (using the same notation as above):\n",
    "\n",
    "$$ \\hat{S}(C_i, C_j) = d(c_i, c_j) $$\n",
    "\n",
    "($i$ and $j$ denote two different clusters)\n",
    "\n",
    "![](images/cohesion.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silhouette coefficient\n",
    "\n",
    "The silhouette coefficient combines the cohesion and the separation into a single metric:\n",
    "\n",
    "$$ SC_i = \\frac{b_i - a_i}{\\max({a_i, b_i})}$$\n",
    "\n",
    "- $a_i$ = mean intra-cluster distance of the sample to others within the cluster (cohesion)\n",
    "- $b_i$= the mean nearest-cluster distance from the sample to those within the nearest non-assigned cluster (separation)\n",
    "\n",
    "The coefficient ranges from -1 to 1:\n",
    "\n",
    "- A score of 1 indicates the maximum cohesion and separation of clusters.\n",
    "- A score of -1 indicates the minimum cohesion and separation of clusters.\n",
    "\n",
    "![](./images/sil_coef_visual.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you calculate the silhouette score using sklearn you get out a single number. This is the average silhouette score for all of the individual observations.\n",
    "\n",
    "In general, we want separation to be high and cohesion to be low. This corresponds to a value of SC close to +1.\n",
    "\n",
    "A negative silhouette coefficient means the cluster radius is larger than the space between clusters, and thus clusters overlap. Another way to think about this is that negative values indicate that non-assigned clusters are more similar than the assigned cluster.\n",
    "\n",
    "We can use the silhouette coefficient to determine an number of clusters. **It is important to keep in mind that this is still a subjective measure and not an official measure of quality. It can help you but it's no substitute for knowing your data.**\n",
    "\n",
    "Visually inspecting your data and evaluating how the silhouette changes across numbers of clusters can give you a rough sense of the quality for K clusters in terms in cohesion and seperation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Elbow Method\n",
    "\n",
    "If we are using `KMeans`, we can also make use of the Elbow method. In essence, we will plot the average distance of all points from their closest centroid for a number of candidate clusters. When doing this, we should see an elbow or kink point where the rate of change of the average distance slopes off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick example with the Iris prices dataset that we worked with earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "ss.fit(df)\n",
    "X = ss.transform(df)\n",
    "\n",
    "distances = []\n",
    "for clusters in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=clusters)\n",
    "    kmeans.fit(X)\n",
    "    distances.append(kmeans.score(X))\n",
    "    \n",
    "plt.plot(list(range(2, 11)), distances)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Average Distance to Centroid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At k=3, we see a stark change in the rate of change (with a more minor one at 5) -- these might be good values to investigate. \n",
    "\n",
    "One major challenge with this approach is that there is no ambiguous location to choose. In most cases, this will be a good place for a hint more than anything else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Understanding (15 Minutes)\n",
    "\n",
    "1. Pick one of the other two datasets we have used in this section (Stocks or Language)\n",
    "2. Use the elbow method to find a couple of number of potential clusters (using KMeans)\n",
    "3. Report the average silhouette score for each of those potential number of clusters -- is one better or worse than the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating clusters when the ground truth is available\n",
    "\n",
    "The silhouette score is a particularly useful metric in that it does not require us to have the true labels for the clusters (which is most often the case if we need to do clustering to begin with!).\n",
    "\n",
    "Very, very rarely we have some true labels available (though, that begs the question as to whether or not it makes sense to use an unstructured technique when your data can be modeled with a structured technique). When the true labels are available, there are other methods we can use to evaluate the performance of our clustering algorithm and choice of clusters.\n",
    "\n",
    "- Completeness Score\n",
    "- Homogeneity\n",
    "- V Measure Score\n",
    "- Mutual Information Score\n",
    "\n",
    "For each of these techniques, I'll score how well it performs on the Iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "ss.fit(df)\n",
    "X = ss.transform(df)\n",
    "\n",
    "targets = iris.target\n",
    "\n",
    "km = KMeans(n_clusters=3)\n",
    "km.fit(X)\n",
    "\n",
    "predicted_labels = km.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Completeness score\n",
    "\n",
    "Completeness indicates that all members of a given class are assigned to the same cluster.\n",
    "\n",
    "A clustering result satisfies completeness if all the data points that are members of a given class are elements of the same cluster. (If a cluster contains all of the data points of a single class.)\n",
    "\n",
    "Score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import completeness_score\n",
    "completeness_score(targets, predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like a pretty good set of clusters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Homogeneity\n",
    "\n",
    "Homogeneity indicates each cluster contains only members of a single class.\n",
    "\n",
    "A clustering result satisfies homogeneity if all of its clusters contain only data points which are members of a single class.\n",
    "\n",
    "Score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import homogeneity_score\n",
    "homogeneity_score(targets, predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, this scores fairly well on homogeneity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V measure score\n",
    "\n",
    "The V measure score is a combination of the homogeneity and completeness metrics -- it is 2 times the quotient of the completeness and homogeneity scores divided by their sum (in other words, the harmonic mean of those two scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import v_measure_score\n",
    "v_measure_score(targets, predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because V Measure Score is the harmonic mean of two strongly performing metrics, this is (unsurprisingly) high as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual information score\n",
    "\n",
    "Mutual information measures the agreement between two assignments (clusters, in this case).\n",
    "\n",
    "There are various mutual information scores in Sklearn. Be wary of the basic `mutual_info_score` as the output is not normalized, thus making it harder to compare across runs:\n",
    "\n",
    "$${\\displaystyle MI(X;Y)=\\sum _{y\\in Y}\\sum _{x\\in X}p(x,y)\\log {\\left({\\frac {p(x,y)}{p(x)\\,p(y)}}\\right)},}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$MI(X;Y)$ is the mutual information of group $X$ and $Y$ which can be predicted vs. true.\n",
    "\n",
    "$P(x,y)$ is the probability that observation $x$ in group $X$ is the same as observation $y$ in group $Y$. In other words, the probability that the predicted value (or cluster) and true value are the same.\n",
    "\n",
    "$P(x)$ is the probability of observation $x$ in group $X$, and $P(y)$ is the probability of observation $y$ in group $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mutual_info_score\n",
    "mutual_info_score(targets, predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it is not guaranteed to be between 0 and 1, it's better to use the adjusted mutual information score, which will normalize the result so that it always comes out be between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "adjusted_mutual_info_score(targets, predicted_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
