{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Timeseries Modeling\n",
    "\n",
    "_Adapted for DSI-EAST-1 by Justin Pounders (ATL)_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Make timeseries \"stationary\" and test for stationarity\n",
    "- Build and test ARMA models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals of *Times Series Analysis*\n",
    "\n",
    "Like \"usual\" statistical analysis\n",
    "\n",
    "- *Inference*: underlying mechanisms represented by observations\n",
    "- *Forecasting*: prediction of the future\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Time Series: Stationarity\n",
    "\n",
    "> Many models we will explore will assume stationarity.\n",
    "\n",
    "*Stationarity* is the assumption that the **time series** has constant mean and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make some fake data\n",
    "\n",
    "n_points = 201\n",
    "t = np.linspace(0, 100, n_points)\n",
    "\n",
    "r = (t/50)**2\n",
    "s = np.sin(t*2*np.pi/10)\n",
    "e = np.random.normal(scale=0.25, size=n_points)\n",
    "y = r + s + e\n",
    "\n",
    "d0 = datetime.now()\n",
    "dt1 = timedelta(1)\n",
    "t = [d0+dt1*i for i in t]\n",
    "df = pd.DataFrame(y, index=t, columns=['signal'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(t, y)\n",
    "plt.xlabel('Time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(C1) **Is the above *stationary***?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Questions:** \n",
    "- *What is the long-term behavior of my series?*\n",
    "- *How does my time series fluctuate?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Time Series Decomposition\n",
    "\n",
    "$$ Y_t = T_t + S_t + C_t + \\varepsilon_t$$\n",
    "\n",
    "- $Y_t = $ observed value at time $t$\n",
    "- $T_t = $ trend component, *long-run behavior*\n",
    "- $S_t = $ seasonality component, *periodic fluctuations*\n",
    "- $C_t = $ cyclical component, *non-periodic fluctuations*\n",
    "- $\\varepsilon_t = $ noise component, *we would like this to be stationary*\n",
    "\n",
    "> Decomposition above is additive; can also be multiplicative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several algorithms for performing this decomposition\n",
    "\n",
    "- Classical decomp., [https://www.otexts.org/fpp/6/3](https://www.otexts.org/fpp/6/3)\n",
    "- X-12-ARIMA, [https://www.otexts.org/fpp/6/4](https://www.otexts.org/fpp/6/4)\n",
    "- STL, [https://www.otexts.org/fpp/6/5](https://www.otexts.org/fpp/6/5)\n",
    "\n",
    "See also `seasonal_decompose` in `statsmodel.tsa.seasonal`.  \n",
    "\n",
    "> Nice tutorial here: [http://machinelearningmastery.com/decompose-time-series-data-trend-seasonality/](http://machinelearningmastery.com/decompose-time-series-data-trend-seasonality/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = seasonal_decompose(df)\n",
    "result.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(C2) **Check**\n",
    "\n",
    "Which of the three trends addresses the following questions?\n",
    "1. What is the long-term behavior of my series?\n",
    "2. What is the effect of time-of-day on my series?\n",
    "3. What is the effect of larger, unseen fluctuations on my series?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Autocorrelation\n",
    "\n",
    "- In *non-time-series* analyses we often **must** assume observations $Y_i$ are **independent**\n",
    "- In *time-series* analysis we often **must** assume observations are **dependent**\n",
    "\n",
    "*Autocorrelation* allows us to check for this type of *sequential dependence*?\n",
    "\n",
    "---\n",
    "\n",
    "The *correlation* between time series is\n",
    "\n",
    "$$ Corr(Y_t, Z_t) = \\frac{Cov(Y_t,Z_t)}{\\sqrt{Var(Y_t)Var(Z_t)}} $$\n",
    "\n",
    "\n",
    "The *autocorrelation of a series* is the correlation between a time series and a **lagged version** of itself.\n",
    "\n",
    "$$ Corr(Y_t, Y_{t+k}) = \\frac{Cov(Y_t,Y_{t+k})}{\\sqrt{Var(Y_t)Var(Y_{t+k})}} $$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do this with `statsmodels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "print(acf(df.signal, nlags=50))\n",
    "\n",
    "plot_acf(df.signal, lags=50);\n",
    "plt.xlabel('Lags')\n",
    "plt.ylabel('ACF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(C3) **Check:** \n",
    "\n",
    "1. What does autocorrelation tell you?\n",
    "2. Which component of the signal is it likely to reveal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Partial* Autocorrelation\n",
    "\n",
    "*Partial autocorrelation* (PACF) is similar to autocorrelation (ACF), but instead of just the correlation at increasing lags, it is the correlation at a given lag _controlling for the effect of previous lags._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import pacf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "print(pacf(df.signal, nlags=50))\n",
    "\n",
    "plot_pacf(df.signal, lags=50);\n",
    "plt.xlabel('Lags')\n",
    "plt.ylabel('PACF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get some real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('datasets/seasonally-adjusted-quarterly-us.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.columns = ['year_quarter', 'unemployment_rate']\n",
    "data['unemployment_rate'] = data['unemployment_rate'].map(lambda x: float(str(x).replace('%','')))\n",
    "data.dropna(inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['date'] = pd.to_datetime(data.year_quarter).dt.to_period('Q')\n",
    "data.set_index('date', inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['unemployment_rate'].plot(lw=2.5, figsize=(12,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the ACF of the data for 50 lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the PACF of the data for 50 lags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "Look at our unemployment data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['unemployment_rate'].plot(lw=2.5, figsize=(12,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is this data \"stationary\"?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time series modeling typically presumes that we have stationary data.\n",
    "\n",
    "To stationarize (maybe not a word...) we could possibly take two approaches:\n",
    "\n",
    "1. Pull the \"residual\" component from the decomposition and check if it is stationary, or\n",
    "2. Take a \"diff\" one or more times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_ts = pd.DataFrame(data[['unemployment_rate']].values, columns=['rate'], index=data.index.to_timestamp())\n",
    "data_ts.head()\n",
    "result = seasonal_decompose(data_ts.rate)\n",
    "result.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Differencing\" a timeseries and stationarity\n",
    "\n",
    "\n",
    "If a time series is stationary, the mean, variance, and autocorrelation (covered in the next section) are constant over time. Forcasting methods typically assume that the timeseries you are forcasting on are stationary, or at least approximately stationary.\n",
    "\n",
    "The most common way to make a timeseries stationary is to perform \"differencing\". This procedure converts a timeseries into the difference between values:\n",
    "\n",
    "<a id=\"-delta-yt--yt---yt--\"></a>\n",
    "### $$ \\Delta y_t = y_t - y_{t-1} $$\n",
    "\n",
    "This removes trends in the timeseries and ensures that the mean across time is zero. In most cases there will only be a need for a single differencing, although sometimes a second difference (or even more) will be taken to remove trends.\n",
    "\n",
    "**Difference the unemployment rate and plot.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the ACF and PACF curves of the diff'd series**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(C4) **Check** Why diff?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning!** Don't diff blindly!  Always check to see if you series is really stationary or not.  You may need to diff more than once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How do you really know if your timeseries is stationary?  You can formulate stationarity as a hypothesis and then test the hypothesis!  An example of this approach is the [Dickey-Fuller test](https://en.wikipedia.org/wiki/Dickey%E2%80%93Fuller_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregressive (AR) models\n",
    "\n",
    "---\n",
    "\n",
    "Autoregressive (AR) models use data from previous time-points to predict the next time-point. These are essentially regression models where the predictors are previous timepoints of the outcome.\n",
    "\n",
    "Typically, AR models are denoted `AR(p)`, where _p_ indicates the number of previous time points to incorporate. `AR(1)` is the most common.\n",
    "\n",
    "In an autoregressive model we learn regression coefficients on the features that are the previous _p_ values.\n",
    "\n",
    "### $$y_i = \\beta_0 + \\beta_1  y_{i-1} + \\beta_2  y_{i-2}\\ +\\ ...\\ +\\ \\beta_p  y_{i-p}\\ +\\ \\epsilon \\\\\n",
    "y_i =\\sum_{j=1}^p \\beta_j y_{i-j} + \\epsilon$$\n",
    "\n",
    "We can build autoregressive models using the `ARMA` class from statsmodels. \n",
    "\n",
    "> Alternatively, there is a newer python package called pyflux that also looks promising for time series modeling.  We won't cover pyflux in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pro-tip**: AIC in the results above refer to the [Akaike Information Criterion](https://en.wikipedia.org/wiki/Akaike_information_criterion).  AIC measures the \"quality\" of your model: lower values of AIC suggest better models!\n",
    "\n",
    "See also: the #1 answer [here](https://stats.stackexchange.com/questions/187373/interpretation-of-aic-value):\n",
    "\n",
    "> ... Lower value of AIC suggests \"better\" model, but it is a relative measure of model fit. It is used for model selection, i.e. it lets you to compare different models estimated on the same dataset.\n",
    "\n",
    "> Recall G.E.P. Box saying that \"all models are wrong, but some are useful\", you are not interested in finding model that has a perfect fit to your data because it is impossible and such model in many cases would be a very poor, overfitted one. Instead, you are looking for the best one that you can get, the most useful one. The general idea behind AIC is that model with lower number of parameters is better, what is somehow consistent with Occam's razor argument, that we prefer simple model over a complicated one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"In-sample\" predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get predictions from the time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date_ticks = data.index.to_timestamp()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "ax.plot(date_ticks[1:], udiff, lw=2, color='grey', ls='dashed')\n",
    "ax.plot(date_ticks[1:], ar1.fittedvalues, lw=2, color='darkred')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also \"score\" the model with the coefficient of determination ($R^2$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Out-of-sample\" predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What if we want to predict more than one time step into the future?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import _arma_predict_out_of_sample\n",
    "\n",
    "# get what you need for predicting \"steps\" steps ahead\n",
    "params = ar1.params\n",
    "residuals = ar1.resid\n",
    "p = ar1.k_ar\n",
    "q = ar1.k_ma\n",
    "k_exog = ar1.k_exog\n",
    "k_trend = ar1.k_trend\n",
    "steps = 73\n",
    "\n",
    "oos_predictions = _arma_predict_out_of_sample(params, steps, residuals, \n",
    "                                p, q, k_trend, k_exog, \n",
    "                                endog=udiff.values, exog=None, start=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "ax.plot(date_ticks[1:], udiff, lw=2, color='grey', ls='dashed')\n",
    "ax.plot(date_ticks[1:], ar1.fittedvalues, lw=2, color='darkred')\n",
    "ax.plot(date_ticks[101:], oos_predictions, lw=2, color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(C5) **Check:** True or False?  Autoregressive (AR) models are just linear regression models where the previous time steps are used to predict the next time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving Average (MA) models\n",
    "---\n",
    "\n",
    "**Moving average models** take previous _error terms_ as inputs. They predict the next value based on deviations from previous predictions. This can be useful for modeling a sudden occurrence - like something going out of stock affecting sales or a sudden rise in popularity.\n",
    "\n",
    "As in autoregressive models, we have an order term, _q_, and we refer to our model as `MA(q)`.  This moving average model is dependent on the last _q_ errors. If we have a time series of sales per week, $y_i$, we can regress each $y_i$ on the last _q_ error terms.\n",
    "\n",
    "### $$y_t = \\epsilon_t + \\beta_{1} \\epsilon_{t-1} + ... \\beta_{n} \\epsilon_{t-n} \\\\\n",
    "y_t = \\sum_{i=1}^n \\beta_i \\epsilon_{t-i} + \\epsilon_t$$\n",
    "\n",
    "Sometimes the mean of the timeseries is included in the equation:\n",
    "\n",
    "### $$ y_t = \\mu + \\sum_{i=1}^n \\beta_i \\epsilon_{t-i} + \\epsilon_t $$\n",
    "\n",
    "Moving average models are not as trivial to fit as autoregressive models because the error terms are unobserved. [There are a variety of different ways you can estimate the parameters, some of which are covered in this paper.](https://www.it.uu.se/research/publications/reports/2006-022/2006-022-nc.pdf)\n",
    "\n",
    "In the simpler fitting procedures, a model is iteratively fit, errors are computed, then refit, over and over again until the parameters on the errors converge.\n",
    "\n",
    "MA includes the mean of the time series. The behavior of the model is therefore characterized by random jumps around the mean value.\n",
    "\n",
    "In an `MA(1)` model, there is one coefficient on the error of our previous prediction impacting our estimate for the next value in the timeseries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "ax.plot(date_ticks[1:], udiff, lw=2, color='grey', ls='dashed')\n",
    "ax.plot(date_ticks[1:], ma1.fittedvalues, lw=2, color='darkred')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r2_score(udiff, ma1.fittedvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARMA and ARIMA models\n",
    "---\n",
    "\n",
    "**ARMA** models combine the autoregressive models and moving average models. We combine both, parameterizing the behavior of the model with `p` and `q` terms corresponding to the `AR(p)` model and `MA(q)` model.\n",
    "\n",
    "Autoregressive models slowly incorporate changes in preferences, tastes, and patterns. Moving average models base their prediction not on the prior value but the prior error, allowing us to correct sudden changes based on random events - supply, popularity spikes, etc.\n",
    "\n",
    "**ARIMA** is just like the `ARMA(p, q)` model, but instead of predicting the value of the series it predicts the _differenced_ series or changes in the series. The order of differencing is set by an _d_ term as in `ARIMA(p, d, q)`, or alternatively you can just fit an `ARMA(p, q)` model on a differenced timeseries.\n",
    "\n",
    "Recall the pandas `diff` function. This computes the difference between two consecutive values. In an ARIMA model, we attempt to predict this difference instead of the actual values.\n",
    "\n",
    "### $$y_t - y_{(t-1)} = ARMA(p, q)$$\n",
    "\n",
    "Timeseries are assumed to be \"stationary\" when modeling. This handles the stationarity assumption: instead of detrending or differencing manually, the model does this via the differencing term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "ax.plot(date_ticks[1:], udiff, lw=2, color='grey', ls='dashed')\n",
    "ax.plot(date_ticks[1:], ar1ma1.fittedvalues, lw=2, color='darkred')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstructing the \"full\" prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to choose the right `p` and `q` parameters.\n",
    "---\n",
    "\n",
    "In general it is never a bad idea to choose your parameters based on hold-out testing. That is to say, checking the performance of your model on future timepoints based on different choices of `p` and `q` for an ARIMA model.\n",
    "\n",
    "However, you can get a sense for what parameters will work best based on the autocorrelation and partial autocorrelation charts.\n",
    "\n",
    "[This site has a very detailed overview of how to use the acf and pacf to determine your parameters.](https://people.duke.edu/~rnau/411arim3.htm)\n",
    "\n",
    "In general though, below are some basic guidelines. Remember that these rules apply to the ACF and PACF plots of differenced timeseries rather than the original timeseries (the exception being if your timeseries is stationary and does not require differencing):\n",
    "\n",
    "1. If the PACF has a sharp cutoff and the lag-1 ACF value is positive then choose an AR(x) term where x is the lag in the PACF after the cutoff.\n",
    "2. If the ACF has a sharp cutoff and the lag-1 ACF value is negative, choose an MA(x) term where x is the lag in the ACF after the cutoff.\n",
    "3. If both the ACF and PACF show a gradual decay, and ARMA model is likely appropriate as opposed to the AR or MA alone.\n",
    "\n",
    "Context 1 above corresponds to timeseries that are \"underdifferenced\" as indicated by a positive autocorrelation at lat 1. Likewise, context 2 is \"overdifferenced\" as indicated by the negative autocorrelation.\n",
    "\n",
    "In general, you should try to choose an AR or MA model alone as opposed to an ARMA model. The AR and MA terms can work against each other in the model and create an overly-complex representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-up\n",
    "\n",
    "(C6) Take a couple of minutes in groups of 2-4 and write down the workflow you would use to build a timeseries model.  Write your workflow on a piece of paper, a whiteboard or in Slack."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
