{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston, load_breast_cancer, load_iris, fetch_lfw_people\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import losses\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_boston' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c9abfd337072>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_boston\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_boston' is not defined"
     ]
    }
   ],
   "source": [
    "data=load_boston()\n",
    "\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss_fit = ss.fit(X_train)\n",
    "\n",
    "X_train_transform = ss_fit.transform(X_train)\n",
    "X_test_transform = ss_fit.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-24a4d47a09aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(13, input_dim=X_train_transform.shape[1], activation='relu'))\n",
    "model.add(Dense(6, activation='relu'))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dense(2, activation='relu'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 404 samples, validate on 102 samples\n",
      "Epoch 1/50\n",
      "404/404 [==============================] - 1s 2ms/step - loss: 565.5280 - val_loss: 633.7134\n",
      "Epoch 2/50\n",
      "404/404 [==============================] - 0s 51us/step - loss: 554.8486 - val_loss: 619.1913\n",
      "Epoch 3/50\n",
      "404/404 [==============================] - 0s 58us/step - loss: 540.7131 - val_loss: 599.1958\n",
      "Epoch 4/50\n",
      "404/404 [==============================] - 0s 66us/step - loss: 521.8912 - val_loss: 572.9293\n",
      "Epoch 5/50\n",
      "404/404 [==============================] - 0s 67us/step - loss: 499.3677 - val_loss: 539.7020\n",
      "Epoch 6/50\n",
      "404/404 [==============================] - 0s 70us/step - loss: 471.1849 - val_loss: 499.6496\n",
      "Epoch 7/50\n",
      "404/404 [==============================] - 0s 60us/step - loss: 437.2971 - val_loss: 450.2598\n",
      "Epoch 8/50\n",
      "404/404 [==============================] - 0s 63us/step - loss: 395.3422 - val_loss: 394.4480\n",
      "Epoch 9/50\n",
      "404/404 [==============================] - 0s 56us/step - loss: 349.5460 - val_loss: 329.4913\n",
      "Epoch 10/50\n",
      "404/404 [==============================] - 0s 64us/step - loss: 295.8079 - val_loss: 264.5278\n",
      "Epoch 11/50\n",
      "404/404 [==============================] - 0s 59us/step - loss: 242.7561 - val_loss: 206.5813\n",
      "Epoch 12/50\n",
      "404/404 [==============================] - 0s 58us/step - loss: 195.5068 - val_loss: 159.3966\n",
      "Epoch 13/50\n",
      "404/404 [==============================] - 0s 54us/step - loss: 157.2519 - val_loss: 127.7135\n",
      "Epoch 14/50\n",
      "404/404 [==============================] - 0s 57us/step - loss: 130.5102 - val_loss: 107.1011\n",
      "Epoch 15/50\n",
      "404/404 [==============================] - 0s 58us/step - loss: 111.6358 - val_loss: 91.8070\n",
      "Epoch 16/50\n",
      "404/404 [==============================] - 0s 54us/step - loss: 96.8212 - val_loss: 79.0696\n",
      "Epoch 17/50\n",
      "404/404 [==============================] - 0s 59us/step - loss: 84.4204 - val_loss: 67.9829\n",
      "Epoch 18/50\n",
      "404/404 [==============================] - 0s 60us/step - loss: 73.3609 - val_loss: 58.6751\n",
      "Epoch 19/50\n",
      "404/404 [==============================] - 0s 47us/step - loss: 64.0043 - val_loss: 51.3300\n",
      "Epoch 20/50\n",
      "404/404 [==============================] - 0s 58us/step - loss: 56.3919 - val_loss: 44.8429\n",
      "Epoch 21/50\n",
      "404/404 [==============================] - 0s 61us/step - loss: 49.7982 - val_loss: 39.8514\n",
      "Epoch 22/50\n",
      "404/404 [==============================] - 0s 61us/step - loss: 44.3565 - val_loss: 36.0575\n",
      "Epoch 23/50\n",
      "404/404 [==============================] - 0s 59us/step - loss: 40.2730 - val_loss: 32.7195\n",
      "Epoch 24/50\n",
      "404/404 [==============================] - 0s 65us/step - loss: 36.9038 - val_loss: 30.6334\n",
      "Epoch 25/50\n",
      "404/404 [==============================] - 0s 96us/step - loss: 34.2593 - val_loss: 28.8925\n",
      "Epoch 26/50\n",
      "404/404 [==============================] - 0s 91us/step - loss: 32.3273 - val_loss: 27.5543\n",
      "Epoch 27/50\n",
      "404/404 [==============================] - 0s 120us/step - loss: 30.7117 - val_loss: 26.8151\n",
      "Epoch 28/50\n",
      "404/404 [==============================] - 0s 66us/step - loss: 29.5432 - val_loss: 26.0557\n",
      "Epoch 29/50\n",
      "404/404 [==============================] - 0s 60us/step - loss: 28.5782 - val_loss: 25.6747\n",
      "Epoch 30/50\n",
      "404/404 [==============================] - 0s 52us/step - loss: 27.7738 - val_loss: 25.1340\n",
      "Epoch 31/50\n",
      "404/404 [==============================] - 0s 61us/step - loss: 27.0455 - val_loss: 24.9134\n",
      "Epoch 32/50\n",
      "404/404 [==============================] - 0s 60us/step - loss: 26.4571 - val_loss: 24.6429\n",
      "Epoch 33/50\n",
      "404/404 [==============================] - 0s 56us/step - loss: 25.9172 - val_loss: 24.1417\n",
      "Epoch 34/50\n",
      "404/404 [==============================] - 0s 53us/step - loss: 25.4217 - val_loss: 23.7613\n",
      "Epoch 35/50\n",
      "404/404 [==============================] - 0s 49us/step - loss: 25.0073 - val_loss: 23.3454\n",
      "Epoch 36/50\n",
      "404/404 [==============================] - 0s 61us/step - loss: 24.5982 - val_loss: 23.2585\n",
      "Epoch 37/50\n",
      "404/404 [==============================] - 0s 59us/step - loss: 24.1496 - val_loss: 22.9090\n",
      "Epoch 38/50\n",
      "404/404 [==============================] - 0s 44us/step - loss: 23.7860 - val_loss: 22.5576\n",
      "Epoch 39/50\n",
      "404/404 [==============================] - 0s 55us/step - loss: 23.4434 - val_loss: 22.2387\n",
      "Epoch 40/50\n",
      "404/404 [==============================] - 0s 60us/step - loss: 23.0508 - val_loss: 22.1146\n",
      "Epoch 41/50\n",
      "404/404 [==============================] - 0s 49us/step - loss: 22.7819 - val_loss: 21.8498\n",
      "Epoch 42/50\n",
      "404/404 [==============================] - 0s 51us/step - loss: 22.4224 - val_loss: 21.5385\n",
      "Epoch 43/50\n",
      "404/404 [==============================] - 0s 56us/step - loss: 22.1407 - val_loss: 21.5276\n",
      "Epoch 44/50\n",
      "404/404 [==============================] - 0s 45us/step - loss: 21.6872 - val_loss: 20.8991\n",
      "Epoch 45/50\n",
      "404/404 [==============================] - 0s 49us/step - loss: 21.4311 - val_loss: 20.6226\n",
      "Epoch 46/50\n",
      "404/404 [==============================] - 0s 54us/step - loss: 21.0469 - val_loss: 20.2960\n",
      "Epoch 47/50\n",
      "404/404 [==============================] - 0s 47us/step - loss: 20.7096 - val_loss: 20.2533\n",
      "Epoch 48/50\n",
      "404/404 [==============================] - 0s 66us/step - loss: 20.3740 - val_loss: 19.8058\n",
      "Epoch 49/50\n",
      "404/404 [==============================] - 0s 57us/step - loss: 20.0407 - val_loss: 19.5172\n",
      "Epoch 50/50\n",
      "404/404 [==============================] - 0s 58us/step - loss: 19.7972 - val_loss: 19.2829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11fe8b1d0>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_transform, y_train, validation_data=(X_test_transform, y_test), epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "\n",
    "target = data.target* -1 + 1  \n",
    "\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss_fit = ss.fit(X_train)\n",
    "\n",
    "X_train_transform = ss_fit.transform(X_train)\n",
    "X_test_transform = ss_fit.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 455 samples, validate on 114 samples\n",
      "Epoch 1/50\n",
      "455/455 [==============================] - 1s 1ms/step - loss: 0.5823 - acc: 0.8308 - val_loss: 0.5242 - val_acc: 0.8947\n",
      "Epoch 2/50\n",
      "455/455 [==============================] - 0s 60us/step - loss: 0.4599 - acc: 0.9297 - val_loss: 0.3997 - val_acc: 0.9298\n",
      "Epoch 3/50\n",
      "455/455 [==============================] - 0s 63us/step - loss: 0.3391 - acc: 0.9495 - val_loss: 0.2861 - val_acc: 0.9386\n",
      "Epoch 4/50\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.2415 - acc: 0.9538 - val_loss: 0.2069 - val_acc: 0.9474\n",
      "Epoch 5/50\n",
      "455/455 [==============================] - 0s 74us/step - loss: 0.1789 - acc: 0.9560 - val_loss: 0.1579 - val_acc: 0.9386\n",
      "Epoch 6/50\n",
      "455/455 [==============================] - 0s 69us/step - loss: 0.1410 - acc: 0.9604 - val_loss: 0.1272 - val_acc: 0.9474\n",
      "Epoch 7/50\n",
      "455/455 [==============================] - 0s 62us/step - loss: 0.1190 - acc: 0.9714 - val_loss: 0.1075 - val_acc: 0.9474\n",
      "Epoch 8/50\n",
      "455/455 [==============================] - 0s 65us/step - loss: 0.1040 - acc: 0.9714 - val_loss: 0.0950 - val_acc: 0.9561\n",
      "Epoch 9/50\n",
      "455/455 [==============================] - 0s 67us/step - loss: 0.0935 - acc: 0.9802 - val_loss: 0.0841 - val_acc: 0.9649\n",
      "Epoch 10/50\n",
      "455/455 [==============================] - 0s 86us/step - loss: 0.0847 - acc: 0.9780 - val_loss: 0.0776 - val_acc: 0.9649\n",
      "Epoch 11/50\n",
      "455/455 [==============================] - 0s 76us/step - loss: 0.0786 - acc: 0.9780 - val_loss: 0.0716 - val_acc: 0.9737\n",
      "Epoch 12/50\n",
      "455/455 [==============================] - 0s 71us/step - loss: 0.0737 - acc: 0.9802 - val_loss: 0.0670 - val_acc: 0.9737\n",
      "Epoch 13/50\n",
      "455/455 [==============================] - 0s 67us/step - loss: 0.0696 - acc: 0.9802 - val_loss: 0.0633 - val_acc: 0.9737\n",
      "Epoch 14/50\n",
      "455/455 [==============================] - 0s 79us/step - loss: 0.0662 - acc: 0.9824 - val_loss: 0.0596 - val_acc: 0.9737\n",
      "Epoch 15/50\n",
      "455/455 [==============================] - 0s 64us/step - loss: 0.0631 - acc: 0.9846 - val_loss: 0.0561 - val_acc: 0.9825\n",
      "Epoch 16/50\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0601 - acc: 0.9846 - val_loss: 0.0559 - val_acc: 0.9825\n",
      "Epoch 17/50\n",
      "455/455 [==============================] - 0s 76us/step - loss: 0.0574 - acc: 0.9868 - val_loss: 0.0541 - val_acc: 0.9825\n",
      "Epoch 18/50\n",
      "455/455 [==============================] - 0s 63us/step - loss: 0.0549 - acc: 0.9868 - val_loss: 0.0530 - val_acc: 0.9825\n",
      "Epoch 19/50\n",
      "455/455 [==============================] - 0s 71us/step - loss: 0.0524 - acc: 0.9890 - val_loss: 0.0501 - val_acc: 0.9825\n",
      "Epoch 20/50\n",
      "455/455 [==============================] - 0s 70us/step - loss: 0.0503 - acc: 0.9890 - val_loss: 0.0495 - val_acc: 0.9825\n",
      "Epoch 21/50\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0484 - acc: 0.9868 - val_loss: 0.0527 - val_acc: 0.9825\n",
      "Epoch 22/50\n",
      "455/455 [==============================] - 0s 63us/step - loss: 0.0462 - acc: 0.9890 - val_loss: 0.0503 - val_acc: 0.9825\n",
      "Epoch 23/50\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0436 - acc: 0.9890 - val_loss: 0.0459 - val_acc: 0.9825\n",
      "Epoch 24/50\n",
      "455/455 [==============================] - 0s 68us/step - loss: 0.0418 - acc: 0.9890 - val_loss: 0.0438 - val_acc: 0.9825\n",
      "Epoch 25/50\n",
      "455/455 [==============================] - 0s 69us/step - loss: 0.0403 - acc: 0.9890 - val_loss: 0.0448 - val_acc: 0.9825\n",
      "Epoch 26/50\n",
      "455/455 [==============================] - 0s 69us/step - loss: 0.0383 - acc: 0.9912 - val_loss: 0.0447 - val_acc: 0.9737\n",
      "Epoch 27/50\n",
      "455/455 [==============================] - 0s 72us/step - loss: 0.0370 - acc: 0.9912 - val_loss: 0.0447 - val_acc: 0.9825\n",
      "Epoch 28/50\n",
      "455/455 [==============================] - 0s 74us/step - loss: 0.0357 - acc: 0.9912 - val_loss: 0.0431 - val_acc: 0.9825\n",
      "Epoch 29/50\n",
      "455/455 [==============================] - 0s 63us/step - loss: 0.0341 - acc: 0.9912 - val_loss: 0.0428 - val_acc: 0.9825\n",
      "Epoch 30/50\n",
      "455/455 [==============================] - 0s 70us/step - loss: 0.0329 - acc: 0.9912 - val_loss: 0.0426 - val_acc: 0.9825\n",
      "Epoch 31/50\n",
      "455/455 [==============================] - 0s 63us/step - loss: 0.0315 - acc: 0.9912 - val_loss: 0.0412 - val_acc: 0.9825\n",
      "Epoch 32/50\n",
      "455/455 [==============================] - 0s 69us/step - loss: 0.0303 - acc: 0.9912 - val_loss: 0.0416 - val_acc: 0.9825\n",
      "Epoch 33/50\n",
      "455/455 [==============================] - 0s 66us/step - loss: 0.0291 - acc: 0.9912 - val_loss: 0.0427 - val_acc: 0.9825\n",
      "Epoch 34/50\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0274 - acc: 0.9912 - val_loss: 0.0393 - val_acc: 0.9825\n",
      "Epoch 35/50\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0265 - acc: 0.9912 - val_loss: 0.0410 - val_acc: 0.9825\n",
      "Epoch 36/50\n",
      "455/455 [==============================] - 0s 58us/step - loss: 0.0252 - acc: 0.9912 - val_loss: 0.0417 - val_acc: 0.9825\n",
      "Epoch 37/50\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0239 - acc: 0.9912 - val_loss: 0.0422 - val_acc: 0.9825\n",
      "Epoch 38/50\n",
      "455/455 [==============================] - 0s 60us/step - loss: 0.0234 - acc: 0.9912 - val_loss: 0.0413 - val_acc: 0.9825\n",
      "Epoch 39/50\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0223 - acc: 0.9912 - val_loss: 0.0356 - val_acc: 0.9825\n",
      "Epoch 40/50\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0213 - acc: 0.9912 - val_loss: 0.0373 - val_acc: 0.9825\n",
      "Epoch 41/50\n",
      "455/455 [==============================] - 0s 53us/step - loss: 0.0202 - acc: 0.9934 - val_loss: 0.0394 - val_acc: 0.9825\n",
      "Epoch 42/50\n",
      "455/455 [==============================] - 0s 59us/step - loss: 0.0194 - acc: 0.9934 - val_loss: 0.0386 - val_acc: 0.9825\n",
      "Epoch 43/50\n",
      "455/455 [==============================] - 0s 49us/step - loss: 0.0183 - acc: 0.9912 - val_loss: 0.0393 - val_acc: 0.9825\n",
      "Epoch 44/50\n",
      "455/455 [==============================] - 0s 53us/step - loss: 0.0175 - acc: 0.9934 - val_loss: 0.0408 - val_acc: 0.9737\n",
      "Epoch 45/50\n",
      "455/455 [==============================] - 0s 52us/step - loss: 0.0170 - acc: 0.9934 - val_loss: 0.0389 - val_acc: 0.9825\n",
      "Epoch 46/50\n",
      "455/455 [==============================] - 0s 52us/step - loss: 0.0156 - acc: 0.9934 - val_loss: 0.0371 - val_acc: 0.9825\n",
      "Epoch 47/50\n",
      "455/455 [==============================] - 0s 52us/step - loss: 0.0149 - acc: 0.9934 - val_loss: 0.0382 - val_acc: 0.9825\n",
      "Epoch 48/50\n",
      "455/455 [==============================] - 0s 54us/step - loss: 0.0145 - acc: 0.9934 - val_loss: 0.0394 - val_acc: 0.9825\n",
      "Epoch 49/50\n",
      "455/455 [==============================] - 0s 55us/step - loss: 0.0136 - acc: 0.9956 - val_loss: 0.0398 - val_acc: 0.9825\n",
      "Epoch 50/50\n",
      "455/455 [==============================] - 0s 61us/step - loss: 0.0126 - acc: 0.9978 - val_loss: 0.0364 - val_acc: 0.9912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1217b4b38>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(X_train_transform.shape[1], input_dim=X_train_transform.shape[1], activation='relu'))\n",
    "model.add(Dense(15, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_transform, y_train, validation_data=(X_test_transform, y_test), epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# plt.plot(history.history['val_acc'], label='val acc')\n",
    "# plt.plot(history.history['acc'], label='train acc')\n",
    "# plt.xlabel('epochs')\n",
    "# plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(455, 30)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transform.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = model.predict_classes(X_test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred [ :, 0 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = load_iris()\n",
    "\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(X_train)\n",
    "\n",
    "X_train = ss.transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120 samples, validate on 30 samples\n",
      "Epoch 1/50\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 1.8146 - acc: 0.1083 - val_loss: 1.1535 - val_acc: 0.3000\n",
      "Epoch 2/50\n",
      "120/120 [==============================] - 0s 55us/step - loss: 1.7864 - acc: 0.1083 - val_loss: 1.1481 - val_acc: 0.3667\n",
      "Epoch 3/50\n",
      "120/120 [==============================] - 0s 60us/step - loss: 1.7575 - acc: 0.3583 - val_loss: 1.1430 - val_acc: 0.3667\n",
      "Epoch 4/50\n",
      "120/120 [==============================] - 0s 76us/step - loss: 1.7295 - acc: 0.3583 - val_loss: 1.1378 - val_acc: 0.3667\n",
      "Epoch 5/50\n",
      "120/120 [==============================] - 0s 66us/step - loss: 1.7037 - acc: 0.1917 - val_loss: 1.1329 - val_acc: 0.3000\n",
      "Epoch 6/50\n",
      "120/120 [==============================] - 0s 54us/step - loss: 1.6750 - acc: 0.1000 - val_loss: 1.1282 - val_acc: 0.3000\n",
      "Epoch 7/50\n",
      "120/120 [==============================] - 0s 92us/step - loss: 1.6474 - acc: 0.1000 - val_loss: 1.1237 - val_acc: 0.3000\n",
      "Epoch 8/50\n",
      "120/120 [==============================] - 0s 77us/step - loss: 1.6237 - acc: 0.1000 - val_loss: 1.1194 - val_acc: 0.3000\n",
      "Epoch 9/50\n",
      "120/120 [==============================] - 0s 70us/step - loss: 1.5971 - acc: 0.1000 - val_loss: 1.1152 - val_acc: 0.3000\n",
      "Epoch 10/50\n",
      "120/120 [==============================] - 0s 82us/step - loss: 1.5717 - acc: 0.1000 - val_loss: 1.1114 - val_acc: 0.3000\n",
      "Epoch 11/50\n",
      "120/120 [==============================] - 0s 112us/step - loss: 1.5487 - acc: 0.1000 - val_loss: 1.1078 - val_acc: 0.3000\n",
      "Epoch 12/50\n",
      "120/120 [==============================] - 0s 80us/step - loss: 1.5232 - acc: 0.1000 - val_loss: 1.1044 - val_acc: 0.3333\n",
      "Epoch 13/50\n",
      "120/120 [==============================] - 0s 117us/step - loss: 1.5015 - acc: 0.0750 - val_loss: 1.1011 - val_acc: 0.3333\n",
      "Epoch 14/50\n",
      "120/120 [==============================] - 0s 128us/step - loss: 1.4796 - acc: 0.0750 - val_loss: 1.0981 - val_acc: 0.3333\n",
      "Epoch 15/50\n",
      "120/120 [==============================] - 0s 116us/step - loss: 1.4566 - acc: 0.0750 - val_loss: 1.0952 - val_acc: 0.3333\n",
      "Epoch 16/50\n",
      "120/120 [==============================] - 0s 71us/step - loss: 1.4345 - acc: 0.0750 - val_loss: 1.0927 - val_acc: 0.3333\n",
      "Epoch 17/50\n",
      "120/120 [==============================] - 0s 76us/step - loss: 1.4146 - acc: 0.0667 - val_loss: 1.0902 - val_acc: 0.3333\n",
      "Epoch 18/50\n",
      "120/120 [==============================] - 0s 90us/step - loss: 1.3937 - acc: 0.0667 - val_loss: 1.0880 - val_acc: 0.3333\n",
      "Epoch 19/50\n",
      "120/120 [==============================] - 0s 58us/step - loss: 1.3743 - acc: 0.0667 - val_loss: 1.0859 - val_acc: 0.3333\n",
      "Epoch 20/50\n",
      "120/120 [==============================] - 0s 73us/step - loss: 1.3556 - acc: 0.0583 - val_loss: 1.0840 - val_acc: 0.3333\n",
      "Epoch 21/50\n",
      "120/120 [==============================] - 0s 102us/step - loss: 1.3362 - acc: 0.0583 - val_loss: 1.0824 - val_acc: 0.3333\n",
      "Epoch 22/50\n",
      "120/120 [==============================] - 0s 72us/step - loss: 1.3180 - acc: 0.0583 - val_loss: 1.0810 - val_acc: 0.3333\n",
      "Epoch 23/50\n",
      "120/120 [==============================] - 0s 56us/step - loss: 1.2994 - acc: 0.0583 - val_loss: 1.0797 - val_acc: 0.3333\n",
      "Epoch 24/50\n",
      "120/120 [==============================] - 0s 87us/step - loss: 1.2830 - acc: 0.0500 - val_loss: 1.0786 - val_acc: 0.3333\n",
      "Epoch 25/50\n",
      "120/120 [==============================] - 0s 55us/step - loss: 1.2660 - acc: 0.0500 - val_loss: 1.0776 - val_acc: 0.3333\n",
      "Epoch 26/50\n",
      "120/120 [==============================] - 0s 69us/step - loss: 1.2497 - acc: 0.0500 - val_loss: 1.0767 - val_acc: 0.3333\n",
      "Epoch 27/50\n",
      "120/120 [==============================] - 0s 69us/step - loss: 1.2344 - acc: 0.0500 - val_loss: 1.0761 - val_acc: 0.3333\n",
      "Epoch 28/50\n",
      "120/120 [==============================] - 0s 68us/step - loss: 1.2185 - acc: 0.0500 - val_loss: 1.0755 - val_acc: 0.3333\n",
      "Epoch 29/50\n",
      "120/120 [==============================] - 0s 59us/step - loss: 1.2040 - acc: 0.0500 - val_loss: 1.0751 - val_acc: 0.3667\n",
      "Epoch 30/50\n",
      "120/120 [==============================] - 0s 57us/step - loss: 1.1898 - acc: 0.0417 - val_loss: 1.0747 - val_acc: 0.3667\n",
      "Epoch 31/50\n",
      "120/120 [==============================] - 0s 96us/step - loss: 1.1751 - acc: 0.0417 - val_loss: 1.0746 - val_acc: 0.3667\n",
      "Epoch 32/50\n",
      "120/120 [==============================] - 0s 61us/step - loss: 1.1621 - acc: 0.0500 - val_loss: 1.0746 - val_acc: 0.3667\n",
      "Epoch 33/50\n",
      "120/120 [==============================] - 0s 59us/step - loss: 1.1482 - acc: 0.0417 - val_loss: 1.0746 - val_acc: 0.3667\n",
      "Epoch 34/50\n",
      "120/120 [==============================] - 0s 64us/step - loss: 1.1356 - acc: 0.0500 - val_loss: 1.0748 - val_acc: 0.3667\n",
      "Epoch 35/50\n",
      "120/120 [==============================] - 0s 87us/step - loss: 1.1237 - acc: 0.0833 - val_loss: 1.0750 - val_acc: 0.3333\n",
      "Epoch 36/50\n",
      "120/120 [==============================] - 0s 76us/step - loss: 1.1120 - acc: 0.0833 - val_loss: 1.0754 - val_acc: 0.3333\n",
      "Epoch 37/50\n",
      "120/120 [==============================] - 0s 51us/step - loss: 1.1004 - acc: 0.1000 - val_loss: 1.0759 - val_acc: 0.3333\n",
      "Epoch 38/50\n",
      "120/120 [==============================] - 0s 87us/step - loss: 1.0888 - acc: 0.1500 - val_loss: 1.0764 - val_acc: 0.3333\n",
      "Epoch 39/50\n",
      "120/120 [==============================] - 0s 71us/step - loss: 1.0782 - acc: 0.1667 - val_loss: 1.0769 - val_acc: 0.3333\n",
      "Epoch 40/50\n",
      "120/120 [==============================] - 0s 55us/step - loss: 1.0677 - acc: 0.2167 - val_loss: 1.0776 - val_acc: 0.3000\n",
      "Epoch 41/50\n",
      "120/120 [==============================] - 0s 65us/step - loss: 1.0573 - acc: 0.2500 - val_loss: 1.0785 - val_acc: 0.3000\n",
      "Epoch 42/50\n",
      "120/120 [==============================] - 0s 118us/step - loss: 1.0472 - acc: 0.2667 - val_loss: 1.0793 - val_acc: 0.3000\n",
      "Epoch 43/50\n",
      "120/120 [==============================] - 0s 59us/step - loss: 1.0380 - acc: 0.2667 - val_loss: 1.0802 - val_acc: 0.3000\n",
      "Epoch 44/50\n",
      "120/120 [==============================] - 0s 67us/step - loss: 1.0287 - acc: 0.2667 - val_loss: 1.0812 - val_acc: 0.3000\n",
      "Epoch 45/50\n",
      "120/120 [==============================] - 0s 64us/step - loss: 1.0195 - acc: 0.2833 - val_loss: 1.0823 - val_acc: 0.3000\n",
      "Epoch 46/50\n",
      "120/120 [==============================] - 0s 134us/step - loss: 1.0107 - acc: 0.5167 - val_loss: 1.0833 - val_acc: 0.3333\n",
      "Epoch 47/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 1.0185 - acc: 0.625 - 0s 66us/step - loss: 1.0019 - acc: 0.5583 - val_loss: 1.0844 - val_acc: 0.3667\n",
      "Epoch 48/50\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.9877 - acc: 0.593 - 0s 98us/step - loss: 0.9936 - acc: 0.5750 - val_loss: 1.0854 - val_acc: 0.3667\n",
      "Epoch 49/50\n",
      "120/120 [==============================] - 0s 65us/step - loss: 0.9861 - acc: 0.5750 - val_loss: 1.0865 - val_acc: 0.3667\n",
      "Epoch 50/50\n",
      "120/120 [==============================] - 0s 71us/step - loss: 0.9783 - acc: 0.5750 - val_loss: 1.0876 - val_acc: 0.4000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1232e6b38>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(4, input_dim=4, activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test_transform, y_test), epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120 samples, validate on 30 samples\n",
      "Epoch 1/20\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 1.4757 - acc: 0.4167 - val_loss: 1.3816 - val_acc: 0.3000\n",
      "Epoch 2/20\n",
      "120/120 [==============================] - 0s 38us/step - loss: 1.4724 - acc: 0.4000 - val_loss: 1.3651 - val_acc: 0.3000\n",
      "Epoch 3/20\n",
      "120/120 [==============================] - 0s 41us/step - loss: 1.6895 - acc: 0.4083 - val_loss: 1.3488 - val_acc: 0.3333\n",
      "Epoch 4/20\n",
      "120/120 [==============================] - 0s 48us/step - loss: 1.4333 - acc: 0.4000 - val_loss: 1.3333 - val_acc: 0.3333\n",
      "Epoch 5/20\n",
      "120/120 [==============================] - 0s 55us/step - loss: 1.3214 - acc: 0.4583 - val_loss: 1.3188 - val_acc: 0.3333\n",
      "Epoch 6/20\n",
      "120/120 [==============================] - 0s 56us/step - loss: 1.3128 - acc: 0.4667 - val_loss: 1.3054 - val_acc: 0.3667\n",
      "Epoch 7/20\n",
      "120/120 [==============================] - 0s 43us/step - loss: 1.4933 - acc: 0.3833 - val_loss: 1.2929 - val_acc: 0.3667\n",
      "Epoch 8/20\n",
      "120/120 [==============================] - 0s 37us/step - loss: 1.4522 - acc: 0.4083 - val_loss: 1.2802 - val_acc: 0.3667\n",
      "Epoch 9/20\n",
      "120/120 [==============================] - 0s 49us/step - loss: 1.3504 - acc: 0.3833 - val_loss: 1.2679 - val_acc: 0.3667\n",
      "Epoch 10/20\n",
      "120/120 [==============================] - 0s 44us/step - loss: 1.4353 - acc: 0.3667 - val_loss: 1.2558 - val_acc: 0.3667\n",
      "Epoch 11/20\n",
      "120/120 [==============================] - 0s 46us/step - loss: 1.3284 - acc: 0.3917 - val_loss: 1.2438 - val_acc: 0.4000\n",
      "Epoch 12/20\n",
      "120/120 [==============================] - 0s 56us/step - loss: 1.3358 - acc: 0.3500 - val_loss: 1.2325 - val_acc: 0.4667\n",
      "Epoch 13/20\n",
      "120/120 [==============================] - 0s 52us/step - loss: 1.3571 - acc: 0.4000 - val_loss: 1.2220 - val_acc: 0.4667\n",
      "Epoch 14/20\n",
      "120/120 [==============================] - 0s 48us/step - loss: 1.4359 - acc: 0.3333 - val_loss: 1.2114 - val_acc: 0.4667\n",
      "Epoch 15/20\n",
      "120/120 [==============================] - 0s 44us/step - loss: 1.4654 - acc: 0.3500 - val_loss: 1.2011 - val_acc: 0.4667\n",
      "Epoch 16/20\n",
      "120/120 [==============================] - 0s 38us/step - loss: 1.4157 - acc: 0.3500 - val_loss: 1.1914 - val_acc: 0.5000\n",
      "Epoch 17/20\n",
      "120/120 [==============================] - 0s 60us/step - loss: 1.4005 - acc: 0.3917 - val_loss: 1.1813 - val_acc: 0.5333\n",
      "Epoch 18/20\n",
      "120/120 [==============================] - 0s 46us/step - loss: 1.3498 - acc: 0.4417 - val_loss: 1.1715 - val_acc: 0.5333\n",
      "Epoch 19/20\n",
      "120/120 [==============================] - 0s 59us/step - loss: 1.3938 - acc: 0.4250 - val_loss: 1.1619 - val_acc: 0.5333\n",
      "Epoch 20/20\n",
      "120/120 [==============================] - 0s 41us/step - loss: 1.2446 - acc: 0.3583 - val_loss: 1.1529 - val_acc: 0.5667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1249b66a0>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Code with Dropouts'''\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dropout(.5, input_shape=(X_train.shape[1],)))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(5, activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Recognition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = fetch_lfw_people(min_faces_per_person=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x129897080>"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMoAAAD8CAYAAAA2RjsYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztfWuMZVl13lr3nnPf9eiu7p7pzIOekAmE4PAIAUZYkTP2\nOIRYJkoiBJEdHCH5T2Jh2ZIDiRTJiiLxy3KkRJFQTJgExzayQUaEYI1tbAuJYJ6GgWGG8ZhhpumZ\n6uqud93nOTs/qqb2t77qc3p3zfStaXp9UqvPqX3uOfucW7vOt9b+9rc0hCAOh6MejZPugMNxK8AH\nisORAB8oDkcCfKA4HAnwgeJwJMAHisORAB8oDkcCXtRAUdW3q+rjqvqkqn7gpeqUw/Fygx53wlFV\nmyLyhIg8JCLPisiXROQ9IYRvv3TdczheHshexGffLCJPhhCeEhFR1d8WkXeKSOVAyVv90OmeOtzX\nEgZpSQNW9XCzaNsXX9GO24HvQOE8Kumo+3tRd55G/KDycdCXG+nKcRGCwvZLf/5Gw540a5axTW3b\ntIjfWTFtmjadxn42x/YazWEJB9o2/D0IxIVwP+TUhtcubdv40rNrIYSzch28mIFyl4g8A/vPishb\n6j7Q6Z6Sv/vAL8SL784OtxujqTm2bMeubd3XNW2br4xPZXy2MG0hh8fS4MGHB1LnyppfZTxP036w\n0YrXb+a2L1lWwjb1M1Rfj3/pTDfhcyX1eTaLv5DFzP5ymmuXlU1HoHC/3e7EtJ0Z7B5u93Pb9oOt\nxcPt9ecXTVvr+fjdLj9hr3fqsZ3YT3pEW3+jf7g97dnG6SDuD++0zw/Pk2/bzz3+H3/paUnATQ/m\nVfXnVfXLqvrl6WT3+h9wOF6GeDED5aKI3AP7dx/8zCCE8OEQwptCCG/KW31udjhuCbwY6vUlEblf\nVe+T/QHybhH5FzdygpABp84tVZgutg63x6fseJ4N4qs1tIhHwP6RmAGvXdQ1VjcdoXMAvh5y+rxp\nqVferOY/WaOa+xdIvYibjKfZNbdFRMoS+H11OChK18N97Bfvn+3smLZWYyZV2GjHP5gb0jZtWsS2\n/iVLx7Nh7Mu0X/39ZUSvyvirJFrIsXDsgRJCmKnqvxGRPxCRpoh8JITwreOez+F4OePFvFEkhPAZ\nEfnMS9QXh+Nlixc1UG4YKlLm8bWomELMLb0arkQqNl62pyl6QAFyykJl1ZQGM00Npj51GTE8jKI6\npCZMW5pATZhqtYCKMb1CSlWXAWPy0c4j3eG+zIr4PPmMdRk4PE+DqFfeiPdwpmWp173dq4fbg9zm\ngC/1lw63L/aWTNtaZ3C4XeYt09bZiNebdW2fRyvwPVA2sOYRJsMlLA5HAnygOBwJ8IHicCRgrjFK\naKhMBpErN0GSUNJE8mgltk1OWW4cesDvWzbfhylZ5t4K7LxJM+WtVnU6E8/D3N/EPcThMUbh1CrG\nHpzmxf3iyD1c+xxH9umbbcAHZwWl2+tilIpz8PVyyruey7cOt3sDO2t/R3v7cLuf2bbHoS8b04Fp\nW3oi/pI0pnTvkO4vOvQdwXazbs6gBv5GcTgS4APF4UjAfKmXipSo7ITX4JTSfRPIGhYD+1pvdiJN\nYrEhUq+iqP47kJOAsQOp1WajOsXMbQXMeDOFQmrCGcpZmfY3iokCnoevhzStoPOzOBuBqoGM0tjY\n1qxJY++VNpU7DZEmDZqjymuvtS29GnRjKnntnJ21H1+O12htkYIAJvFLUkqhUDbkx8sV+xvF4UiA\nDxSHIwE+UByOBMxfwtIEHg3xymSJFuIsgCShTWnXJi6Ism11kpI6NW+/FdOU/DkEp3nrYo0JyEY4\n1ujmkVSjFOR6mEIena/dgH1OHbeq13GZY9uZTZNj3/ici3mMPXoNm+ZFjEu75LCA5YgsfTnbj/vb\nCzZGmS7EGKW7Zq/RihlnKTq2LcCKytngBlatAfyN4nAkwAeKw5GAOc/M29diCQu3JnZZtcwgJZzR\n7DtSL54NNzPJtL49w1QnLcCaAm3hNChSjia5E+SQnuZ0bdU5RETazUhxeJETHlunLD46ow+p6ho7\nC6SEIpYKNaT6enUYl/ZXaX0G69tJdlF3f2dgAdjesk05f/9ML/b5ov0b396AaYGW7TNkqiXfruGg\nNfA3isORAB8oDkcCfKA4HAk40RWOmDUsepSShZQwK32bNcYMRtHKn4N4hiUek1n1o8DYhtPKDVDN\ncuo4BFiRF1hSksb9j6SAa8wesC/8uQnEEBwzjODeuVeZWaVp7x3vaacguQmlhBHdZkwlc4zSbca0\n+aBF7nhLsW18yuaAu5ch7qFs+wy60rniEhaH46bBB4rDkYC5q4fxDV1A9q/MSA2aVftzoZVoIEqj\nWfVMcl7jl2XOX0OLhlNLKTDFnVHqGFO0PPvOtAkxw9n3UE29bgSY9uW+FA2wqKXUcQAKx4oFfE7D\nwj6XXeg3X2+KbfzMoI3T5t1+pGKTJUv1cpjgZ++uGdD6yTSN8jL8jeJwJMAHisORAB8oDkcC5hqj\naBBpwEq0KaxEK8kQoJlDWjKvNn6oW8XIQE5dt3KQJSy4cpDTrrNJDLR49SPGRELKieEMz1OdSmVJ\niVmNqJweTkubNyg9XNasjKwzntiZxjiB47q6GAzT03WrQjke67TiL89Gnwz+uiBBIiEzSlgmy54e\ndjhuGq47UFT1I6q6qqqPws9Oq+ojqvrdg/9P1Z3D4bjVkUK9Pioi/0VE/if87AMi8kchhA8dFDn9\ngIj82+udSEuRfAdSdYvw2qU3dQk+TSXRgbqFVXXmC1XHXffYGmpSQKpayZi4AAXBiGb+8TxM2epm\nwzHlPKW7wOfCnmb4uTH1BVPeBfn2osqan5FZRBYsnesBx+5ndoZ9expn1UeF7QvSrQkpkqfon9wi\n9Teozwua0Ddl647Joa77sRDCn4nIVfrxO0Xk4YPth0Xknxzv8g7HrYHjxih3hBAuHWw/JyJ3VB2I\npelmQy9N57g18aKD+bBff7uSC2FpuqzrpekctyaOmx5+XlXPhxAuqep5EVlN+lQpko1gteAYyiiP\nOO0K/JfKZ6N6mLl4Xa0PBB+F3PhI6rjGsCIDal4nmZlSbGPKS1MFXzTg4HOGmhWOtbVUcJUmx0Rm\n5ac9ZwfMJo6U1wNpSqdpy8i1a0rTYbzEsQ3+yeXUOKbtA93rDKYXMvZrhtu9aTFKBT4lIu892H6v\niPz+Mc/jcNwSSEkP/5aIfEFEXqWqz6rq+0TkQyLykKp+V0R+4mDf4fihxXWpVwjhPRVNP36jF9uf\nmYeZV3g7H2FMiSJPplrTKZQGuAGlbaOGXuE1eMYZfbDqUswtoi14JM/24/W57APSn7zm/pi2aM2M\ndzebVrb1oCwDKwHqDDIwtYvp4Ov1E9XSGamO++3Ylw161Njtok33DqepES/UwmfmHY4E+EBxOBLg\nA8XhSMDcDfBQ5YmrHcsOKWHzaj9eIykpLFktIEbh+AVLsrHqGI/l2iktiEOyVnWJuboVjhgHiBzl\n5oghuCHwCkc8T931BiQbqTpORGQBju2S9BaN7XZnZCAB8pP1cc+0bYy7h9t7tCq0Lh7ElHovt31Z\nbEev44tkYIiPs+hSCn8P0u2+wtHhuHnwgeJwJGD+vl5wRVw/xCXD2DcYMQN6VczsWC9hRn9K9GoC\nC42am+SHC6/kET2VPfAYW6cSFPlipAeDvi3B1oWFRuxZfK4b6xRglVwRm2Zmv6w6fy5cuHWWyimg\nXxYbQQyhrBzTq01I7T63aw2ir+5EujXctp+TEZS8mJBBBrJQNg45E5/nHec2TRuqBJS+d6MQJmWx\nboPCO73ChoG/URyOBPhAcTgS4APF4UjAfGOUQBwRhymVmEOFMHsN1yqEZ8BHN+3tda7GC3bWiMfC\nJYoOrQ4EiXBo2LhgNoh8f3Opa9rW+/FmW0s2XTs6FT/XJeXtSh7X7SxmNu7ZmsWYgf1920D++XML\nUMJ6LdiS1c8M40ruKyO7FGJtJ+5vXbVtzavx+bbG9MygayXHDDNM15omKfbiOY+uJo37rB4uemCY\nSCngfAemE2zJlWT4G8XhSIAPFIcjAXP39crGoGLFVySxqTrqhbPqgcwQFNKSrQ1a8AVshOnVDFjT\nbFA969sckn/VXtxujIkqdKH03tBStmcgjc1+v+f7W4fbd3Zt6rjfrJ5FxxlvLhXX1Mg59krLP7Ym\nkc6t71n6uL0Z95FqiYhku1AKj+jVbAkWfC1VqwSmE3tOfLpTWtCGCmwd0MKwzcj12pft95Bvx77N\nzvnMvMNx0+ADxeFIgA8UhyMB81UPi13JiDGDkAoYU8B1BhJhYnksZlq53N3kFJgTEKcObVwGRx2H\ndGZzx14v24H6ITWr5/JtiolClHysipWGTGbVxnIr7Zg65lWFzZqVkSiF2Z7ZFYcjUCvv7FkpShjG\nXxH2gZgsg8lHz2pD2kvxyz21sGfaxtN4zgmVRscVqqOJTX/jCse8be897MR+51a9Y+JRXv2YCn+j\nOBwJ8IHicCRgvtQrExmtQEoRr85pXqQRpAJGzy+kRfsXAc+oPs3oc0oRrwdq5TClvx+wizPAIiIl\nvsqZsgGraEzo/qAr5VVLd9bhHjpU8gIXZPHCLTRwnpGyGFXHV8Z2hv3KblQBT4fVJShY4W0b7e5k\nFM+z1bRUr05Zgd87lnkQsUYeGVV8xiOnlsnKFEpEFAvHc5fwN4rDkQAfKA5HAnygOBwJmLu5xGTB\n7r8AVnwijz1StnmGKVn6HJbh5j8DmIJmntysWfoGn1NKY5u4hE+Bsc2ATCnAa7nJytsrUWKymlnC\njfKW89nQtE0hf8tlqXFV45WhNYLYuRr3ddfGNvh8s12S75h9+7miHa837lHKGeLBctHGYI1OfIi9\njpXoLOQxPtvtWhnOqnm+FNOejtdYvsNKglLhbxSHIwEp3sP3qOrnVPXbqvotVX3/wc+9PJ3jtkEK\n9ZqJyC+HEL6qqgsi8hVVfUREfk5utDxdadW2U1g/xKnH0lAqdiCAmXn2d8I7IiqkY3SzsG1hVk2v\nGkCN2Bcqg0VBXI0WJ8BnVMW2hPvle0Afqum6pS0XTy0dbi/ldnEWqofZDwx9tzZ2rUJYQdl8xPeq\nJiOMi654AZZJ/RN1bm3Gvs3I86v8a9UUGH2Q717YMG2rC/G5lGN7TgUvtnZNhek6pJSmuxRC+OrB\n9raIPCYid4mXp3PcRrihGEVVL4jIG0Tki5JYng5L0xVems5xiyJ5oKjqQER+T0R+MYSwhW115emw\nNF3TS9M5blEkpYdVNZf9QfKbIYRPHPz4hsvTNQqR1hbw6B7w4bblpriqkSUsDVCcsiAhQAnrI6lc\nOLg5pHNCfMHetbh6L9+2n0OTCubpGahWx6yOzqHEXJ00hG4QFbVsZIeYkIRldxbTqVyOPHTj8ywo\nlso7kdNPyBkwNOI5OcU9PgVmD6ftCsepQNzFWXrwfeZpAYzBUEUtIpJ348OfNem57MR+P18sy3GQ\nkvVSEfkNEXkshPBr0OTl6Ry3DVLeKG8TkZ8VkW+q6tcPfvbvZL8c3ccPStU9LSLvujlddDhOHiml\n6T4v1YXibqg8XVDr9zRdjK/Sdt/mVusq/2ZAvYiVSQGp1sDUC2b0w4ToVQaLexaJBvbjaz1ctbPa\nSLfYsGIMM0vjFfItG1UvJirRo6pr05lYxu754YJpQ2oyJcOK0Sx+1azKbcACrAmlVluwQIp9nnFm\n/shiqXZsW3qlpUnlctwfju0MO6ZvW1l1FWL2QlsE3+c1UgIo+EU3W8czH/aZeYcjAT5QHI4E+EBx\nOBIwX/VwU2Q6gBWIC5Evtlk5AXEJG+DtJ+JeOI4u0q6OUUIez1PaUMOkIhcXrDQE65WMulb+gXHJ\nhFbWjc5iPprMLGDFY3nK8u3BslUFI2ZgPLExtCsHc3hOaODAYBkHxjOzPfu52U6MWZRS+MNXYM0V\nkv0MYls3t/d3phsDmlMte6+Y8uYUN5bza5Oxxkovxj0bC/bLXVmO1+OaOanwN4rDkQAfKA5HAuZe\nmg4rrZlFVoQGVIfNGpZ6YYqUZ+0R7P3UbsX9QcfOFveAHnDF3vVRpFvje22qc/McpFNZyQwIIyoX\nAVSsQSUv6hat4XNp0vX4WATSxxYtUsNnMWzbdG0JPltK9LG9HClqi+jcmUF8Tud7RvEkd7Tj/rmW\nXUg1gvkDLsvXA3n2qcx+D6u9mCp/vm/T5meAlvGitVT4G8XhSIAPFIcjAT5QHI4EzDc9rNZQAlGy\nSUQNT0cP2h7JMbB8Wb9lZTGdjOS9AJR/cEk05PTnlq1WA+MnNJITEdndjulb7di4AMuDNyhmwPtt\nZeSxi2nzRnVsM6N7wOfbpnOe68U4gVO5O+MYJxT0HWGaeUDPeqUT44J+ZuNBTO1O2dAYMGjaz53J\nYz9XmvZ7eCo7e7i9wPEnrIxcC8db6uFvFIcjAT5QHI4EzDc93KBSDEA/mF6ZtC/1cqkd05KD3L5m\nG4kp0p1pu7KNqQlSnOHMqmuR7jBNKvvVJdkakNrl9Deqe3Nqm+AsOimEZ0V12hz7WZIaG00qOk17\nD5MulGGoWSjGJSjQF5ln0RF11CvXonYfgd/7oGWfu7m/7CaZSzgcDh8oDkcSfKA4HAmYs3o4yOQ0\n8EyQbiiph5HDN2oqHnNMgtyYTeDqZBwsWzHnbFTX1NgDA7cuyThO96IydjyrftTMqfF6E4pDpmVM\nOY9n1fz+iF8zpHY5/V3CAlZeObicx3tgM4vNaewLxz2o/B0WVhaD31nJqmNo6zVsynmaxXNOamIb\n/r5wZWQvn/DhSfA3isORAB8oDkcC5psebopoP9ITVNSOqSQaKn857TouYrc5ZYml21oNS6/6WfVr\nF2nF3sxSBU4Jm37WlIuoSzl3QSXQo36VNTPsddWSEWzMUMCiLqZJiLpZ9F2iUPicjtBT2N2e2VQ8\n7nNf8DuaNS29WoIyF+PSfif4/fEzm5bVNC0V/kZxOBLgA8XhSIAPFIcjAfONUYI1fFAoU80mEQXI\nW5hzDiElezQ9HLk5Gxdg6nNKqWOMe/h6CL7eANKNJfkEYkqYYxRMR3MKGOOunYnl90PwHmaZSg5x\nSUaGHAWkkncnlt/jvbdpVWgTgo2FzJpubEJZ7BmlazH9zqn3UVH9a1dAzNKg8np7ZYyJlprVBhws\na8I466qvcHQ4bh5STLo7qvrnqvoXB6XpfvXg516aznHbIIV6jUXkwRDCzkH5h8+r6v8VkX8qN1qa\nLojx/1VTKsCO2WIS96ct283dGiUsUiOefc8aab6znOrEmWymVzj7nxFVwFlgPieeZ3Ns6cDWKNIt\n9hAegzdwg0vawf0O2pZ+NBvVKW5M866NB6YN+8kKYaQ4rMauUw/jd8T0ql3zHf1gFEs27OU2VT2B\nWngLlOLemEZzkNU1Ml9LREppuhBCeGE5WX7wL4iXpnPcRkiKUVS1eVDyYVVEHgkhHK803baXpnPc\nmkgaKCGEIoTwehG5W0TerKqvpfa00nQLXprOcWvihtLDIYQNVf2ciLxdjlGaTptB8kHk7cVO5I5K\nZZsDxDLTSXU3R0fKl7UrjrRcnPl2nWwEVziyCQYeyx7JKE3htPLmON47xiQiIrvDuM9xCCIjmUof\nVkaeau+ZtvO9eL+sEMa+jUnusTGJ8RM/l9NwjbMdKpACyCkOweuPS/vdotKXU86oVuZ+NuEe+P6+\nvxvzTHrVxjapSMl6nVXV5YPtrog8JCLfES9N57iNkPJGOS8iD6tqU/YH1sdDCJ9W1S+Il6Zz3CZI\nKU33DdmvLc8/vyI3WJrO4bhVMVcJi2owNQH3oCx1Y0TSb6TmxO9R7sKloLH+Ba4+FLGSEl7phu4j\nLM9PnX9ZIOkExkGbU1tXZW0vcv/NbZJVoMkdxSEYB9WZcjO/b2jk7Sylx2N5XqoDfP/yxM6xbGi8\nJ1wJKSKyaCTx9tcMnVd4jgXnbQqu3QmxTpPufQmuz/eAxoTNYc1y2Rq4hMXhSIAPFIcjAfM1lwgq\nkwlQAnh7HvEKqHlDohEFq45R8sErALGeCPv2GulLDdXiNC9SE05LbkE689ntZdO2vh5pDNdOyaCs\nW6dtz7m3F1PHwzVL2bbzOE+1ukD+u53q1Z34zFhqs9yrVumu7g4q2y70rx5utxvV6egGmdptmxQw\nlckDuVCXpu3QiOLyxNZH2dyMz6nl1MvhuHnwgeJwJMAHisORgDnHKJTOLWr4IrQFko0UIG8paKzP\nIA6ZUDzBEhNzTugXxyEYz+QUv4yaMQV9dWy1bFgv8PK65c1hDHEJ1UacjeLXsrVpJRedS5Di3rT3\nMIOQZbpAZnVgjl72KAarzjLL7lK8PteGwXopP9iy8nV8hvf21k0bxh4sRUG5/i5J9/F74CUNeL3L\nIxs76Wo8T4ueWSr8jeJwJMAHisORgPmaS9RAqWyFIvWa2fFc1qSHzRu5ZuZ6UuPby+BUMgLTqXtk\n2rC5FblQObSPOutDWrlnU7nTKRr8dUzb5DT4Ep+2fcFn2CA1Niofyha1Lca+sAd0nkea1ic1wyLU\nqXl+x9IdTB13KG1uTO6IeuFKRfaOHsNzYdNAxJNXzpj97vOg8J7U8Mwa+BvF4UiADxSHIwE+UByO\nBMw1Rmk0gvSBj28N4PLr1BWUtzCtNBKI6uuxuhYlLVOKUfDII8bb6MJCfUHevL1jFcIYlzQ65GDS\nj/y+17Z8u2hVl/me9uI5ubYI7h8R4eCxlI7u9uL1F7rW5A5VuhyroQvLXtumsdf34rO4uLNk2s52\no3dCXazRpnqSY/h1ZSP1L165cLg9fNqm4hdAhVNmLmFxOG4afKA4HAmYO/XqAq3Y7USCEDLblVBj\nqoAvzyxPW1Qlwgu+iLYgveK0MlAONqQbDSMFKPbocQLFycnTF32C2agPyQgbSGC/m3Q5pJrtFpWz\nhmvwPfAiKIQxniDPYEz7cnk9NMzYGtoUt7mebbLlA0kFgXVONsf2gz94ZuVwe/H79u8/ThmUx/OW\n8DeKw5ECHygORwJ8oDgcCZhrjFIUDbmKso6NSBhZcqHNavUwpnk51kCezuZxdmUkxyhxn0sz7zs1\n7WNGcprpLshWKF1rZCq0UhGVt9yXHGIirl9Z1ef9fsZtVkCb83NMBKYbu2NL4hc7MfY4Ep9BHReu\nSdKDWHSDYjesB7M9rTYsZJMINODjuKf1XLxGvk1q7C486+Nlh/2N4nCkwAeKw5GA+aqHhw3JvhlV\npTBBK1PyKRhnqB6unoFWpVRgA32v7Dkb0Ma0DGnMjGftoa0g6mWkzJmduc4hRcsqAaQ7m+RNhscW\nNWXymLJhCphpS1FjIIHA+isiIluwzWnkKfSN+4nUj6keeq/tTizVs8+JSqpDvyfkR90EdfSRqQV4\nFDWMtBb+RnE4EpA8UA5qpHxNVT99sO+l6Ry3DW7kjfJ+EXkM9j8g+6Xp7heRPzrYdzh+KJEUo6jq\n3SLyj0XkP4nILx38+J0i8mMH2w+LyJ/IdWo4tq9O5cLvXDrcn9wTX0KXHiAtA3JJzulBG3P/unoi\nZU0tE3N6OgXyfY4LtAVxT4s8i7Nq2Qhy7DGleZFI5yTRwftjecsCpHI5DhlBTDQtqmMifn4YrxX0\nrNEbuG7FKJfyNueoic9m1M8OKJtLLh0OjyLwn3/sdnVXapH6Rvl1EfkVuswNl6abFNWugw7Hyxkp\nhYR+SkRWQwhfqTomtTRdq9m91iEOx8seKdTrbSLy06r6DtnXei6q6sfkGKXpwngixZN/dbif94Fu\nBUu9jEkEDcFQM6ttZu15YZOhUPacSMWO0Dc4mFOPJaaLqS/oExy4L1OgKuxvBqrjI0u4oKw404ht\nhQVLPAMNlEY5xW08oOneQeHd6tneIKUa0ox+o8bnGdPYndwqD0xBdaJ6SCeLKZUPhPVmnAI2VOyY\ned6U8tkfDCHcHUK4ICLvFpE/DiH8jHhpOsdthBczj/IhEXlIVb8rIj9xsO9w/FDiRqsC/4nsZ7e8\nNJ3jtsKJGuDpJPLTzHoayHQCsUZuX3zIo8ucPGix/MqRtHJ1bGPjEntOo8qlVOcE5S2TGnkLA47N\nNqk+yh6sYrSiXMFyIgUJb5GbH7l1UIOUOZP4uNnaovQ3rGocnbFxyPb5uBazyanxmpWnLUhr162u\n7OY2JjLlu3esvAVLsJRNTrdXXiIZLmFxOBLgA8XhSMD8qRdyo2mkXg2iGA3jo2vbilb1uxRnq0tK\n5aLy9+iiLlOGuLKtSenTMsfZ92pvssa2bRuAAcLgIlG9EtKuK/Zv2c49cXtyxtIbhVRugygp0kKl\newiwsGrwfUsDVx7di8eRidra6+IivK377fXGpyMta3ftF4iL0dq0MA3pFasLVrejxDy/Wp3iLi0r\nM1+nL9xyOG4ifKA4HAnwgeJwJGC+MYqqaA6XbMA4rc7kirKJAtZOKaolLEcuD3FJg/ivkU4cUSBX\nr9YzqxG5jsswkuX+M7bt3FejQDTbsLnx8dnI/ceLNiXbXo89bdAqv7IV4wuqPC15jWo2347nXHrK\nBovZo1FyFEj3s7Dytw63d15hYxt8SqzUXgCzvA75C0/A5G7KZet2osypx2nsUB1jml8Jj1EcjpsH\nHygORwLmS726HZHX/s3D3eFKfJUyVUC6pTNK5YLZBC/gmQId4UVdiCMz8zXH4kwyzxajR9V0aqnC\nGDy/eIa9uRcpR9m2Nz8+HT83WSLVMaQ+OW3egOdyZGYeLlF0KW0O+5Ml8hc+Fz19G1TzYrgCfmdL\nlkK1wGuZvcnQA4xNMFrwde5NLe0MUCGZn2cA+QRZFguumfP0sMNxE+EDxeFIgA8UhyMBc41RJksN\nefodcRUermI8wqkxlUu+xAEKiJSk2J2BTzBLNTA9rJQCnsDn2AwBV+ix5AI5NtckmS3Gjo7O2kc9\nOhPjs+6lXdPWnEA9loG9v9FZIOA1JgpKpoG4yi+0SXE9io3jRYr5zi8fbs96NgbbuxPigp69d/aL\nRmDdEy6RPYGYc22nb9raq/DdkiWzsYuuic+OC3+jOBwJ8IHicCRgzunhUopXR5ox3Ygrjxpj8ncC\nupVbZmLVwqLnAAANuUlEQVTSokqfQ8oWakycmBpgapdTx0i9pll1NWH2qEIl8+huW/12HUpe5Ns2\nDZrt1UyjL8ebb7CSGQwXjpSEADrZpGn6EkovFB37OVysNTpFNPAMUFJSCBdAoWb0XLCib4tm5jcn\nkZLurvVM28JO3FZSCZSgbKaqHVbl4d7DDsfNgw8UhyMBPlAcjgTMNUZpZTN5xbmrh/tPzc4cbgeS\nKwjwTBKRSmMM8QtdY5IbHbBpCyDBKNnoDT1vKR2N2t4mpZXRzI1N9VCF3F22CuHtV8abao6tgybG\nZMy3G1CDZWlhT6rAcVYbjOYWqdT195qnD7f31m1KtoSYbLJor1FCvNTv2BhsOOFvJmIXYpSG2njp\nym6MS/Ir9tcTY1NebYlf9RHvYYBLWByOmwgfKA5HAuZLvRqFXBhE6jW7M47T76+dN8eiEpbNAjJg\nHM0xqWuBKhQD+1o31IvKyClSPfrzMYPU8ZjSw5g65hl9nLVndfIEzBe277O+y9lu/Nz4tJXCDnqR\nNr3y1BXTdmcnFpLL1X5uB0zANqeW6j3XjmqJrXNURg6UAWXP3l9vKS4+G7RpwRdQ0jplNlYWFhHZ\nvhKp3+AKpbgxk8wWaqgQ5goUeBpPDzscNw+phYS+JyLbsr/CcxZCeJOqnhaR3xGRCyLyPRF5Vwhh\n/eZ00+E4WdzIG+UfhBBeH0J408G+l6Zz3DZ4MTHKDZema2iQNjjb3TOIL6CnV1bMsWE1phADZY4F\njAUapCLNd1CvYP8OFMCxA9cIgTQll902rsQUwGAc0qJScUWNgraJZev6JFmB/CargLcvRxO4r48s\nvx/0Yrqd46WdYYxRxkP7uRIM8Fj1Uw7AJ7hvH/ZCN8YlbZKiWAMJK2/BuGRjZOOlbC225btk1Acp\n4YJ+J1AhzCng48pWEKlvlCAif6iqX1HVnz/42Q2Xphuuj651iMPxskfqG+VHQwgXVfWciDyiqt/B\nxhBC0IoF6iGED4vIh0VEzr1m5SUY2w7H/JE0UEIIFw/+X1XVT4rIm+UYpelE7EzsHe3tw+3777If\nf2II6WJ6l6LhQmeNTCJggrhJL7ASZu2pEp4p0RBozCunGwGYHs5rKvHyTDWqa/l6yPWaE0qRboGh\nA83obwjs18xAhxbxq2oxgymT1yRq2apJAa+0o7zgNMm/nxnGatCrm2dNW/sqmIqQmQVOE9TNvjPm\noh5W1b7qfnFAVe2LyE+KyKPipekctxFS3ih3iMgndT/CzUTkf4cQPquqXxKRj6vq+0TkaRF5183r\npsNxsrjuQAkhPCUir7vGz700neO2wVwlLE0tZQDOZQUwv7+38rQ5djiLhPTZ50+ZthlQ7GLL3gLS\n4TKn+AVUFqFFKxWhJkkgyQx67s4orYyl1RbbnNWLgdAel5eGz5WUdi3AT7m5Sys/IV18JI0NJecC\nmw1nYKxB8p0wRtMGMqWA74jzNWi0gaZ2IiJdSAlznZMdWFE5uWxXMQ7W4B44NsR6MzUmd7X+wq4e\ndjhuHnygOBwJmC/1klKWsqg43ZzFdCarXX/y/GOH23+a3W/a/vJiTClOlu37GT2Lm3YtkQSothuI\nlhVYSZbS0QG6NpvaR4bGCTwDnXUi5ZgUtp/gk3Bktr8AmsRUAZUISiUvBFPJRNmQNB1ZDFZdwFdm\nC2BKQbP9OXyQ770FHR2SQvi57ahW7jzH3191/hZ/RY54ddWUdtAar45U+BvF4UiADxSHIwE+UByO\nBMw1Rsm1kPP5utl/AVMizmeyKG/5Z+e/ato+k/3I4faj5V2mbQJS484qGSxswA4pi8sWpiWZ5MbN\ngryOd0Yx1Tnr23voZTFIWqAVgKg65roqRTN+LSEjBS2mjsecH4YusyAZ66O0WZULbV364EKMPTot\nG4dgijvX6tWdQ5b6AthDuABp0ZHYoqYMdqhpk3lIWBwOhw8UhyMJc5+ZX25GZ4gSxulGYWdoRzA9\n/ur2D0zbe89HWva/5AHT9o1GpGJjMFQQsRV18x3TJGUOXrlcZgKNC0h5O2pFWrE2tJ5YZ0DMm1MO\ndqkTZ/G50vBV8AMbt0l1DMqAKdFATI0H9i2DmXqlWXssgdGi0hV9WJy10Lb5dqRXZc2UN9/7vUuR\nAz/WO23aWptwD/Tbid8DUyhD0+qU0z4z73DcPPhAcTgS4APF4UjAXGOUhgTpaEwxns22oM3y5ikQ\n1N3Sxhp3ZTHF/LPnv2DaPtuJqePPd/+6aRv9VTRm4NQxms4dSa1i+rRDchNI5W50yUMYYo9uZlOr\nfUgdszED9myPYhRU8LL9Lqp5ucw3YjyzXzuXsEbgebifGTyojB4apoSXc+uR/NqlGHN+s3+facNw\npqQ0vdZIbUybxygOx8nAB4rDkYC5Ui+VIC14R7YE3peZzdcWwHf2iHpdBFp2T279d9939s8Ot8+2\ntk3bp5qRlo1LW8Mgh7Rkc1j9fj6qRI05y92smnq1BsQbaiS7/RxoWU0V4qWWXSh2bz/6OrPZA/oN\nY/k3EeuzxTQMz8OLsxYy8PUig7W1SUyV8/dwOour69jPuMyrPcbqYChbzXF6zFeDv1EcjgT4QHE4\nEuADxeFIwFxjlCAqE9AhYLzSVyuPyIHzjsjtoYD8H6uO+5B+ftfyl0zbuVfFdPTDzbeatu0nooEF\nmrAxmiM2rIC+UCGXzSzKctgLeBEkLJw6xtim5FWMNcCU7Iwc4tbHsS8TrvUHKOhzYzDqQzW0iDUz\nXB0PTBvGPedaW6athy6FjWqTuyPpYDi0zgCP/agxXXzcxY7+RnE4EuADxeFIwFyp15VZXz52Oap9\n//mZLx9uo6pYxM7MLzeoDVOylDrGqr0LDUsVHuxHb/Gzr7Ypy0+efsPh9leeuGDaOs9ESpNxIV54\nl4cdS5PKK/FzV4KlJrv92Dbo2rRrB1LCUzKewGq/BbWhFxrTq51xfE7s35ADLRyTeQYqAYZTSy3R\nn+t5MIwQETkziCngH+k8Y9q+N4nmIDokfzWkSbTeq84kwmTDeVEXVgyu8ZGug79RHI4EJA0UVV1W\n1d9V1e+o6mOq+oCqnlbVR1T1uwf/n7r+mRyOWxOpb5T/LCKfDSG8WvZ9iB8TL03nuI1w3RhFVZdE\n5O+LyM+JiIQQJiIyUdUbLk23u92V//enf/tw/+xDUbbyD5e+Wfm5FuUJR5BDbJJUo1FDZDtwnge7\n1uv4jfdEHv2pZetJ/vDSW+K1n7RcHFfkgbefiIgokOOxWMI9HEWyPGxRnMX1SxIRjMudbSvBwEKb\n1SscSzK6yLvVKuTVtSgDCrTa8j4o7X1/vmnaPr/zqsPtzmXycoZwbWrDOpM6ZgWQufW60trkK52K\nlDfKfSJyWUT+h6p+TVX/+0GdlKTSdA7HDwNSBkomIm8Ukf8WQniDiOwK0aywb/d+TSMYrOFY7O5e\n6xCH42WPlPTwsyLybAjhiwf7vyv7AyWpNB3WcFxcuCu84v/EGelPnHnj4fZdb90wn3stpRQRuPir\nWUO1ekTZFsxKJ5tDvBNez/9y6WumbfCa2OePdqyZxZWnYg6jc9nSFptKtn+TZqhQrqMKZBKBTJO9\nh82joHPW/UUMQL0alD6djqHsA1U5ztdj22TFPusHTj91uH1vZjnU5Ql4D1+x99faAn+1BnuvwTaX\n5oBDSzalAG80bkvFdd8oIYTnROQZVX2BWP64iHxbvDSd4zZC6vj6BRH5TVVtichTIvKvZH+QeWk6\nx22B1KrAXxeRN12jyUvTOW4LaAjHNGM9zsVUL8v+2+eMiKzN7cL18L5cG7dLX14RQjh7vYPmOlAO\nL6r65RDCtd5Qc4f35drwvli41svhSIAPFIcjASc1UD58Qte9Frwv14b3BXAiMYrDcavBqZfDkYC5\nDhRVfbuqPq6qT6rq3GX5qvoRVV1V1UfhZ3NfV6Oq96jq51T126r6LVV9/wn2paOqf66qf3HQl189\nqb5An5oHAtxPn3RfXsDcBoqqNkXkv4rIPxKR14jIe1T1NfO6/gE+KiJvp5+dxLqamYj8cgjhNSLy\nVhH51wfP4iT6MhaRB0MIrxOR14vI21X1rSfUlxfwftlf8/QCTn7tUwhhLv9E5AER+QPY/6CIfHBe\n14frXhCRR2H/cRE5f7B9XkQeP4E+/b6IPHTSfRGRnoh8VUTeclJ9EZG7ZX8wPCgin365fEfzpF53\niQhKgp89+NlJ40TX1ajqBRF5g4h88aT6ckB1vi77CvBHwr5S/KSey6+LyK+IteA68bVPHswDwv6f\nrLmlAVV1ICK/JyK/GEIwLnHz7EsIoQghvF72/5q/WVVfexJ9UdWfEpHVEMJXqo6Z93f0AuY5UC6K\nyD2wf/fBz04azx+sp5G6dTUvNVQ1l/1B8pshhE+cZF9eQAhhQ0Q+J/tx3En05W0i8tOq+j0R+W0R\neVBVP3ZCfTGY50D5kojcr6r3Hcj13y37a1pOGnNfV6OqKiK/ISKPhRB+7YT7clZVlw+2u7IfK33n\nJPoSQvhgCOHuEMIF2f/9+OMQws+cRF+u1bl5BovvEJEnROQvReTfzzsgE5HfEpFLIjKV/RjpfSKy\nIvvB43dF5A9F5PQc+vGjsk8fviEiXz/4944T6svfEZGvHfTlURH5Dwc/n3tfqF8/JjGYP9G+hBB8\nZt7hSIEH8w5HAnygOBwJ8IHicCTAB4rDkQAfKA5HAnygOBwJ8IHicCTAB4rDkYD/D7YzX4H3LWpI\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1297f40b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(data.images[275])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data.data\n",
    "y = to_categorical(data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 137.        ,  145.        ,  154.66667175, ...,   83.66666412,\n",
       "          23.        ,   15.33333302],\n",
       "       [ 180.66667175,  182.        ,  186.66667175, ...,   57.66666794,\n",
       "          24.        ,   14.33333302],\n",
       "       [  53.33333206,   63.33333206,   70.66666412, ...,  166.66667175,\n",
       "         197.33332825,  204.        ],\n",
       "       ..., \n",
       "       [  48.66666794,   58.66666794,   97.        , ...,   65.66666412,\n",
       "          65.        ,   60.66666794],\n",
       "       [  66.        ,   73.        ,   76.66666412, ...,   89.33333588,\n",
       "          20.        ,   10.        ],\n",
       "       [  94.66666412,  110.66666412,  134.        , ...,  168.66667175,\n",
       "         101.66666412,   70.33333588]], dtype=float32)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train/255.\n",
    "X_test = X_test/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1030 samples, validate on 258 samples\n",
      "Epoch 1/5\n",
      "1030/1030 [==============================] - 2s 2ms/step - loss: 1.7676 - acc: 0.3903 - val_loss: 1.7583 - val_acc: 0.3798\n",
      "Epoch 2/5\n",
      "1030/1030 [==============================] - 2s 2ms/step - loss: 1.6804 - acc: 0.4194 - val_loss: 1.7062 - val_acc: 0.3798\n",
      "Epoch 3/5\n",
      "1030/1030 [==============================] - 2s 2ms/step - loss: 1.6648 - acc: 0.4194 - val_loss: 1.7062 - val_acc: 0.3798\n",
      "Epoch 4/5\n",
      "1030/1030 [==============================] - 2s 2ms/step - loss: 1.6532 - acc: 0.4194 - val_loss: 1.6968 - val_acc: 0.3798\n",
      "Epoch 5/5\n",
      "1030/1030 [==============================] - 2s 2ms/step - loss: 1.6469 - acc: 0.4194 - val_loss: 1.6925 - val_acc: 0.3798\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12b8affd0>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(X_train.shape[1], input_dim = X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1030 samples, validate on 258 samples\n",
      "Epoch 1/10\n",
      "1030/1030 [==============================] - 3s 3ms/step - loss: 1.8248 - acc: 0.3893 - val_loss: 1.8542 - val_acc: 0.3798\n",
      "Epoch 2/10\n",
      "1030/1030 [==============================] - 2s 2ms/step - loss: 1.7176 - acc: 0.4194 - val_loss: 1.7299 - val_acc: 0.3798\n",
      "Epoch 3/10\n",
      "1030/1030 [==============================] - 2s 2ms/step - loss: 1.6702 - acc: 0.4194 - val_loss: 1.7155 - val_acc: 0.3798\n",
      "Epoch 4/10\n",
      "1030/1030 [==============================] - 2s 2ms/step - loss: 1.6630 - acc: 0.4194 - val_loss: 1.7016 - val_acc: 0.3798\n",
      "Epoch 5/10\n",
      "1030/1030 [==============================] - 2s 2ms/step - loss: 1.6627 - acc: 0.4194 - val_loss: 1.7214 - val_acc: 0.3798\n",
      "Epoch 6/10\n",
      "1030/1030 [==============================] - 2s 2ms/step - loss: 1.6588 - acc: 0.4194 - val_loss: 1.6960 - val_acc: 0.3798\n",
      "Epoch 7/10\n",
      "1030/1030 [==============================] - 2s 2ms/step - loss: 1.6537 - acc: 0.4194 - val_loss: 1.7017 - val_acc: 0.3798\n",
      "Epoch 8/10\n",
      "1030/1030 [==============================] - 2s 2ms/step - loss: 1.6490 - acc: 0.4194 - val_loss: 1.6955 - val_acc: 0.3798\n",
      "Epoch 9/10\n",
      "1030/1030 [==============================] - 2s 2ms/step - loss: 1.6424 - acc: 0.4194 - val_loss: 1.6856 - val_acc: 0.3798\n",
      "Epoch 10/10\n",
      "1030/1030 [==============================] - 2s 2ms/step - loss: 1.6332 - acc: 0.4194 - val_loss: 1.6869 - val_acc: 0.3798\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1468fd908>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Code from Riley'''\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(X_train.shape[1], input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
